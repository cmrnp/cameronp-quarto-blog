[
  {
    "objectID": "post/2023/06/dplyr-fitting-multiple-models-at-once/index.html",
    "href": "post/2023/06/dplyr-fitting-multiple-models-at-once/index.html",
    "title": "Fitting many statistical models at once using dplyr",
    "section": "",
    "text": "One common task in applied statistics is to fit and interpret a number of statistical models at once. For example, fitting a model with the same structure to a number of different outcome or explanatory variables, or fitting several models with different structure to the same data. Here are some examples of how I usually do this, using features that were introduced with dplyr version 1.1.0.\nFor this demonstration, we’ll be using the R packages dplyr, tidyr, ggplot2 (all of which are included in the tidyverse), as well as gt for making tables, emmeans for obtaining estimated means and comparisons, and performance for model-checking.\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(emmeans)\nlibrary(performance)\n\nWe’ll be using the Palmer Penguins data collected at Palmer Station, Antarctica, made available by Dr Kristen Gorman, and conveniently accessible in R using the palmerpenguins package. This dataset contains measurements on a number of penguins of different species on different islands.\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSuppose we want to compare bill length, bill depth, flipper length, and body mass between species. We could manually run a separate model for each, but here’s a way to to automate the process. As with many things in R, the trick to doing this easily is to get the data in long form, with the outcomes stacked on top of each other, and a variable indicating which outcome is being measured.\nThe pivot_longer() function from tidyr gets the data into this format. I’ve also taken an extra step of recoding the “outcome” variable to give more descriptive labels. This isn’t required but it will make the tables and plots that we make later look nicer.\n\npenguins_long &lt;- penguins %&gt;%\n  pivot_longer(\n    c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),\n    names_to = \"outcome\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    outcome = fct_inorder(outcome),\n    outcome = fct_recode(\n      outcome,\n      \"Bill length (mm)\" = \"bill_length_mm\",\n      \"Bill depth (mm)\" = \"bill_depth_mm\",\n      \"Flipper length (mm)\" = \"flipper_length_mm\",\n      \"Body mass (g)\" = \"body_mass_g\"\n    )\n  )\n\nLooking at the first few rows of this data frame, you can see we now have four rows for each penguin, one for each type of measurement:\n\nhead(penguins_long)\n\n# A tibble: 6 × 6\n  species island    sex     year outcome              value\n  &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie  Torgersen male    2007 Bill length (mm)      39.1\n2 Adelie  Torgersen male    2007 Bill depth (mm)       18.7\n3 Adelie  Torgersen male    2007 Flipper length (mm)  181  \n4 Adelie  Torgersen male    2007 Body mass (g)       3750  \n5 Adelie  Torgersen female  2007 Bill length (mm)      39.5\n6 Adelie  Torgersen female  2007 Bill depth (mm)       17.4\n\n\nWe can use group_by() and summarise() from the dplyr package to group the rows by outcome, and then “summarise” these groups of rows down to a single row containing a statistical model for each outcome. This makes use of a couple of R tricks: ‘list columns’, a column in a data frame that contains a more complex object than the standard R data types (numeric, character, etc); and the new pick() verb which returns a data frame containing selected columns (in this case, all of them) within the group that’s being operated on.\n\npenguin_models &lt;- penguins_long %&gt;%\n  group_by(outcome) %&gt;%\n  summarise(\n    model = list(\n      lm(value ~ species, data = pick(everything()))\n    )\n  )\n\nYou can see the resulting data from contains four rows, one for each outcome, and a statistical model (“lm”) for each:\n\nprint(penguin_models)\n\n# A tibble: 4 × 2\n  outcome             model \n  &lt;fct&gt;               &lt;list&gt;\n1 Bill length (mm)    &lt;lm&gt;  \n2 Bill depth (mm)     &lt;lm&gt;  \n3 Flipper length (mm) &lt;lm&gt;  \n4 Body mass (g)       &lt;lm&gt;  \n\n\nIdeally we would also do a visual check of model assumptions. One way to do this is something like the code below, which saves a residual plot for each model to an image file, which can be inspected later. It uses the check_model() function in the performance package to generate the plots. The generated residual plots aren’t shown here, but they all look fine.\n\nwalk2(\n  penguin_models$outcome, \n  penguin_models$model,\n  ~ ggsave(\n    paste0(.x, \".png\"),\n    plot(check_model(.y, check = c(\"pp_check\", \"linearity\",\n                                   \"homogeneity\", \"qq\"))),\n    width = 12,\n    height = 9\n  )\n)\n\nOnce we’ve fitted the models, we can obtain quantities of interest from them. In this example we’ll look at estimated means for each species, p-values testing the hypothesis that all species means are equal (against at least one pair of means being different), and comparisons (differences in means) between all pairs of species.\nThe reframe() function from dplyr allows us to run some code that produces a data frame on each model and stack the results on top of each other. We can use the emmeans() function from the emmeans package to obtain estimated marginal means and as_tibble() to convert the result into a data frame. The rowwise(outcome) at the start tells reframe() that we want to call emmeans() separately for each row of the data frame (i.e., each outcome model), and preserve the outcome variable in the result.\n\npenguin_means &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    emmeans(model, \"species\") %&gt;%\n      as_tibble()\n  )\n\nThe first few rows of the resulting data frame are shown below. There is a row for each outcome for each species, containing the mean (emmean), standard error (SE), degrees of freedom (df) and lower and upper confidence limits (lower.CL and upper.CL).\n\nhead(penguin_means)\n\n# A tibble: 6 × 7\n  outcome          species   emmean     SE    df lower.CL upper.CL\n  &lt;fct&gt;            &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Bill length (mm) Adelie      38.8 0.241    339     38.3     39.3\n2 Bill length (mm) Chinstrap   48.8 0.359    339     48.1     49.5\n3 Bill length (mm) Gentoo      47.5 0.267    339     47.0     48.0\n4 Bill depth (mm)  Adelie      18.3 0.0912   339     18.2     18.5\n5 Bill depth (mm)  Chinstrap   18.4 0.136    339     18.2     18.7\n6 Bill depth (mm)  Gentoo      15.0 0.101    339     14.8     15.2\n\n\nWe can use ggplot() to present the results visually. The plot shows that there’s a substantial variation between species in means of all of these measurements, with little or no overlap between many of the confidence intervals. Gentoo penguins appear to be heavier, and have longer flippers but shorter and shallower bills, than the other species.\n\npenguin_means %&gt;%\n  ggplot(aes(x = emmean, y = species, xmin = lower.CL, xmax = upper.CL)) +\n  geom_errorbar(width = 0.5) +\n  geom_point() +\n  scale_y_discrete(limits = rev) +\n  facet_wrap(vars(outcome), nrow = 2, scales = \"free_x\") +\n  labs(\n    x = \"Mean\", \n    y = \"Species\",\n    caption = \"Error bars show 95% confidence interval for mean.\"\n  )\n\n\n\n\nThe gt() function can be used to produce a nice table of results. The code shown below combines the lower.CL and upper.CL columns to produce a single column with the confidence interval, and separately specifies fewer decimal places for body mass than the other measures. The group_by() function before gt() results in a table sub-heading for each outcome. You could easily change this to group_by(species) to arrange the results by species.\n\npenguin_means %&gt;%\n  group_by(outcome) %&gt;%\n  gt() %&gt;%\n  fmt_number(c(emmean, SE, lower.CL, upper.CL),\n             decimals = 1, use_seps = FALSE) %&gt;%\n  fmt_number(c(emmean, SE, lower.CL, upper.CL),\n             rows = outcome == \"Body mass (g)\",\n             decimals = 0, use_seps = FALSE) %&gt;%\n  fmt_number(df, decimals = 0) %&gt;%\n  cols_align(\"left\", species) %&gt;%\n  cols_merge_range(lower.CL, upper.CL, sep = \", \") %&gt;%\n  cols_label(\n    species = \"Species\",\n    emmean = \"Mean\",\n    lower.CL = \"95% Confidence Interval\"\n  )\n\n\n\n\n\n  \n    \n    \n      Species\n      Mean\n      SE\n      df\n      95% Confidence Interval\n    \n  \n  \n    \n      Bill length (mm)\n    \n    Adelie\n38.8\n0.2\n339\n38.3, 39.3\n    Chinstrap\n48.8\n0.4\n339\n48.1, 49.5\n    Gentoo\n47.5\n0.3\n339\n47.0, 48.0\n    \n      Bill depth (mm)\n    \n    Adelie\n18.3\n0.1\n339\n18.2, 18.5\n    Chinstrap\n18.4\n0.1\n339\n18.2, 18.7\n    Gentoo\n15.0\n0.1\n339\n14.8, 15.2\n    \n      Flipper length (mm)\n    \n    Adelie\n190.0\n0.5\n339\n188.9, 191.0\n    Chinstrap\n195.8\n0.8\n339\n194.2, 197.4\n    Gentoo\n217.2\n0.6\n339\n216.0, 218.4\n    \n      Body mass (g)\n    \n    Adelie\n3701\n38\n339\n3627, 3775\n    Chinstrap\n3733\n56\n339\n3623, 3843\n    Gentoo\n5076\n42\n339\n4994, 5158\n  \n  \n  \n\n\n\n\nWe can do similar to produce an overall “F” test for each model, testing the hypothesis that all species have equal means for a particular measure against the hypothesis that at least one pair of means is different. The joint_tests() function in emmeans does this.\n\npenguin_tests &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    joint_tests(model) %&gt;%\n      as_tibble()\n  )\n\nThe resulting data frame is shown below. This time there is one row per model, but if there had been multiple variables in the model, there would have been one row per variable or interaction term (distinguished by the model term column). Each row contains the results of a hypothesis test: numerator and denominator degrees of freedom (df1 and df2), F-statistics (F.ratio) and p-value (p.value).\n\nprint(penguin_tests)\n\n# A tibble: 4 × 6\n  outcome             `model term`   df1   df2 F.ratio   p.value\n  &lt;fct&gt;               &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Bill length (mm)    species          2   339    411. 2.69e- 91\n2 Bill depth (mm)     species          2   339    360. 1.51e- 84\n3 Flipper length (mm) species          2   339    595. 1.35e-111\n4 Body mass (g)       species          2   339    344. 2.89e- 82\n\n\nAgain, this can be presented in a table using gt():\n\npenguin_tests %&gt;%\n  gt() %&gt;%\n  fmt_number(F.ratio, decimals = 1) %&gt;%\n  fmt_number(p.value, decimals = 3) %&gt;%\n  cols_merge_range(df1, df2, sep = \", \") %&gt;%\n  sub_small_vals(p.value, threshold = 0.001) %&gt;%\n  cols_label(\n    outcome = \"Outcome\",\n    `model term` = \"Predictor\",\n    df1 = \"df\",\n    F.ratio = \"F\",\n    p.value = \"p-value\"\n  )\n\n\n\n\n\n  \n    \n    \n      Outcome\n      Predictor\n      df\n      F\n      p-value\n    \n  \n  \n    Bill length (mm)\nspecies\n2, 339\n410.6\n&lt;0.001\n    Bill depth (mm)\nspecies\n2, 339\n359.8\n&lt;0.001\n    Flipper length (mm)\nspecies\n2, 339\n594.8\n&lt;0.001\n    Body mass (g)\nspecies\n2, 339\n343.6\n&lt;0.001\n  \n  \n  \n\n\n\n\nFinally, we often want to obtain comparisons between particular estimated quantities. In this example we use the emmeans package again for this, this time using the pairs() function to produce comparisons between all pairs of species.\n\npenguin_pairs &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    emmeans(model, \"species\") %&gt;%\n      pairs(infer = TRUE, reverse = TRUE, adjust = \"none\") %&gt;%\n      as_tibble()\n  )\n\nThe first few rows of the data frame are shown below. The contents are similar to what we saw earlier for the estimated means, but this time each row contains information on a difference between pairs of means (described in the contrast column), along with the estimated difference in means, standard error, degrees of freedom, confidence interval, t-statistic and p-value.\n\nhead(penguin_pairs)\n\n# A tibble: 6 × 9\n  outcome       contrast estimate    SE    df lower.CL upper.CL t.ratio  p.value\n  &lt;fct&gt;         &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Bill length … Chinstr…  10.0    0.432   339    9.19    10.9    23.2   4.23e-72\n2 Bill length … Gentoo …   8.71   0.360   339    8.01     9.42   24.2   5.33e-76\n3 Bill length … Gentoo …  -1.33   0.447   339   -2.21    -0.449  -2.97  3.18e- 3\n4 Bill depth (… Chinstr…   0.0742 0.164   339   -0.248    0.396   0.453 6.50e- 1\n5 Bill depth (… Gentoo …  -3.36   0.136   339   -3.63    -3.10  -24.7   7.93e-78\n6 Bill depth (… Gentoo …  -3.44   0.169   339   -3.77    -3.11  -20.3   1.59e-60\n\n\nThese comparisons can be plotted or presented in a table using code very similar to what we used for the estimated means. The plot below also includes a dotted line indicating zero difference, which can be used as a visual indicator for whether comparisons are statistically significant.\n\npenguin_pairs %&gt;%\n  ggplot(aes(x = estimate, y = contrast, xmin = lower.CL, xmax = upper.CL)) +\n  geom_vline(xintercept = 0, linetype = \"dotted\") +\n  geom_errorbar(width = 0.5) +\n  geom_point() +\n  scale_y_discrete(limits = rev) +\n  facet_wrap(vars(outcome), nrow = 2, scales = \"free_x\") +\n  labs(\n    x = \"Difference in means\", \n    y = \"Contrast\",\n    caption = \"Error bars show 95% confidence interval for difference in mean.\"\n  )"
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "",
    "text": "Missing data refers to data which was intended to have been collected but was not, and is a common scenario in biomedical and social science research. Correctly analysing datasets that have missing data requires extra care and consideration to produce correct results.\nThe best method for dealing with missing data depends on the underlying process causing the missingness. There is no single approach that is always the best, and the terminology commonly used to describe missingness doesn’t directly relate to what analysis methods perform best. In this blog post I’ll begin by defining the commonly used but unhelpful classifications for missing data mechanisms (you’ll see them everywhere, so you might as well know what they mean) and then explain how to describe missing data processes using causal diagrams (DAGs) and provide some references to help choose an analysis based on that."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#mcar-mar-mnar-or-nah",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#mcar-mar-mnar-or-nah",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "MCAR, MAR, MNAR or nah?",
    "text": "MCAR, MAR, MNAR or nah?\nThe most common classification for missing data mechanisms is as either Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR). This classification is based on concepts first introduced in Rubin (1976). These terms are widespread, but can be confusing when first encountered because they do not mean quite what you might expect them to mean. I’ll define them briefly here, but the goal is to not have to focus specifically about which of these mechanisms apply to your data, or use this classification to guide your analytical choices.\nData is considered to be Missing Completely at Random (MCAR) if the probability of being missing does not depend on the values of any of the variables in the data, whether those values are missing or observed (Little & Rubin, 2014, p. 12). In other words, the missingness cannot be related to the research question of interest (Lee et al., 2021).\nData is Missing at Random (MAR) if the probability of being missing depends only on data that was observed (Little & Rubin, 2014, p. 12). Most commonly this refers to a situation where a variable with incomplete data has probability of being missing which relates to completely-observed variables, but there are other possibilities which fit this definition too1. This term is misleading if encountered without context, as a plain-language interpretation suggests it may mean something more like MCAR. However, MCAR is a stricter requirement than MAR. If data is MCAR, it is also considered MAR.\nData is Missing Not at Random (MNAR) if it’s not Missing at Random (Little & Rubin, 2014, p. 12). At least that’s relatively straightforward. In other words, the probability of data being missing may be related to what the missing values would have been, had we observed them. It is sometimes implied that MNAR data is a lost cause for statistical analysis, but later in this post we will see specific examples of MNAR mechanisms where common statistical procedures produce valid results for common questions.\nStrictly speaking, these classifications refer to the entire dataset collectively, usually consisting of many variables, some of which may have missing values and some of which may not. When there are multiple variables with missing values — a common situation in real datasets — these classifications are sometimes informally applied to specific variables with missing data, but doing so doesn’t relate to any of the mathematical guarantees about which analysis methods are applicable.\nThese missing data classifications depend on the set of available variables. For example, if additional variables (sometimes called auxiliary variables) which relate to probability of partially-observed variables having missing values are added to the dataset, the missing data mechanism may change from being MNAR to being MAR.\n\n\n\n\n\n\nHistorical aside\n\n\n\nRubin (1976) is often cited as the source of this classification system, but didn’t actually introduce the terms Missing Completely at Random or Missing Not at Random, only Missing at Random. Rubin originally defined an additional condition, Observed at Random. Data which is both Missing at Random and Observed at Random is what we would now commonly refer to as Missing Completely at Random. The term Missing Completely at Random came later, in Marini et al. (1980). This history is given in Little (2021) and was also confirmed in a Tweet by Raphael Nishimura: ‘I was curious about that too and did some digging with the authors. Rod said that “Rubin’s 1976 Biometrika paper defines MAR and OAR (observed at random) but he may not have put the two together.” Don confirmed it and added that “MCAR was first formally defined in a joint paper with Marini and Olsen, I think in 1980, in a more applied paper.”’\n\n\n\n\n\n\n\n\nIgnorable and non-ignorable missingness\n\n\n\nSometimes you may hear missing data described as “ignorable” or “non-ignorable”. These terms are also potentially misleading. “Ignorable” missing data doesn’t mean that you can just ignore the fact that you have missing data when doing an analysis. This term was defined in Rubin (1976) to mean that (1) the data is MAR; (2) the likelihood can be factorised into a part relating to the missingness probability and a part relating to the distribution of the underlying data. These provide a sufficient condition for missing data to be dealt with using likelihood-based methods.\n\n\n\nHow do I know what kind of missing data mechanism I have?\nUnfortunately, there’s no way to determine the missing data mechanism purely by looking at the data — you need to think about the process generating the data and why some of it is missing. A causal DAG is a good way to reason about the relationship between the variables and their reasons for being missing and can help you decide what analysis to use. Rather than using the DAG to determine whether your data is MCAR, MAR, or MNAR, and then use those classifications to choose an analysis, you can use the DAG directly to guide your choice of analysis method. I’ll have examples of this later in this blog.\n\n\nIf I know my missing data mechanism, what analysis should I do?\nIt depends! (I’m statistician, you should have known I would say that.) These are the standard results about what missing data methods are appropriate under different processes:\nComplete cases analysis is unbiased if the data is MCAR. This is a sufficient condition, not a necessary condition (Little, 2021): there are situations where complete cases may be valid, or at least provide a valid estimate of a particular quantity of interest, under MAR or MNAR. Depending on the amount of missing data, complete cases analysis may be inefficient (i.e., have lower power than other methods) because only cases where no variables have missing values are used in the analysis.\nMultiple imputation, likelihood-based methods and full Bayesian methods are unbiased if the data is MAR or MCAR (Little, 2021). Multiple imputation is approximately equivalent to a maximum-likelihood method if the underlying model is the same (Collins et al., 2001).\nIn some specific MNAR situations, either or both of the above methods may still be valid, depending on what you’re trying to estimate (White & Carlin, 2010)."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#some-examples-of-missing-data-mechanisms",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#some-examples-of-missing-data-mechanisms",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Some examples of missing data mechanisms",
    "text": "Some examples of missing data mechanisms\nTo make these definitions more concrete, here are some examples of missing data mechanisms that may occur in real studies.\n\nExample 1: Planned missing data\nSometimes missing data arises from a study design where some data is not collected on some participants. For example, consider a psychometric instrument with a large number of items. The items could be split up into sets which are asked at different times; for example, half at baseline and half a follow-up. Ideally this process would be randomised so each participant receives a different random split. The items which were not asked at a particular time point are missing values, and because the missingness was deliberately introduced by the experimenter using randomisation, we know that the probability of a particular item being missing is unrelated to any other variable in the study.\nThis is one of the few cases where we can be sure the data is MCAR.\n\n\nExample 2: Missingness related to confounders only\nConsider a longitudinal study where participants need to regularly travel to a site, for example a hospital, in order for their data to be recorded. Participants living in rural or remote areas may find the travel more inconvenient if they need to visit a city to participate in the study, and thus be more likely to have missing values in many variables.\nIf we know the location of all of the participants, or the missingness in the participant location is unrelated to any other study variable, this missingness mechanism would be MAR.\n\n\nExample 3: Missingness related to confounders and exposure\nThis example is drawn from Moreno-Betancur et al. (2018). Suppose we want to investigate the relationship between childhood maternal mental illness (the exposure, in epidemiologist-speak) and child behaviour in subsequent years (the outcome). In order to provide a causal estimate of the effect of this exposure, we must control for confounding variables, such as maternal alcohol consumption, smoking, and other variables relating to the child’s health.\nIt was considered likely that missingness in all variables was related to both maternal mental illness and the counfounding variables. Since child behaviour was measured at a later time than the exposure and confounding variables, it was considered unlikely to have affected missingness in the confounders or exposure.\nIt was uncertain whether missingness in child behaviour could be related to the child behaviour itself — so there are two plausible missing data mechanisms to consider in this example. Both of those plausible mechanisms are MNAR, but we will see later than one possibility is amenable for analysis and the other is not."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#making-the-missing-data-process-explicit-using-causal-diagrams-dags",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#making-the-missing-data-process-explicit-using-causal-diagrams-dags",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Making the missing data process explicit using causal diagrams (DAGs)",
    "text": "Making the missing data process explicit using causal diagrams (DAGs)\nCausal diagrams are used to represent causal relationships between variables in a study (Pearl, 1995). These relationships are shown visually using a directed acyclic graph (DAG), which is a mathematical term for a bunch of circles with arrows between them. The circles (nodes) represent variables and arrows (edges) between them represent direct causal pathways. The direction of the arrows represents the direction of causation, and a valid DAG never has a path leading from a particular point back to that same point2. If there is no path following the arrows between two variables in a DAG, there is no causal connection between them.\nThe structure of the DAG behind your data cannot be inferred just from looking at the data. It needs to come from substantive expertise about the underlying variables in the data, their relationship to each other, and how the data was collected.\nThe idea of using a DAG to represent missing data assumptions was first introduced in Mohan & Pearl (2014). To provide more practical advice for specific scenarios, Moreno-Betancur et al. (2018) introduces “canonical” causal diagrams for missing data in the setting where we want to estimate the relationship between an outcome Y and an exposure X3, adjusting for some confounding variables which may influence both the outcome and the exposure. The confounding variables may be either completely observed (Z1) or have some missing data (Z2). There may also be some unknown, unmeasured factors U which affect the exposure and the confounders.\nThe DAG corresponding to the scenario above is shown below. If you click the “Code” button you can see how it was produced in R, using the packages dagitty (Textor et al., 2017) and ggdag. You don’t need to understand R code to use and apply DAGs but it is provided as an example for those who may want to make causal diagrams for their own studies. The first part of the code describes the underlying relationships between the variables, using the same syntax used to describe regression models in R. The second part of the code sets up the visual coordinates used for plotting the DAG — an aesthetic consideration only.\n\n\nCode\ndag_nomiss &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3),\n    y = c(U = 0, Z1 = 1, Z2 = -1, X = 0, Y = 0)\n  )\n)\nggdag(dag_nomiss) + theme_dag()\n\n\n\n\n\nThis DAG is called a complete-data DAG, or c-DAG. It represents the causal relationships between the variables if all of them had been completely observed.\nTo represent possible causes of missing data, we can introduce three new variables in the DAG: MX, MY and MZ2, representing whether or not the exposure (X), outcome (Y), or partially-observed confounders (Z2) are missing. Think of them as binary indicator variables, where for example MX = 1 means that X wasn’t observed (for a particular case/participant/observation) and MX = 0 means that X was observed. Adding these variables produces a missingness DAG, or m-DAG. Moreno-Betancur et al. (2018) also considers unmeasured causes of missingness (W) which are not related to any of the substantive variables in the study but do affect the probability of missingness.\nThe variables in the DAG which aren’t prefixed with M still refer to values of the variables as if they had been completely observed — which we no longer have full access to in our real data since we only get to see the variables after the missingness mechanism has occurred. This ensures that the causal mechanism in the c-DAG still applies to the m-DAG, so the m-DAG can be produced by adding additional nodes and arrows to the c-DAG without changing any of the arrows between existing nodes.\nIf we turn the c-DAG above into an m-DAG by adding nodes for MX, MY, and MZ2 but no additional arrows, the absence of arrows to MX, MY and MZ2 would mean that there is no causal relationship between any of the variables and their missingness — in other words, the MCAR missingness mechanism. This scenario would be quite unusual in most real studies, unless the missing data was deliberately planned like our first example in the previous section.\nThis m-DAG is shown below, with an additional variable W representing unmeasured variables which affect the missing data process but are not related to any of the variables in the study. The new variables representing the missing data mechanism are shown in blue. In that MCAR scenario, the blue nodes (missing data mechanism) are completely disconnected from black nodes (relating to the main research question).\n\n\nCode\ndag_mcar &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ W,\n  MY ~ W,\n  MZ2 ~ W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_mcar %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nFigure 2 in Moreno-Betancur et al. (2018) provides 10 examples of m-DAGs, with different combinations of the following features:\n\nincomplete confounders and exposure related to missingness of other variables;\nincomplete confounders and exposure related to their own missingness;\noutcome related to missingness of other variables; and\noutcome related to its own missingness.\n\nThe first of these canonical DAGs, m-DAG A, is an example of a MAR process, where the missingness is only related to completely-observed confounders (Z1) and unmeasured variables unrelated to other variables in the study (W). This DAG describes Example 2 in the previous section, with participant location (part of Z1) causing missingness in other variables (MX, MY, MZ2). This DAG is reproduced below, with the nodes and arrows related to the missing data mechanism shown in blue:\n\n\nCode\ndag_a &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ Z1 + W,\n  MY ~ Z1 + W,\n  MZ2 ~ Z1 + W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_a %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nIn this DAG, so long as the model’s adjustment for confounders is correctly specified (a big “if”!), both complete cases analysis and multiple imputation provide valid estimates of not just the relationship between the exposure and the outcome, but also the complete distribution of the outcome. This isn’t something that should be obvious just from looking at the DAG — there’s a derivation in the supplementary materials for Moreno-Betancur et al. (2018) for this and the other DAGs, with the results summarised in Tables 1 and 2 of that paper. Those who are familiar with causal DAGs may want to know that when there is missing data, this derivation is more complex than rules like “d-separation” and cannot be done algorithmically.\nIn other missing data situations, it may be possible to obtain a valid estimate of the effect of the exposure but not, for example, the population mean of the outcome. In some situations, it may not be possible to obtain a valid estimate of either.\nThe figure below shows m-DAG E, which is an example of an MNAR process. The arrows new to this DAG which weren’t in m-DAG A are shown in red. The probability of missingness in all variables is affected by both the exposure X (which may itself have missing values) and partially-observed confounders Z2. This DAG corresponds to Example 3 in the previous section, if we believe that missingness in child behaviour is not related to the missing values of child behaviour. Perhaps surprisingly, even though this missingness process is MNAR, it is possible to obtain a valid estimate of the relationship between the exposure and the outcome using either complete cases analysis or multiple imputation. However, it is not possible to recover the overall population mean of the outcome.\n\n\nCode\ndag_e &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ Z1 + Z2 + X + W,\n  MY ~ Z1 + Z2 + X + W,\n  MZ2 ~ Z1 + Z2 + X + W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_e %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") & name %in% c(\"X\", \"Z2\") ~ \"firebrick3\",\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nThe final example in this blog is m-DAG J, which is another MNAR process. The arrows new to this DAG are shown in purple. In this example, missingness is also affected by the value of the outcome Y. This DAG is similar to the one for Example 3 in the previous section, if we believe that missingness in the outcome (child behaviour) is related to its own unobserved values. For this m-DAG, it is not possible to recover the relationship between the outcome and the exposure.\n\n\nCode\ndag_j &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ Z1 + Z2 + X + Y + W,\n  MY ~ Z1 + Z2 + X + Y + W,\n  MZ2 ~ Z1 + Z2 + X + Y + W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_j %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") & name == \"Y\" ~ \"darkorchid3\",\n      str_starts(to, \"M\") & name %in% c(\"X\", \"Z2\") ~ \"firebrick3\",\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nIf you have an m-DAG where a quantity of interest cannot be recovered, you can consider doing a sensitivity analysis using a \\(\\delta\\)-adjustment. This process is described in popular texts on missing data, e.g. van Buuren (2018) sec 9.2 (also available online) or Molenberghs et al. (2015)."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#applying-dags-to-real-missing-data-problems",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#applying-dags-to-real-missing-data-problems",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Applying DAGs to real missing data problems",
    "text": "Applying DAGs to real missing data problems\nThis section draws heavily from Lee et al. (2023) — especially Figure 1 of that paper — which provides practical guidance for data analysis using missingness DAGs.\nStart by determining the estimand of interest: the scientific quantity you would like to estimate, if you had access to the complete data.\nNext, draw a DAG representing the causal relationships between your variables and their possible reasons for being missing. This will require some thinking about which arrows are plausible and which are not, based on your understanding of the real-world relationships between variables and the way the data has been collected.\nBefore deciding on an analysis method, you must first determine whether the estimand is recoverable, i.e. whether it is possible to estimate it from the data4. One possibility is to compare your DAG to the canonical DAGs in Moreno-Betancur et al. (2018) and the recoverability results in Table 1 of that paper.\nIf your estimand is recoverable, you can proceed to deciding on an analysis method to handle the missing data. Again, the canonical DAGs are useful for this. For canonical DAGs A, B, D and E, complete cases analysis was shown to provide a valid estimate of the relationship between the outcome and the exposure. For canonical DAGs A, B, C, D and E, multiple imputation was shown to provide a valid estimate of the relationship between the outcome and the exposure.\nRandomised clinical trials provide some additional guarantees about the nature of the missing data process which observational studies do not: the assigned intervention is completely observed and known to be unrelated to baseline covariates. While it doesn’t use the m-DAG framework, Sullivan et al. (2018) considers several methods for dealing with missing data in randomised trials that may be useful for those working in that setting.\nThe papers above provide examples of situations where multiple imputation is not necessarily better than simpler methods, as well as situations where multiple imputation is required. The situations where particular missing data methods work do not neatly align with the definitions of MCAR/MAR/MNAR but by drawing a DAG and considering the results in the papers above you should be able to determine which popular methods are applicable to your data and research question.\n\nThanks to Isabella Ghement and Lachlan Cribb for providing valuable feedback which greatly improved this blog post. Last updated: 29 June 2023."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#footnotes",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#footnotes",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are a few subtly different ways of defining this concept mathematically; Seaman et al. (2013) goes into the detail for those who are interested.↩︎\nA “cycle” in mathematician-speak. Hence, directed (arrows have a direction) acyclic (can never get back to where you started) graph (set of nodes and edges).↩︎\nThat’s epidemiologist-speak for the main predictor of interest.↩︎\nThis step is analogous to determining identifiability in causal inference.↩︎"
  },
  {
    "objectID": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html",
    "href": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html",
    "title": "Plotting multiple variables at once using ggplot2 and tidyr",
    "section": "",
    "text": "In exploratory data analysis, it’s common to want to make similar plots of a number of variables at once. For example, a randomised trial may look at several outcomes, or a survey may have a large number of questions. Here is a way to achieve to plot them efficiently using R and ggplot2."
  },
  {
    "objectID": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html#pivoting-longer-turning-your-variables-into-rows",
    "href": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html#pivoting-longer-turning-your-variables-into-rows",
    "title": "Plotting multiple variables at once using ggplot2 and tidyr",
    "section": "Pivoting longer: turning your variables into rows",
    "text": "Pivoting longer: turning your variables into rows\nggplot2 doesn’t provide an easy facility to plot multiple variables at once because this is usually a sign that your data is not “tidy”. For example, in situations where you want to plot two columns on a graph as points with different colours, the two columns often really represent the same variable, and there is a hidden grouping factor which distinguishes the data points you want to colour differently. The usual answer in this scenario is that you should restructure your data before plotting it. As a bonus, it will probably be easier to analyse your data in that form too.\nLikewise, if you want to split a plot into panels (or facets, in ggplot2-speak), you must plot a single response variable, with a grouping variable to indicate which panel the data should be plotted in. The best structure for your data depends on what you’re trying to do with it, and in this situation, even if your data is in the right form for analysis, it may not be right for some of the plots you want to make.\nFortunately, restructuring your data into the right form is straightforward using the tidyr package and the pivot_longer() function. In this example, I’m going to look at some mocked-up survey data, with six questions stored in variables Q1 through Q6. The original data frame looks like this:\n\nprint(survey_data)\n\n# A tibble: 300 × 7\n   group    Q1    Q2    Q3    Q4    Q5    Q6\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 B         4     1     4     1     2     3\n 2 B         5     2     5     4     3     3\n 3 B         5     4     4     2     2     3\n 4 B         5     1     5     2     4     3\n 5 A         5     2     5     1     1     2\n 6 A         5     3     5     3     2     2\n 7 A         4     3     5     1     4     1\n 8 B         4     3     5     1     2     1\n 9 B         4     4     4     1     3     2\n10 B         4     4     5     2     5     4\n# ℹ 290 more rows\n\n\nYou can convert this into a longer data frame where the question number is stored in one column and the response is stored in a separate column:\n\nlonger_data &lt;- survey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\")\nprint(longer_data)\n\n# A tibble: 1,800 × 3\n   group question response\n   &lt;fct&gt; &lt;chr&gt;       &lt;int&gt;\n 1 B     Q1              4\n 2 B     Q2              1\n 3 B     Q3              4\n 4 B     Q4              1\n 5 B     Q5              2\n 6 B     Q6              3\n 7 B     Q1              5\n 8 B     Q2              2\n 9 B     Q3              5\n10 B     Q4              4\n# ℹ 1,790 more rows\n\n\nYou don’t even need to store the ‘long form’ data as a separate variable. If you’re not going to use the data in this form for anything else, it’s simpler to pipe the data straight into ggplot2. Here I use the facet_wrap() function to plot each question in a separate panel, so we can see the distribution of all of the questions at once:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(x = response)) +\n  geom_bar() +\n  facet_wrap(vars(question), ncol = 3) +\n  labs(x = \"Response (on a 1 to 5 scale)\", y = \"Number of respondents\")\n\n\n\n\nYou can use question as a factor anywhere else you would use a categorical variable with ggplot. For example, you can make some box plots:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(y = response, x = question)) +\n  geom_boxplot() +\n  labs(x = \"Question\", y = \"Response (on a 1 to 5 scale)\")\n\n\n\n\nThis is also a nice demonstration of how box plots are rarely the best way to present Likert scale data.\nAny other variables are retained after you call pivot_longer(), so you can e.g. compare the responses to survey questions based on a demographic variable:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(x = response, colour = group)) +\n  facet_wrap(vars(question), ncol = 3) +\n  geom_point(stat = \"count\") +\n  geom_line(stat = \"count\") +\n  labs(x = \"Response (on a 1 to 5 scale)\", y = \"Number of respondents\")\n\n\n\n\nBy default, R will sort the levels of factors alphabetically. This isn’t always what you want in this situation—often the order of the variables in your data frame has some meaning to it. The fct_inorder() function allows you to reorder levels of a factor in the order of first appearance in the file. If you use that with the column produced by pivot_longer(), the factor will be ordered by the order of the columns in the original data frame."
  },
  {
    "objectID": "post/2019/12/equivalent-to-t-test/index.html",
    "href": "post/2019/12/equivalent-to-t-test/index.html",
    "title": "Things which are equivalent to t-tests",
    "section": "",
    "text": "If you have a continuous measurement and two groups you’d like to compare based on that measurement, what’s the first statistical test that comes to mind? Chances are it’s the two-sample t-test, sometimes known as Student’s t-test. It’s typically the first statistical test taught in an introductory statistics course, it’s well known and understood, and it has good theoretical properties—so if a t-test answers your research question, you should probably use it. (Actually, in practice, you should probably use Welch’s t-test which doesn’t assume equal variance within groups. For the rest of the post, I’m only going to consider the equal variance case.)\nEvery now and again, I find a client in this situation who has done something which is… not a t-test. Here are some other ways to do the same thing which turn out to be identical to the two-sample t-test:\n\nOne-way ANOVA: since ANOVA is often taught immediately after the t-test as “what do I do if I have more than two groups”, it should perhaps be no surprise that ANOVA gives identical p-values and confidence intervals to the t-test when you are only comparing two groups. This applies to both the overall F-test and the “post-hoc” pairwise t-test, which produce identical p-values in the two group scenario.\nLinear regression with an indicator variable: by this I mean an explanatory variable which takes one numerical value for the first group and a different numerical value for the second group. For example, 0 for the first group and 1 for the second group. This also turns out to be equivalent to a t-test. If you’ve seen how ANOVA is implemented by most software as a linear model with indicator variables for categories, this might not surprise you either.\nLinear regression with the indicator variable as the outcome and the measurement as the explanatory variable: as backwards as this may sound, this will also produce exactly the same p-value as a t-test. The hypothesis test for linear regression slope is the same regardless of which variable is the outcome and which is the predictor, although the regression equation is usually different.\nCorrelation with an indicator variable: this is also equivalent to a t-test. This follows from the above two because the hypothesis test for correlation being equal to zero is equivalent to the hypothesis test for linear regression slope being equal to zero.\nLogistic regression with the group variable as the outcome and the continuous variable as a predictor: this is not quite the same as a t-test. However, if the measurements of the two groups are normally distributed, then the t-test and logistic regression are asymptotically equivalent—meaning that for a sufficiently large sample size, they will give the same p-values. The two methods also answer different research questions: the t-test is for a difference in means between two groups while logistic regression estimates the odds ratio of having a positive outcome (being in a particular group) for a given increase in the continuous predictor. The choice between the two methods should be based on which variable is your outcome and which variable is your explanatory variable; or whether you would prefer to discuss an odds ratio or a difference in means.\nPairwise comparisons from an ANOVA model with more than two groups are also not quite equivalent to a t-test. What’s the difference? Both use a test statistic that has a t distribution calculated from the same difference in means, but the ANOVA pairwise comparisons will have better power because they use a pooled standard deviation from all groups (which, if the assumption of equal standard deviation in every group is true, will be more a precise estimate) and a t distribution with more degrees of freedom (which can make a big difference with a small sample size). As the sample size increases, the advantage of ANOVA over individual t-tests diminishes.\nBonus: The Mann-Whitney test (or Wilcoxon rank sum test) is equivalent to proportional odds logistic regression. (This claim is made in Frank Harrell’s Regression Modeling Strategies text; unlike the other examples above, I haven’t proved or read a proof of this equivalence.)"
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html",
    "href": "post/2019/12/understanding-levels-variation/index.html",
    "title": "Understanding levels of variation and mixed models",
    "section": "",
    "text": "Data that has some kind of hierarchical structure to it is very common in many fields, but is rarely discussed in introductory statistics courses. Terms used to describe this kind of data include hierarchical data, multi-level data, longitudinal data, split-plot designs or repeated measures designs. Statistical models used for these types of data include mixed-effects models (often abbreviated to just mixed models), repeated measures ANOVA and generalised estimating equations (GEEs).\nAll of these terms and models arose from different contexts, but they share a common feature. Observations are not independent, as many classical statistical methods assume, and there is structure to the dependence which can be incorporated into a statistical model.\nUsing a statistical model which doesn’t account for the hierarchical nature of this data will give incorrect results."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#common-examples-of-hierarchical-data",
    "href": "post/2019/12/understanding-levels-variation/index.html#common-examples-of-hierarchical-data",
    "title": "Understanding levels of variation and mixed models",
    "section": "Common examples of hierarchical data",
    "text": "Common examples of hierarchical data\nData with two levels of variation often arise when multiple measurements are made on the same units of observation. In the case of designed experiments, treatments may also be assigned to different levels of the hierarchy. Factors are commonly described as within-subject (varying at the lowest level) or between-subject (varying at a higher level). Some examples:\n\nMeasurements made on the same people at several points in time. This is often called longitudinal data. In this example, time would be a within-subject factor and most other variables of interest—e.g., treatment, age or gender—would be between-subject.\nMeasurements made at different depths of a number of rock core samples. In this example, the depths would be a within-subject factor and the location where the sample was obtained would be a between-subject factor.\nAssigning different treatments to different legs, arms or eyes of a number of people. For example, one eye may be given a new drug and the other eye a placebo. In this example, treatment is a within-subject factor.\nA split-plot experiment in agriculture: splitting plots of land into sections, planting different crops in each section, and using different irrigation methods on different plots. In this example, variety is a within-plot factor and irrigation is a between-plot factor.\nA split-mouth design in dentistry: assigning different treatments to different parts of participants’ mouths.\n\nIt is possible to have more than two levels of variation. Some examples from different fields of research:\n\nStudents within classrooms within schools.\nRepeated surveys administered to individuals within organisations.\nGlands within lesions within patients.\nBlocks of land divided into plots divided into subplots.\n\nIn situations with multiple levels, it is common to describe variables based on the level at which they are measured or assigned, e.g. student-level, classroom-level or school-level."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#random-effects",
    "href": "post/2019/12/understanding-levels-variation/index.html#random-effects",
    "title": "Understanding levels of variation and mixed models",
    "section": "Random effects",
    "text": "Random effects\nMulti-level data is commonly modelled using mixed-effects models, which get their name because they have both fixed effects and random effects. Fixed effects are the kind of explanatory variables you may be used to in ANOVA or linear regression: you would like to directly estimate the effect of these variables on your outcome. For example: treatment (drug or placebo) and time; crop variety and irrigation; depth and location of rock core samples. In these examples, the random effects would be the variables which group together correlated observations: participants in a trial; plots of land; rock core samples.\nHere are two different ways to think about random effects:\n\nRandom effects are factors where the individual levels are random samples from a larger population, or can be thought of in this way.\nRandom effects are factors where you don’t care about the actual effect they have on your outcome, just their ability to account for correlation between observations.\n\nFor a random effect, instead of estimating the effect of each specific level of the factor (e.g. each individual in a study), the model estimates the variance explained by that factor. This is sometimes reported as the proportion of variation explained at each level, e.g. 63% of variance was at the individual level.\nRandom effects can be “nested” inside other random effects if there are more than two levels of variation. For example, “classroom within school” could be specified as a random effect."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#simpler-analysis-options",
    "href": "post/2019/12/understanding-levels-variation/index.html#simpler-analysis-options",
    "title": "Understanding levels of variation and mixed models",
    "section": "Simpler analysis options",
    "text": "Simpler analysis options\nIt is sometimes possible to simplify an analysis if there are no variables which distinguish individuals at a particular level. For example, consider an experiment in which treatments were randomly assigned to litters of pigs but measurements were made on individual pigs, with no pig-level variables (e.g. sex) of interest in the analysis. In this situation, analysing litter averages would be a simpler analysis providing the same results as the mixed model.\nAnother common situation is when there is a single within-subjects factor with two levels, for example before and after measurements. This kind of design can be analysed with a paired t-test or Wilcoxon signed rank test.\nAs a practical consideration, random effects work best when they have a reasonable number of different levels. It may be better to treat a factor which is conceptually random as a fixed effect instead in some cases; e.g. if you are studying students from two or three schools, school should probably be a fixed effect rather than a random effect. There are also situations where this approach is not appropriate.\nIf in doubt about how to analyse your multi-level data, consult a statistician."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#study-design-considerations",
    "href": "post/2019/12/understanding-levels-variation/index.html#study-design-considerations",
    "title": "Understanding levels of variation and mixed models",
    "section": "Study design considerations",
    "text": "Study design considerations\nEffects at the lowest level of the hierarchy (e.g. within-subject) are usually estimated more precisely than effects at higher levels (e.g. between-subject). Or equivalently, tests of within-subject effects tend to be more powerful than tests of between-subject effects. One intuition about this is that there are more observations at the lowest level (e.g. number of subplots) than there are at higher levels (e.g. number of plots). Another way to look at this is that for within-subject factors, each individual unit of observation is effectively their own control."
  },
  {
    "objectID": "post/2020/03/beautiful-bar-charts-ggplot/index.html",
    "href": "post/2020/03/beautiful-bar-charts-ggplot/index.html",
    "title": "Making beautiful bar charts with ggplot",
    "section": "",
    "text": "Bar charts (or bar graphs) are commonly used, but they’re also a simple type of graph where the defaults in ggplot leave a lot to be desired. This is a step-by-step description of how I’d go about improving them, describing the thought processess along the way. Every plot is different and the decisions you make need to reflect the message you’re trying to convey, so don’t treat this post as a recipe, treat it as some points to consider—and hopefully, a few tips that will help you achieve the look you want in your own plots.\nFor this blog post, I’m going to use the number of seats won by each political party in the 2018 Victorian state election as an example. This data was obtained from the Victorian Electoral Commission. Victorians may remember this election being described as a “Danslide”, where Labor, led by Premier Daniel Andrews, won a clear majority of seats.\nHere’s the data as an R command you can paste if you want to try making these plots yourself:\n\nelection_data &lt;- tribble(\n                    ~party, ~seats_won,\n       \"Australian Greens\",          3,\n  \"Australian Labor Party\",         55,\n                 \"Liberal\",         21,\n           \"The Nationals\",          6,\n        \"Other Candidates\",          3\n)\n\nThe first decision to make when you’re thinking of making a bar chart is whether you’d be better off using a different type of plot entirely. Bar charts are most suitable for displaying counts, percentages or other quantities where zero has a special meaning. If you make a bar chart, your axis should always start at zero, or the area of the bar gives a misleading visual impression. Other ways of representing data, such as box plots or points with error bars, may be more appropriate for quantities where zero is not an important reference point. If you are representing time series data (repeated observations made over time), a continuous line (perhaps with points at the the times where observations were made) is almost always better than a sequence of bars.\nThe second decision to make is which axis to put the categorical variable and which axis to put the numerical variable. Having the categories on the y axis often works best. It gives you more space when you have either a large number of categories or categories with long labels.\nA lot of software either makes it more difficult to put categories on the y axis, or requires you to change a setting away from the default. This used to be the case for ggplot, but as of version 3.3.0, you just need to tell it which variable goes on which axis, and it will figure out the rest:\n\nggplot(election_data,\n       aes(x = seats_won, y = party)) +\n  geom_col()\n\n\n\n\nThe bar chart above is a good starting point, but quite a few things could be improved. The order of the categories is a bit odd: from top to bottom, it’s in reverse alphabetical order. This is the default in ggplot, but it is almost never what you want. The easiest way to change this is to give the option limits = rev to the y axis scale (this is also new in ggplot version 3.3.0):\n\nggplot(election_data,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_y_discrete(limits = rev)\n\n\n\n\nIn this particular case, alphabetical ordering isn’t the best choice. It’s often best to order categories from most common to least common, or from most to least in the variable you’re displaying (e.g. most to least seats won). There are two functions in the forcats package which can help with this: fct_infreq orders the level of a factor by how frequently they occur in the data, and fct_reorder orders the level of a factor by the values of a different variable.\n\nelection_data_sorted &lt;- election_data %&gt;%\n  mutate(party = fct_reorder(party, seats_won, .desc = TRUE))\n\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_y_discrete(limits = rev)\n\n\n\n\nIf I was doing exploratory data analysis, or making a quick plot to show a colleague, I might stop at this point. But there is still plenty of room for improvement. To start with, the axis labels are the variable names in our data frame, which is better than no labels at all, but are usually too brief or jargon-laden for a wider audience. A good plot can be interpreted clearly with as little supporting information as possible—remember that a reader’s eye will be drawn to a large, colourful figure and ignore the paragraphs of text you’ve written describing the full context.\nI’ve also changed the origin of the x axis so the bars are hard against the axis. Putting a blank space to the left of zero on a bar chart is something I’ve only ever seen in ggplot. It’s caused by ggplot’s standard rule of adding 10% padding on either side of the biggest and smallest values plotted. You can turn it off by setting the expand option on the x axis scale.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\")\n\n\n\n\nThe grey background with white gridlines is a very distinctive ggplot default. Sometimes it works well—it can reduce the visual noise of gridlines in complex plots—but in this case I would normally opt for a simpler. I often use theme_bw or theme_minimal.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw()\n\n\n\n\nThe gridlines on the x axis are useful guides to the eye, but for a categorical variable with only a few categories, the gridlines only introduce clutter. They can be removed using the theme function.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\nThe default dark grey bars look a bit drab. You can choose a colour and give it as an option to geom_col. If you’re like me, you’ll probably try a couple of wrong things first: either passing the colour inside aes() (won’t work, because the colour will be interpreted as data to plot) or using colour instead of fill (which changes the border colour instead). You can either use a hexadecimal colour code (like in HTML), or one of a number of built-in colour names.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col(fill = \"darkorchid\") +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\nIn this context, it would be conventional to colour the bars based on the party being represented. Here I’ve set fill = party, then given a manual scale for the fill based on party colours found on Wikipedia. I also disabled the legend, which isn’t necessary in this instance—the party names are already next to the bars. I’ve also added a title and a caption indicating the source of the data; these wouldn’t normally be included in an academic publication but are a very good idea for a plot which might be copied out of context.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party, fill = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                               \"Australian Greens\", \"Other Candidates\"),\n                    values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                               \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"off\")\n\n\n\n\nDepending on what you’re trying to show with the graph, you may want to add annotations beyond what is contained in the original data. For example, you could add a dashed line at 44 seats indicating the number required for a party to form a majority government (which could slightly misleading, since the Liberal and National parties govern in coalition). In ggplot, you can pass specific data to any geom_ function; in this example I’m using geom_vline to draw the dashed line and geom_text to draw the label:\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party, fill = party)) +\n  geom_vline(xintercept = 44, linetype = 2, colour = \"grey20\") +\n  geom_text(x = 45, y = 4, label = \"majority of\\nparliament\", \n            hjust = 0, size = 11 * 0.8 / .pt, colour = \"grey20\") +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                               \"Australian Greens\", \"Other Candidates\"),\n                    values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                               \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"off\")\n\n\n\n\nFinally, another good option for representing the same type of data as a bar chart is a line with a point at the end. The point draws the eye to the end of the line, which is the actual value being represented. You can create this in ggplot by using a geom_segment to draw the line segment and geom_point to draw the point. You also need to give xend and yend for geom_segment to work—it’s a general function for drawing line segments, not a specific functon for creating this type of plot.\n\nggplot(election_data_sorted,\n       aes(x = seats_won,\n           xend = 0,\n           y = party,\n           yend = party,\n           colour = party)) +\n  geom_segment() +\n  geom_point() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_colour_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                                 \"Australian Greens\", \"Other Candidates\"),\n                      values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                                 \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme(legend.position = \"off\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cameron Patrick",
    "section": "",
    "text": "I am a PhD student with the Clinical Epidemiology & Biostatistics Unit (CEBU) at the Murdoch Children’s Research Institute in Melbourne, Australia. My research is about estimating complier average causal effects (CACE) in clinical trials in the presence of missing data. My supervisors are Professor Katherine Lee, Associate Professor Margarita Moreno-Betancur and Dr Thomas Sullivan.\nI also work part-time as a statistical consultant at the University of Melbourne Statistical Consulting Centre (SCC), where I’ve worked since 2017. This involves working with researchers at the University and clients outside the University. We can assist with all stages of quantitative research: designing experiments or surveys, planning an appropriate analysis, analysing data, making graphs and communicating results clearly in papers or reports. As part of my role at the SCC, I also teach the short course Introduction to R and Reproducible Research twice per year, alongside Dr Sandy Clarke-Errey.\nI’ve been on the Statistical Society of Australia Victoria and Tasmania Branch Council since 2021.\nYou can find me on Twitter or email me at cameron.patrick@unimelb.edu.au"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Cameron Patrick",
    "section": "Education",
    "text": "Education\nPhD (in progress) University of Melbourne, 2023—\nMaster of Science (Mathematics and Statistics) University of Melbourne, 2016\nBachelor of Science (Pure Mathematics) University of Western Australia, 2009"
  },
  {
    "objectID": "index.html#recent-blog-posts",
    "href": "index.html#recent-blog-posts",
    "title": "Cameron Patrick",
    "section": "Recent blog posts",
    "text": "Recent blog posts"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Understanding missing data mechanisms using causal DAGs\n\n\n\n\n\n\n\nmissing-data\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nFitting many statistical models at once using dplyr\n\n\n\n\n\n\n\nr\n\n\nstatistical-models\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nMaking beautiful bar charts with ggplot\n\n\n\n\n\n\n\nr\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2020\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nThings which are equivalent to t-tests\n\n\n\n\n\n\n\nstatistics\n\n\nt-test\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding levels of variation and mixed models\n\n\n\n\n\n\n\nstatistics\n\n\nmixed-models\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nPlotting multiple variables at once using ggplot2 and tidyr\n\n\n\n\n\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\nNo matching items"
  }
]
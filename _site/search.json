[
  {
    "objectID": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html",
    "href": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html",
    "title": "Plotting multiple variables at once using ggplot2 and tidyr",
    "section": "",
    "text": "In exploratory data analysis, it’s common to want to make similar plots of a number of variables at once. For example, a randomised trial may look at several outcomes, or a survey may have a large number of questions. Here is a way to achieve to plot them efficiently using R and ggplot2."
  },
  {
    "objectID": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html#pivoting-longer-turning-your-variables-into-rows",
    "href": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html#pivoting-longer-turning-your-variables-into-rows",
    "title": "Plotting multiple variables at once using ggplot2 and tidyr",
    "section": "Pivoting longer: turning your variables into rows",
    "text": "Pivoting longer: turning your variables into rows\nggplot2 doesn’t provide an easy facility to plot multiple variables at once because this is usually a sign that your data is not “tidy”. For example, in situations where you want to plot two columns on a graph as points with different colours, the two columns often really represent the same variable, and there is a hidden grouping factor which distinguishes the data points you want to colour differently. The usual answer in this scenario is that you should restructure your data before plotting it. As a bonus, it will probably be easier to analyse your data in that form too.\nLikewise, if you want to split a plot into panels (or facets, in ggplot2-speak), you must plot a single response variable, with a grouping variable to indicate which panel the data should be plotted in. The best structure for your data depends on what you’re trying to do with it, and in this situation, even if your data is in the right form for analysis, it may not be right for some of the plots you want to make.\nFortunately, restructuring your data into the right form is straightforward using the tidyr package and the pivot_longer() function. In this example, I’m going to look at some mocked-up survey data, with six questions stored in variables Q1 through Q6. The original data frame looks like this:\n\nprint(survey_data)\n\n# A tibble: 300 × 7\n   group    Q1    Q2    Q3    Q4    Q5    Q6\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 B         4     1     4     1     2     3\n 2 B         5     2     5     4     3     3\n 3 B         5     4     4     2     2     3\n 4 B         5     1     5     2     4     3\n 5 A         5     2     5     1     1     2\n 6 A         5     3     5     3     2     2\n 7 A         4     3     5     1     4     1\n 8 B         4     3     5     1     2     1\n 9 B         4     4     4     1     3     2\n10 B         4     4     5     2     5     4\n# ℹ 290 more rows\n\n\nYou can convert this into a longer data frame where the question number is stored in one column and the response is stored in a separate column:\n\nlonger_data &lt;- survey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\")\nprint(longer_data)\n\n# A tibble: 1,800 × 3\n   group question response\n   &lt;fct&gt; &lt;chr&gt;       &lt;int&gt;\n 1 B     Q1              4\n 2 B     Q2              1\n 3 B     Q3              4\n 4 B     Q4              1\n 5 B     Q5              2\n 6 B     Q6              3\n 7 B     Q1              5\n 8 B     Q2              2\n 9 B     Q3              5\n10 B     Q4              4\n# ℹ 1,790 more rows\n\n\nYou don’t even need to store the ‘long form’ data as a separate variable. If you’re not going to use the data in this form for anything else, it’s simpler to pipe the data straight into ggplot2. Here I use the facet_wrap() function to plot each question in a separate panel, so we can see the distribution of all of the questions at once:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(x = response)) +\n  geom_bar() +\n  facet_wrap(vars(question), ncol = 3) +\n  labs(x = \"Response (on a 1 to 5 scale)\", y = \"Number of respondents\")\n\n\n\n\nYou can use question as a factor anywhere else you would use a categorical variable with ggplot. For example, you can make some box plots:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(y = response, x = question)) +\n  geom_boxplot() +\n  labs(x = \"Question\", y = \"Response (on a 1 to 5 scale)\")\n\n\n\n\nThis is also a nice demonstration of how box plots are rarely the best way to present Likert scale data.\nAny other variables are retained after you call pivot_longer(), so you can e.g. compare the responses to survey questions based on a demographic variable:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(x = response, colour = group)) +\n  facet_wrap(vars(question), ncol = 3) +\n  geom_point(stat = \"count\") +\n  geom_line(stat = \"count\") +\n  labs(x = \"Response (on a 1 to 5 scale)\", y = \"Number of respondents\")\n\n\n\n\nBy default, R will sort the levels of factors alphabetically. This isn’t always what you want in this situation—often the order of the variables in your data frame has some meaning to it. The fct_inorder() function allows you to reorder levels of a factor in the order of first appearance in the file. If you use that with the column produced by pivot_longer(), the factor will be ordered by the order of the columns in the original data frame."
  },
  {
    "objectID": "post/2019/12/equivalent-to-t-test/index.html",
    "href": "post/2019/12/equivalent-to-t-test/index.html",
    "title": "Things which are equivalent to t-tests",
    "section": "",
    "text": "If you have a continuous measurement and two groups you’d like to compare based on that measurement, what’s the first statistical test that comes to mind? Chances are it’s the two-sample t-test, sometimes known as Student’s t-test. It’s typically the first statistical test taught in an introductory statistics course, it’s well known and understood, and it has good theoretical properties—so if a t-test answers your research question, you should probably use it. (Actually, in practice, you should probably use Welch’s t-test which doesn’t assume equal variance within groups. For the rest of the post, I’m only going to consider the equal variance case.)\nEvery now and again, I find a client in this situation who has done something which is… not a t-test. Here are some other ways to do the same thing which turn out to be identical to the two-sample t-test:\n\nOne-way ANOVA: since ANOVA is often taught immediately after the t-test as “what do I do if I have more than two groups”, it should perhaps be no surprise that ANOVA gives identical p-values and confidence intervals to the t-test when you are only comparing two groups. This applies to both the overall F-test and the “post-hoc” pairwise t-test, which produce identical p-values in the two group scenario.\nLinear regression with an indicator variable: by this I mean an explanatory variable which takes one numerical value for the first group and a different numerical value for the second group. For example, 0 for the first group and 1 for the second group. This also turns out to be equivalent to a t-test. If you’ve seen how ANOVA is implemented by most software as a linear model with indicator variables for categories, this might not surprise you either.\nLinear regression with the indicator variable as the outcome and the measurement as the explanatory variable: as backwards as this may sound, this will also produce exactly the same p-value as a t-test. The hypothesis test for linear regression slope is the same regardless of which variable is the outcome and which is the predictor, although the regression equation is usually different.\nCorrelation with an indicator variable: this is also equivalent to a t-test. This follows from the above two because the hypothesis test for correlation being equal to zero is equivalent to the hypothesis test for linear regression slope being equal to zero.\nLogistic regression with the group variable as the outcome and the continuous variable as a predictor: this is not quite the same as a t-test. However, if the measurements of the two groups are normally distributed, then the t-test and logistic regression are asymptotically equivalent—meaning that for a sufficiently large sample size, they will give the same p-values. The two methods also answer different research questions: the t-test is for a difference in means between two groups while logistic regression estimates the odds ratio of having a positive outcome (being in a particular group) for a given increase in the continuous predictor. The choice between the two methods should be based on which variable is your outcome and which variable is your explanatory variable; or whether you would prefer to discuss an odds ratio or a difference in means.\nPairwise comparisons from an ANOVA model with more than two groups are also not quite equivalent to a t-test. What’s the difference? Both use a test statistic that has a t distribution calculated from the same difference in means, but the ANOVA pairwise comparisons will have better power because they use a pooled standard deviation from all groups (which, if the assumption of equal standard deviation in every group is true, will be more a precise estimate) and a t distribution with more degrees of freedom (which can make a big difference with a small sample size). As the sample size increases, the advantage of ANOVA over individual t-tests diminishes.\nBonus: The Mann-Whitney test (or Wilcoxon rank sum test) is equivalent to proportional odds logistic regression. (This claim is made in Frank Harrell’s Regression Modeling Strategies text; unlike the other examples above, I haven’t proved or read a proof of this equivalence.)"
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html",
    "href": "post/2019/12/understanding-levels-variation/index.html",
    "title": "Understanding levels of variation and mixed models",
    "section": "",
    "text": "Data that has some kind of hierarchical structure to it is very common in many fields, but is rarely discussed in introductory statistics courses. Terms used to describe this kind of data include hierarchical data, multi-level data, longitudinal data, split-plot designs or repeated measures designs. Statistical models used for these types of data include mixed-effects models (often abbreviated to just mixed models), repeated measures ANOVA and generalised estimating equations (GEEs).\nAll of these terms and models arose from different contexts, but they share a common feature. Observations are not independent, as many classical statistical methods assume, and there is structure to the dependence which can be incorporated into a statistical model.\nUsing a statistical model which doesn’t account for the hierarchical nature of this data will give incorrect results."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#common-examples-of-hierarchical-data",
    "href": "post/2019/12/understanding-levels-variation/index.html#common-examples-of-hierarchical-data",
    "title": "Understanding levels of variation and mixed models",
    "section": "Common examples of hierarchical data",
    "text": "Common examples of hierarchical data\nData with two levels of variation often arise when multiple measurements are made on the same units of observation. In the case of designed experiments, treatments may also be assigned to different levels of the hierarchy. Factors are commonly described as within-subject (varying at the lowest level) or between-subject (varying at a higher level). Some examples:\n\nMeasurements made on the same people at several points in time. This is often called longitudinal data. In this example, time would be a within-subject factor and most other variables of interest—e.g., treatment, age or gender—would be between-subject.\nMeasurements made at different depths of a number of rock core samples. In this example, the depths would be a within-subject factor and the location where the sample was obtained would be a between-subject factor.\nAssigning different treatments to different legs, arms or eyes of a number of people. For example, one eye may be given a new drug and the other eye a placebo. In this example, treatment is a within-subject factor.\nA split-plot experiment in agriculture: splitting plots of land into sections, planting different crops in each section, and using different irrigation methods on different plots. In this example, variety is a within-plot factor and irrigation is a between-plot factor.\nA split-mouth design in dentistry: assigning different treatments to different parts of participants’ mouths.\n\nIt is possible to have more than two levels of variation. Some examples from different fields of research:\n\nStudents within classrooms within schools.\nRepeated surveys administered to individuals within organisations.\nGlands within lesions within patients.\nBlocks of land divided into plots divided into subplots.\n\nIn situations with multiple levels, it is common to describe variables based on the level at which they are measured or assigned, e.g. student-level, classroom-level or school-level."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#random-effects",
    "href": "post/2019/12/understanding-levels-variation/index.html#random-effects",
    "title": "Understanding levels of variation and mixed models",
    "section": "Random effects",
    "text": "Random effects\nMulti-level data is commonly modelled using mixed-effects models, which get their name because they have both fixed effects and random effects. Fixed effects are the kind of explanatory variables you may be used to in ANOVA or linear regression: you would like to directly estimate the effect of these variables on your outcome. For example: treatment (drug or placebo) and time; crop variety and irrigation; depth and location of rock core samples. In these examples, the random effects would be the variables which group together correlated observations: participants in a trial; plots of land; rock core samples.\nHere are two different ways to think about random effects:\n\nRandom effects are factors where the individual levels are random samples from a larger population, or can be thought of in this way.\nRandom effects are factors where you don’t care about the actual effect they have on your outcome, just their ability to account for correlation between observations.\n\nFor a random effect, instead of estimating the effect of each specific level of the factor (e.g. each individual in a study), the model estimates the variance explained by that factor. This is sometimes reported as the proportion of variation explained at each level, e.g. 63% of variance was at the individual level.\nRandom effects can be “nested” inside other random effects if there are more than two levels of variation. For example, “classroom within school” could be specified as a random effect."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#simpler-analysis-options",
    "href": "post/2019/12/understanding-levels-variation/index.html#simpler-analysis-options",
    "title": "Understanding levels of variation and mixed models",
    "section": "Simpler analysis options",
    "text": "Simpler analysis options\nIt is sometimes possible to simplify an analysis if there are no variables which distinguish individuals at a particular level. For example, consider an experiment in which treatments were randomly assigned to litters of pigs but measurements were made on individual pigs, with no pig-level variables (e.g. sex) of interest in the analysis. In this situation, analysing litter averages would be a simpler analysis providing the same results as the mixed model.\nAnother common situation is when there is a single within-subjects factor with two levels, for example before and after measurements. This kind of design can be analysed with a paired t-test or Wilcoxon signed rank test.\nAs a practical consideration, random effects work best when they have a reasonable number of different levels. It may be better to treat a factor which is conceptually random as a fixed effect instead in some cases; e.g. if you are studying students from two or three schools, school should probably be a fixed effect rather than a random effect. There are also situations where this approach is not appropriate.\nIf in doubt about how to analyse your multi-level data, consult a statistician."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#study-design-considerations",
    "href": "post/2019/12/understanding-levels-variation/index.html#study-design-considerations",
    "title": "Understanding levels of variation and mixed models",
    "section": "Study design considerations",
    "text": "Study design considerations\nEffects at the lowest level of the hierarchy (e.g. within-subject) are usually estimated more precisely than effects at higher levels (e.g. between-subject). Or equivalently, tests of within-subject effects tend to be more powerful than tests of between-subject effects. One intuition about this is that there are more observations at the lowest level (e.g. number of subplots) than there are at higher levels (e.g. number of plots). Another way to look at this is that for within-subject factors, each individual unit of observation is effectively their own control."
  },
  {
    "objectID": "post/2020/03/beautiful-bar-charts-ggplot/index.html",
    "href": "post/2020/03/beautiful-bar-charts-ggplot/index.html",
    "title": "Making beautiful bar charts with ggplot",
    "section": "",
    "text": "Bar charts (or bar graphs) are commonly used, but they’re also a simple type of graph where the defaults in ggplot leave a lot to be desired. This is a step-by-step description of how I’d go about improving them, describing the thought processess along the way. Every plot is different and the decisions you make need to reflect the message you’re trying to convey, so don’t treat this post as a recipe, treat it as some points to consider—and hopefully, a few tips that will help you achieve the look you want in your own plots.\nFor this blog post, I’m going to use the number of seats won by each political party in the 2018 Victorian state election as an example. This data was obtained from the Victorian Electoral Commission. Victorians may remember this election being described as a “Danslide”, where Labor, led by Premier Daniel Andrews, won a clear majority of seats.\nHere’s the data as an R command you can paste if you want to try making these plots yourself:\n\nelection_data &lt;- tribble(\n                    ~party, ~seats_won,\n       \"Australian Greens\",          3,\n  \"Australian Labor Party\",         55,\n                 \"Liberal\",         21,\n           \"The Nationals\",          6,\n        \"Other Candidates\",          3\n)\n\nThe first decision to make when you’re thinking of making a bar chart is whether you’d be better off using a different type of plot entirely. Bar charts are most suitable for displaying counts, percentages or other quantities where zero has a special meaning. If you make a bar chart, your axis should always start at zero, or the area of the bar gives a misleading visual impression. Other ways of representing data, such as box plots or points with error bars, may be more appropriate for quantities where zero is not an important reference point. If you are representing time series data (repeated observations made over time), a continuous line (perhaps with points at the the times where observations were made) is almost always better than a sequence of bars.\nThe second decision to make is which axis to put the categorical variable and which axis to put the numerical variable. Having the categories on the y axis often works best. It gives you more space when you have either a large number of categories or categories with long labels.\nA lot of software either makes it more difficult to put categories on the y axis, or requires you to change a setting away from the default. This used to be the case for ggplot, but as of version 3.3.0, you just need to tell it which variable goes on which axis, and it will figure out the rest:\n\nggplot(election_data,\n       aes(x = seats_won, y = party)) +\n  geom_col()\n\n\n\n\nThe bar chart above is a good starting point, but quite a few things could be improved. The order of the categories is a bit odd: from top to bottom, it’s in reverse alphabetical order. This is the default in ggplot, but it is almost never what you want. The easiest way to change this is to give the option limits = rev to the y axis scale (this is also new in ggplot version 3.3.0):\n\nggplot(election_data,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_y_discrete(limits = rev)\n\n\n\n\nIn this particular case, alphabetical ordering isn’t the best choice. It’s often best to order categories from most common to least common, or from most to least in the variable you’re displaying (e.g. most to least seats won). There are two functions in the forcats package which can help with this: fct_infreq orders the level of a factor by how frequently they occur in the data, and fct_reorder orders the level of a factor by the values of a different variable.\n\nelection_data_sorted &lt;- election_data %&gt;%\n  mutate(party = fct_reorder(party, seats_won, .desc = TRUE))\n\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_y_discrete(limits = rev)\n\n\n\n\nIf I was doing exploratory data analysis, or making a quick plot to show a colleague, I might stop at this point. But there is still plenty of room for improvement. To start with, the axis labels are the variable names in our data frame, which is better than no labels at all, but are usually too brief or jargon-laden for a wider audience. A good plot can be interpreted clearly with as little supporting information as possible—remember that a reader’s eye will be drawn to a large, colourful figure and ignore the paragraphs of text you’ve written describing the full context.\nI’ve also changed the origin of the x axis so the bars are hard against the axis. Putting a blank space to the left of zero on a bar chart is something I’ve only ever seen in ggplot. It’s caused by ggplot’s standard rule of adding 10% padding on either side of the biggest and smallest values plotted. You can turn it off by setting the expand option on the x axis scale.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\")\n\n\n\n\nThe grey background with white gridlines is a very distinctive ggplot default. Sometimes it works well—it can reduce the visual noise of gridlines in complex plots—but in this case I would normally opt for a simpler. I often use theme_bw or theme_minimal.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw()\n\n\n\n\nThe gridlines on the x axis are useful guides to the eye, but for a categorical variable with only a few categories, the gridlines only introduce clutter. They can be removed using the theme function.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\nThe default dark grey bars look a bit drab. You can choose a colour and give it as an option to geom_col. If you’re like me, you’ll probably try a couple of wrong things first: either passing the colour inside aes() (won’t work, because the colour will be interpreted as data to plot) or using colour instead of fill (which changes the border colour instead). You can either use a hexadecimal colour code (like in HTML), or one of a number of built-in colour names.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col(fill = \"darkorchid\") +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\nIn this context, it would be conventional to colour the bars based on the party being represented. Here I’ve set fill = party, then given a manual scale for the fill based on party colours found on Wikipedia. I also disabled the legend, which isn’t necessary in this instance—the party names are already next to the bars. I’ve also added a title and a caption indicating the source of the data; these wouldn’t normally be included in an academic publication but are a very good idea for a plot which might be copied out of context.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party, fill = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                               \"Australian Greens\", \"Other Candidates\"),\n                    values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                               \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"off\")\n\n\n\n\nDepending on what you’re trying to show with the graph, you may want to add annotations beyond what is contained in the original data. For example, you could add a dashed line at 44 seats indicating the number required for a party to form a majority government (which could slightly misleading, since the Liberal and National parties govern in coalition). In ggplot, you can pass specific data to any geom_ function; in this example I’m using geom_vline to draw the dashed line and geom_text to draw the label:\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party, fill = party)) +\n  geom_vline(xintercept = 44, linetype = 2, colour = \"grey20\") +\n  geom_text(x = 45, y = 4, label = \"majority of\\nparliament\", \n            hjust = 0, size = 11 * 0.8 / .pt, colour = \"grey20\") +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                               \"Australian Greens\", \"Other Candidates\"),\n                    values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                               \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"off\")\n\n\n\n\nFinally, another good option for representing the same type of data as a bar chart is a line with a point at the end. The point draws the eye to the end of the line, which is the actual value being represented. You can create this in ggplot by using a geom_segment to draw the line segment and geom_point to draw the point. You also need to give xend and yend for geom_segment to work—it’s a general function for drawing line segments, not a specific functon for creating this type of plot.\n\nggplot(election_data_sorted,\n       aes(x = seats_won,\n           xend = 0,\n           y = party,\n           yend = party,\n           colour = party)) +\n  geom_segment() +\n  geom_point() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_colour_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                                 \"Australian Greens\", \"Other Candidates\"),\n                      values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                                 \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme(legend.position = \"off\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cameron Patrick",
    "section": "",
    "text": "I am a PhD student with the Clinical Epidemiology & Biostatistics Unit (CEBU) at the Murdoch Children’s Research Institute in Melbourne, Australia. My research is about estimating complier average causal effects (CACE) in clinical trials in the presence of missing data. My supervisors are Professor Katherine Lee, Associate Professor Margarita Moreno-Betancur and Dr Thomas Sullivan.\nI also work part-time as a statistical consultant at the University of Melbourne Statistical Consulting Centre (SCC), where I’ve worked since 2017. This involves working with researchers at the University and clients outside the University. We can assist with all stages of quantitative research: designing experiments or surveys, planning an appropriate analysis, analysing data, making graphs and communicating results clearly in papers or reports. As part of my role at the SCC, I also teach the short course Introduction to R and Reproducible Research twice per year, alongside Dr Sandy Clarke-Errey.\nI’ve been on the Statistical Society of Australia Victoria and Tasmania Branch Council since 2021.\nYou can find me on Twitter or email me at cameron.patrick@unimelb.edu.au"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Cameron Patrick",
    "section": "Education",
    "text": "Education\nPhD (in progress) University of Melbourne, 2023—\nMaster of Science (Mathematics and Statistics) University of Melbourne, 2016\nBachelor of Science (Pure Mathematics) University of Western Australia, 2009"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Fitting many statistical models at once using dplyr\n\n\n\n\n\n\n\nr\n\n\nstatistical-models\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nMaking beautiful bar charts with ggplot\n\n\n\n\n\n\n\nr\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2020\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nThings which are equivalent to t-tests\n\n\n\n\n\n\n\nstatistics\n\n\nt-test\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding levels of variation and mixed models\n\n\n\n\n\n\n\nstatistics\n\n\nmixed-models\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nPlotting multiple variables at once using ggplot2 and tidyr\n\n\n\n\n\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2023/06/dplyr-fitting-multiple-models-at-once/index.html",
    "href": "post/2023/06/dplyr-fitting-multiple-models-at-once/index.html",
    "title": "Fitting many statistical models at once using dplyr",
    "section": "",
    "text": "One common task in applied statistics is to fit and interpret a number of statistical models at once. For example, fitting a model with the same structure to a number of different outcome or explanatory variables, or fitting several models with different structure to the same data. Here are some examples of how I usually do this, using features that were introduced with dplyr version 1.1.0.\nFor this demonstration, we’ll be using the R packages dplyr, tidyr, ggplot2 (all of which are included in the tidyverse), as well as gt for making tables, emmeans for obtaining estimated means and comparisons, and performance for model-checking.\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(emmeans)\nlibrary(performance)\n\nWe’ll be using the Palmer Penguins data collected at Palmer Station, Antarctica, made available by Dr Kristen Gorman, and conveniently accessible in R using the palmerpenguins package. This dataset contains measurements on a number of penguins of different species on different islands.\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSuppose we want to compare bill length, bill depth, flipper length, and body mass between species. We could manually run a separate model for each, but here’s a way to to automate the process. As with many things in R, the trick to doing this easily is to get the data in long form, with the outcomes stacked on top of each other, and a variable indicating which outcome is being measured.\nThe pivot_longer() function from tidyr gets the data into this format. I’ve also taken an extra step of recoding the “outcome” variable to give more descriptive labels. This isn’t required but it will make the tables and plots that we make later look nicer.\n\npenguins_long &lt;- penguins %&gt;%\n  pivot_longer(\n    c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),\n    names_to = \"outcome\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    outcome = fct_inorder(outcome),\n    outcome = fct_recode(\n      outcome,\n      \"Bill length (mm)\" = \"bill_length_mm\",\n      \"Bill depth (mm)\" = \"bill_depth_mm\",\n      \"Flipper length (mm)\" = \"flipper_length_mm\",\n      \"Body mass (g)\" = \"body_mass_g\"\n    )\n  )\n\nLooking at the first few rows of this data frame, you can see we now have four rows for each penguin, one for each type of measurement:\n\nhead(penguins_long)\n\n# A tibble: 6 × 6\n  species island    sex     year outcome              value\n  &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie  Torgersen male    2007 Bill length (mm)      39.1\n2 Adelie  Torgersen male    2007 Bill depth (mm)       18.7\n3 Adelie  Torgersen male    2007 Flipper length (mm)  181  \n4 Adelie  Torgersen male    2007 Body mass (g)       3750  \n5 Adelie  Torgersen female  2007 Bill length (mm)      39.5\n6 Adelie  Torgersen female  2007 Bill depth (mm)       17.4\n\n\nWe can use group_by() and summarise() from the dplyr package to group the rows by outcome, and then “summarise” these groups of rows down to a single row containing a statistical model for each outcome. This makes use of a couple of R tricks: ‘list columns’, a column in a data frame that contains a more complex object than the standard R data types (numeric, character, etc); and the new pick() verb which returns a data frame containing selected columns (in this case, all of them) within the group that’s being operated on.\n\npenguin_models &lt;- penguins_long %&gt;%\n  group_by(outcome) %&gt;%\n  summarise(\n    model = list(\n      lm(value ~ species, data = pick(everything()))\n    )\n  )\n\nYou can see the resulting data from contains four rows, one for each outcome, and a statistical model (“lm”) for each:\n\nprint(penguin_models)\n\n# A tibble: 4 × 2\n  outcome             model \n  &lt;fct&gt;               &lt;list&gt;\n1 Bill length (mm)    &lt;lm&gt;  \n2 Bill depth (mm)     &lt;lm&gt;  \n3 Flipper length (mm) &lt;lm&gt;  \n4 Body mass (g)       &lt;lm&gt;  \n\n\nIdeally we would also do a visual check of model assumptions. One way to do this is something like the code below, which saves a residual plot for each model to an image file, which can be inspected later. It uses the check_model() function in the performance package to generate the plots. The generated residual plots aren’t shown here, but they all look fine.\n\nwalk2(\n  penguin_models$outcome, \n  penguin_models$model,\n  ~ ggsave(\n    paste0(.x, \".png\"),\n    plot(check_model(.y, check = c(\"pp_check\", \"linearity\",\n                                   \"homogeneity\", \"qq\"))),\n    width = 12,\n    height = 9\n  )\n)\n\nOnce we’ve fitted the models, we can obtain quantities of interest from them. In this example we’ll look at estimated means for each species, p-values testing the hypothesis that all species means are equal (against at least one pair of means being different), and comparisons (differences in means) between all pairs of species.\nThe reframe() function from dplyr allows us to run some code that produces a data frame on each model and stack the results on top of each other. We can use the emmeans() function from the emmeans package to obtain estimated marginal means and as_tibble() to convert the result into a data frame. The rowwise(outcome) at the start tells reframe() that we want to call emmeans() separately for each row of the data frame (i.e., each outcome model), and preserve the outcome variable in the result.\n\npenguin_means &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    emmeans(model, \"species\") %&gt;%\n      as_tibble()\n  )\n\nThe first few rows of the resulting data frame look like this:\n\nhead(penguin_means)\n\n# A tibble: 6 × 7\n  outcome          species   emmean     SE    df lower.CL upper.CL\n  &lt;fct&gt;            &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Bill length (mm) Adelie      38.8 0.241    339     38.3     39.3\n2 Bill length (mm) Chinstrap   48.8 0.359    339     48.1     49.5\n3 Bill length (mm) Gentoo      47.5 0.267    339     47.0     48.0\n4 Bill depth (mm)  Adelie      18.3 0.0912   339     18.2     18.5\n5 Bill depth (mm)  Chinstrap   18.4 0.136    339     18.2     18.7\n6 Bill depth (mm)  Gentoo      15.0 0.101    339     14.8     15.2\n\n\nWe can use ggplot() to present the results visually. The plot shows that there’s a huge variation between species in means of all of these measurements.\n\npenguin_means %&gt;%\n  ggplot(aes(x = emmean, y = species, xmin = lower.CL, xmax = upper.CL)) +\n  geom_errorbar(width = 0.5) +\n  geom_point() +\n  scale_y_discrete(limits = rev) +\n  facet_wrap(vars(outcome), nrow = 2, scales = \"free_x\") +\n  labs(\n    x = \"Mean\", \n    y = \"Species\",\n    caption = \"Error bars show 95% confidence interval for mean.\"\n  )\n\n\n\n\nThe gt() function can be used to produce a nice table of results. The code shown below combines the lower.CL and upper.CL columns to produce a single column with the confidence interval, and separately specifies fewer decimal places for body mass than the other measures. The group_by() function before gt() results in a table sub-heading for each outcome. You could easily change this to group_by(species) to arrange the results by species.\n\npenguin_means %&gt;%\n  group_by(outcome) %&gt;%\n  gt() %&gt;%\n  fmt_number(c(emmean, SE, lower.CL, upper.CL),\n             decimals = 1, use_seps = FALSE) %&gt;%\n  fmt_number(c(emmean, SE, lower.CL, upper.CL),\n             rows = outcome == \"Body mass (g)\",\n             decimals = 0, use_seps = FALSE) %&gt;%\n  fmt_number(df, decimals = 0) %&gt;%\n  cols_align(\"left\", species) %&gt;%\n  cols_merge_range(lower.CL, upper.CL, sep = \", \") %&gt;%\n  cols_label(\n    species = \"Species\",\n    emmean = \"Mean\",\n    lower.CL = \"95% Confidence Interval\"\n  )\n\n\n\n\n\n  \n    \n    \n      Species\n      Mean\n      SE\n      df\n      95% Confidence Interval\n    \n  \n  \n    \n      Bill length (mm)\n    \n    Adelie\n38.8\n0.2\n339\n38.3, 39.3\n    Chinstrap\n48.8\n0.4\n339\n48.1, 49.5\n    Gentoo\n47.5\n0.3\n339\n47.0, 48.0\n    \n      Bill depth (mm)\n    \n    Adelie\n18.3\n0.1\n339\n18.2, 18.5\n    Chinstrap\n18.4\n0.1\n339\n18.2, 18.7\n    Gentoo\n15.0\n0.1\n339\n14.8, 15.2\n    \n      Flipper length (mm)\n    \n    Adelie\n190.0\n0.5\n339\n188.9, 191.0\n    Chinstrap\n195.8\n0.8\n339\n194.2, 197.4\n    Gentoo\n217.2\n0.6\n339\n216.0, 218.4\n    \n      Body mass (g)\n    \n    Adelie\n3701\n38\n339\n3627, 3775\n    Chinstrap\n3733\n56\n339\n3623, 3843\n    Gentoo\n5076\n42\n339\n4994, 5158\n  \n  \n  \n\n\n\n\nWe can do similar to produce an overall “F” test for each model, testing the hypothesis that all species have equal means for a particular measure against the hypothesis that at least one pair of means is different. The joint_tests() function in emmeans does this.\n\npenguin_tests &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    joint_tests(model) %&gt;%\n      as_tibble()\n  )\n\nThis is what the resulting data frame looks like:\n\nprint(penguin_tests)\n\n# A tibble: 4 × 6\n  outcome             `model term`   df1   df2 F.ratio   p.value\n  &lt;fct&gt;               &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Bill length (mm)    species          2   339    411. 2.69e- 91\n2 Bill depth (mm)     species          2   339    360. 1.51e- 84\n3 Flipper length (mm) species          2   339    595. 1.35e-111\n4 Body mass (g)       species          2   339    344. 2.89e- 82\n\n\nAgain, this can be presented in a table using gt():\n\npenguin_tests %&gt;%\n  gt() %&gt;%\n  fmt_number(F.ratio, decimals = 1) %&gt;%\n  fmt_number(p.value, decimals = 3) %&gt;%\n  cols_merge_range(df1, df2, sep = \", \") %&gt;%\n  sub_small_vals(p.value, threshold = 0.001) %&gt;%\n  cols_label(\n    outcome = \"Outcome\",\n    `model term` = \"Predictor\",\n    df1 = \"df\",\n    F.ratio = \"F\",\n    p.value = \"p-value\"\n  )\n\n\n\n\n\n  \n    \n    \n      Outcome\n      Predictor\n      df\n      F\n      p-value\n    \n  \n  \n    Bill length (mm)\nspecies\n2, 339\n410.6\n&lt;0.001\n    Bill depth (mm)\nspecies\n2, 339\n359.8\n&lt;0.001\n    Flipper length (mm)\nspecies\n2, 339\n594.8\n&lt;0.001\n    Body mass (g)\nspecies\n2, 339\n343.6\n&lt;0.001\n  \n  \n  \n\n\n\n\nFinally, we often want to obtain comparisons between particular estimated quantities. In this example we use the emmeans package again for this, this time using the pairs() function to produce comparisons between all pairs of species.\n\npenguin_pairs &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    emmeans(model, \"species\") %&gt;%\n      pairs(infer = TRUE, reverse = TRUE, adjust = \"none\") %&gt;%\n      as_tibble()\n  )\n\nThe first few rows of the data frame look like this:\n\nhead(penguin_pairs)\n\n# A tibble: 6 × 9\n  outcome       contrast estimate    SE    df lower.CL upper.CL t.ratio  p.value\n  &lt;fct&gt;         &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Bill length … Chinstr…  10.0    0.432   339    9.19    10.9    23.2   4.23e-72\n2 Bill length … Gentoo …   8.71   0.360   339    8.01     9.42   24.2   5.33e-76\n3 Bill length … Gentoo …  -1.33   0.447   339   -2.21    -0.449  -2.97  3.18e- 3\n4 Bill depth (… Chinstr…   0.0742 0.164   339   -0.248    0.396   0.453 6.50e- 1\n5 Bill depth (… Gentoo …  -3.36   0.136   339   -3.63    -3.10  -24.7   7.93e-78\n6 Bill depth (… Gentoo …  -3.44   0.169   339   -3.77    -3.11  -20.3   1.59e-60\n\n\nThese comparisons can be plotted or presented in a table using code very similar to what we used for the estimated means. The plot below also includes a dotted line indicating zero difference, which can be used as a visual indicator for whether comparisons are statistically significant.\n\npenguin_pairs %&gt;%\n  ggplot(aes(x = estimate, y = contrast, xmin = lower.CL, xmax = upper.CL)) +\n  geom_vline(xintercept = 0, linetype = \"dotted\") +\n  geom_errorbar(width = 0.5) +\n  geom_point() +\n  scale_y_discrete(limits = rev) +\n  facet_wrap(vars(outcome), nrow = 2, scales = \"free_x\") +\n  labs(\n    x = \"Difference in means\", \n    y = \"Contrast\",\n    caption = \"Error bars show 95% confidence interval for difference in mean.\"\n  )"
  },
  {
    "objectID": "index.html#recent-blog-posts",
    "href": "index.html#recent-blog-posts",
    "title": "Cameron Patrick",
    "section": "Recent blog posts",
    "text": "Recent blog posts"
  }
]
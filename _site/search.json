[
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html",
    "href": "post/2023/07/quarto-thesis-formatting/index.html",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "",
    "text": "I decided to write my PhD thesis in Quarto. Between my idiosyncratic standards for the PDF output and the university’s rigid requirements for what a PhD thesis ought to look like, I knew that some degree of tweaking the formatting settings would be required. I’ve never really used Quarto for PDF output before, but since I’m fairly confident using LaTeX, I figured “how hard could it possibly be”.\nThis post is a slightly elaborated version of the notes I made for myself during the process, shared in the hope that it may be useful for others who want to use Quarto to write reports or theses with idiosyncratic styling.\nThe one piece of this post that I haven’t seen spelled out anywhere else on the web is the section on additional front matter before and after the table of contents, which took a bit of LaTeX trickery to achieve. The solution I found is better than the other alternatives I’ve seen (for my purposes, anyway), because it allows you to write the front matter in Markdown instead of LaTeX and have it present in the HTML output as well as PDF."
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#basic-principles",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#basic-principles",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "Basic principles",
    "text": "Basic principles\n\nStart by creating a “Quarto book project” using your favourite IDE (RStudio)\nIt’s probably a good idea to use git and periodically commit as you mess with stuff, but I just YOLO’ed it until I had something I was happy with\nYou should probably set up renv so your package versions are tracked and reproducible. renv::init()\nMost of the configuration begins in editing the YAML file, _quarto.yml\nThe most basic but also most important options are documented in the Quarto PDF Options section of the Quarto manual\nAchieving more complex goals requires writing chunks of LaTeX code which you reference in the YAML (or sometimes include inline in your chapter .qmd files)\nSome of what I’m explaining here wasn’t documented and required reading the Quarto source code. I am not to held responsible for any voided warranty or code that stops working with the next release of Quarto\nI’m assuming you have some familiarity with YAML, Markdown, and LaTeX; but not necessarily with Quarto. In other words, I’m writing for me-last-week. Good luck"
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#bibliography-workflow",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#bibliography-workflow",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "Bibliography workflow",
    "text": "Bibliography workflow\nThanks to Brenton Wiernik and Matthijs Hollanders on Twitter for pointing me in the right direction here.\n\nUse Zotero and the Better BibTeX for Zotero plugin\n(Aside: the ZotFile plugin may also be useful if/when I exceed my free Zotero PDF storage limit, although I like Zotero enough I may well just give them cash for cloud storage.)\nSet a sensible schema for naming your reference keys in the Better BibTeX settings. I’m using auth.lower + year + shorttitle.lower which generates keys like @rubin1974estimatingcausaleffects\nSet Zotero “quick copy” format to Better BibTeX Citation Key and set it to use Markdown, so you can hit Command-Shift-C to copy the citation in Markdown format\nExport using the Better CSL YAML plugin (you can also set it to automatically update when your Zotero library changes), or use the R Better BibTeX package to get an automatically updated bibliography export every time you knit your document (I haven’t tried this yet)\nGrab a CSL file for your preferred bibliography and citation format and copy it into your Quarto project\nAdd bibliography and csl top-level keys to your YAML:\n\nbibliography: references.yaml\ncsl: apa.csl\nI’ve also edited my references.qmd Markdown file which generates the bibliography so it has a ragged right margin (i.e. left justified, not full justified):\n# References {.unnumbered}\n\n\\begingroup\n\\raggedright\n::: {#refs}\n:::\n\\endgroup"
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#yeeting-the-rstudio-visual-editor",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#yeeting-the-rstudio-visual-editor",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "Yeeting the RStudio visual editor",
    "text": "Yeeting the RStudio visual editor\nIf you’re like me, and can’t stand the RStudio visual Markdown editor but accidentally clicked the “Visual editor” check box when creating the project, change your YAML to say:\neditor: source\nInstead of editor: visual. Or vice versa, if you prefer the visual editor."
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#easy-pdf-tweaks-only-yaml-needed",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#easy-pdf-tweaks-only-yaml-needed",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "Easy PDF tweaks (only YAML needed)",
    "text": "Easy PDF tweaks (only YAML needed)\nAll of these go inside the format: → pdf: chunk of the YAML. They’re documented in the Quarto manual but that’s very long and sometimes unclear (partly because Quarto can produce PDF output both via LaTeX and via HTML, and different options apply to each). Here’s what I cared about enough to mess with:\n\nLaTeX document class: Quarto’s default templates will take advantage of the KOMA Script classes if you use those, and they seem to make some customisation nicer than the LaTeX packages I’ve used previously. So documentclass: scrbook for two-sided or documentclass: scrreprt for single-sided (note the lack of “o” in “scrreprt”)\nYou can change the name of the PDF file produced, e.g. output-file: \"FirstnameLastname_thesis.pdf\"\nSet keep-tex: true so you can take a squiz at the generated TeX file. I found this helped when figuring out what I needed to change to bend the output to my whims\nEnable Table of Contents, List of Figures, List of Tables; toc-depth of three means that the Table of Contents will show up to \\subsection (or ### headings in Markdown):\n\n    toc: true\n    toc-depth: 3\n    toc-title: \"Table of contents\"\n    lof: true\n    lot: true\n\nSection numbering: number-depth appears to count differently from toc-depth, so the below will have numbered \\subsection (###) but not \\subsubsection (####):\n\n    number-sections: true\n    number-depth: 2\n\nPaper size: papersize: a4 or you’ll get US Letter\nMargins: you can either use the KOMA Script options (see below) which use some kind of fancy formula to derive margins, or you can specify options to the LaTeX geometry package in your YAML. Below is what I’m currently using, copied from my MSc thesis LaTeX preamble. I can’t remember what the header and footer bits do exactly. heightrounded helps prevent “underfull vbox” warnings by making sure the text height is a multiple of the line height\n\n    geometry:\n      - inner=3cm\n      - outer=4cm\n      - top=3cm\n      - bottom=4cm\n      - headsep=22pt\n      - headheight=11pt\n      - footskip=33pt\n      - ignorehead\n      - ignorefoot\n      - heightrounded\n\nIndented paragraphs vs space between paragraphs: indent: true or indent: false. You can use KOMA Script options for greater control over the indent or skip distances but the defaults look fine to me\nSpacing between lines: you can use e.g. linestretch: 1.25 or linestretch: 1.5 to get increased line spacing\nAs far as I can tell, you can’t choose between full justified and ragged right (left justified) in the YAML, you’ll need to add LaTeX commands to the preamble (see below)\nFont size: set the base font size used for body text, e.g. fontsize: 11pt\nI prefer the XeLaTeX engine: pdfengine: xelatex because…\nIf you’re using the XeLaTeX engine, you can specify any (Unicode, TrueType/OpenType) system font if you don’t like the standard Computer Modern Roman look that screams “my document was made with TeX”. The TeX Gyre Math font families provide OpenType math fonts compatible with XeLaTeX that fit well with Times and Palatino, amongst others. Here’s an example of using Times New Roman and other common Microsoft fonts, alongside TeX Gyre Termes Math which provides mathematical symbols which blend in nicely with these fonts:\n\n    mainfont: \"Times New Roman\"\n    sansfont: \"Arial\"\n    monofont: \"Courier New\"\n    mathfont: \"TeX Gyre Termes Math\""
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#koma-script-options-fonts-headings-headers-and-footers",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#koma-script-options-fonts-headings-headers-and-footers",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "KOMA Script options: fonts, headings, headers, and footers",
    "text": "KOMA Script options: fonts, headings, headers, and footers\nThe KOMA Script manual is comprehensive but inscrutable, and it takes a bit of messing around to find out where to put the options anyway.\nTo set these options, you’ll need to add them to what Quarto calls the LaTeX header (which I’ve always known as the LaTeX preamble). That means you need to add a line like include-in-header: include-in-header.tex to the PDF format options in your YAML, and then add the code here to a file called include-in-header.tex.\nHere are a few things I did here:\n\nMake the headings the same font as the rest of the document, instead of sans serif: \\addtokomafont{disposition}{\\rmfamily}\nRestore the classic LaTeX chapter headings that are two lines, the first saying e.g. “Chapter 2” on a line before the chapter title: \\KOMAoptions{chapterprefix=true,appendixprefix=true}\nSmaller fonts for headings: \\KOMAoptions{headings=small}\nIf you’re fussy about the size of the indent or spacing between paragraphs, you would do that here too\nIf you prefer left-justified (ragged right margin) instead of full-justified, you can do that here: \\raggedright\nHeader and footer fonts: normal upright font (instead of slanted, the KOMA Script default) and a smaller size. See this handy web site for more info on LaTeX relative font size commands like \\footnotesize.\n\n\\setkomafont{pageheadfoot}{\\normalfont\\normalcolor\\footnotesize}\n\\setkomafont{pagenumber}{\\normalfont\\normalcolor\\footnotesize}\n\nHeaders and footers! For this we’ll need the scrlayer-scrpage package, and commands like \\lefoot[]{} where “l” starts for left (there’s also “c” and “r”), “e” starts for even page (there’s also “o”), “foot” for footer (there’s also “head”). Inside the square brackets you put what you want on “plain” pages (start of chapter) and inside the curly braces you put what want on pages with a running-head (inside chapters). Here’s an example that gives output similar to many technical books: (1) centred page numbers (\\pagemark) in the footer on the first page of a chapter; (2) page numbers on the outside edge of the header inside chapters; (3) chapter and section titles (“running heads”, \\leftmark and \\rightmark) on the inside margins:\n\n\\usepackage{scrlayer-scrpage}\n\\lefoot[]{}\n\\cefoot[\\pagemark]{}\n\\refoot[]{}\n\\lofoot[]{}\n\\cofoot[\\pagemark]{}\n\\rofoot[]{}\n\\lehead[]{\\pagemark}\n\\cehead[]{}\n\\rehead[]{\\leftmark}\n\\lohead[]{\\rightmark}\n\\cohead[]{}\n\\rohead[]{\\pagemark}\n\nHeaders and footers! If you’re using single-sided output, beware: all of your pages will be considered “odd”, for some odd reason."
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#title-page",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#title-page",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "Title page",
    "text": "Title page\nMy university has a specific requirement for the formatting of title pages, and even if it didn’t I’d still want to change the default Quarto title page because it’s kind of ugly.\nTo do this, we will once again need to write some LaTeX code. This time, rather than just adding extra code to the preamble, we’ll be replacing some of the built-in Quarto Pandoc templates. You can see what templates are available and what they contain by looking at the Quarto source code (!). Ignore the seductively-named title.tex because to edit the title page you will need to replace before-body.tex. Start by adding before-body.tex to the template-partials list under format: pdf: in your YAML (yeesh).\n    template-partials:\n      - before-body.tex\nThen, in before-body.tex, we’ll add the code to create the title page. You’ll notice that this isn’t quite normal LaTeX, there’s some kind of crazy templating language going on, with directives inside pairs of $ signs. I’m not aware of any documentation on this, I just pieced it together from reading other Quarto templates.\nThe first few lines (copied from the standard Quarto template) enable front matter mode in LaTeX, which causes pages to be numbered in roman numerals instead of normal (arabic) digits. Later on, \\mainmatter will cause the page numbering to restart from 1. The remainder of the code is LaTeX code to generate the title page, with a bit of cleverness to pull the title and author information out of the YAML. Instead of just using \\maketitle built into LaTeX, we’ll make our own title page from scratch.\n$if(has-frontmatter)$\n\\frontmatter\n$endif$\n$if(title)$\n\\cleardoublepage\n\\thispagestyle{empty}\n{\\centering\n\\hbox{}\\vskip 0cm plus 1fill\n{\\Huge\\bfseries $title$ \\par}\n$if(subtitle)$\n\\vspace{3ex}\n{\\Large\\bfseries $subtitle$ \\par}\n$endif$\n\\vspace{12ex}\n$for(by-author)$\n{\\Large\\bfseries $by-author.name.literal$ \\par}\n\\vspace{3ex}\n{\\Large ORCID: $by-author.orcid$ \\par}\n\\vskip 0cm plus 2fill\n{\\bfseries\\large Doctor of Philosophy \\par}\n\\vspace{3ex}\n{\\bfseries\\large $date$ \\par}\n\\vspace{12ex}\n$for(by-author.affiliations)$%\n$if(it.department)$%\n{\\bfseries\\large $it.department$ \\par}\n\\vspace{3ex}\n$endif$%\n{\\bfseries\\large $it.name$ \\par}\n$endfor$$endfor$%\n\\vspace{12ex}\n{\\small Submitted in total fulfilment of the requirements\nof the degree of Doctor of Philosophy \\par}\n}\n$endif$\nTo go along with this, you’ll also need to provide appropriate information in the book: section of the YAML:\nbook:\n  title: \"PhD thesis main title\"\n  subtitle: \"Clever subtitle probably with a pun\"\n  author: \n    - name: \"Cameron James Patrick\"\n      orcid: \"0000-0002-4677-535X\"\n      affiliations:\n        - name: \"The University of Melbourne\"\n          department: \"Department of Paediatrics\"\n  date: \"01 June 2026\"\n  date-format: \"MMMM YYYY\"\n\n\n\nHere’s one I prepared earlier. Or will eventually have prepared, or something. Still have to write the damned thing."
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#sec-additional-front-matter",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#sec-additional-front-matter",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "Additional front matter before and after the table of contents",
    "text": "Additional front matter before and after the table of contents\nAccording to my university, a PhD thesis needs to contain the following items, in this order: title page, abstract, authorship declaration, preface, acknowledgements, table of contents, list of tables, list of figures, abbreviations; the body of the thesis; references; and finally appendices.\nUnfortunately, the default Quarto template places the table of contents immediately after the title page. To change this, we’ll need to edit another “template partial”, this time toc.tex. In the YAML:\nformat:\n  pdf:\n    template-partials:\n      - before-body.tex\n      - toc.tex\nThen in toc.tex:\n$if(toc)$\n$if(toc-title)$\n\\renewcommand*\\contentsname{$toc-title$}\n$endif$\n$if(colorlinks)$\n\\hypersetup{linkcolor=$if(toccolor)$$toccolor$$else$$endif$}\n$endif$\n\\setcounter{tocdepth}{$toc-depth$}\n$endif$\n\\renewcommand*\\listfigurename{List of figures}\n\\renewcommand*\\listtablename{List of tables}\nIf you compare the above to the standard Quarto toc.tex template you’ll see that I’ve removed a heap of code. Some of that code was for presentations but most importantly I removed the \\tableofcontents, \\listoffigures and \\listoftables commands which actually produce the table of contents and lists of figures and tables. (I’ve also added some bonus code to make the “List of figures” and “List of tables” headings in sentence case instead of title case, all modern-like.)\nWe’ll also need to stop Quarto from switching from \\frontmatter to \\mainmatter at around this point. I couldn’t find the partial template that was responsible for this, so instead I added these two lines of LaTeX to the end of before-body.tex:\n\\let\\mainmatterreal\\mainmatter\n\\let\\mainmatter\\relax\nThe above code effectively neuters the \\mainmatter command, until we’re ready to bring it back to life.\nNow we can write the ‘chapters’ that make our extra front matter. These can be written just like normal Quarto chapters, though I added {.unnumbered .unlisted} to the end of the chapter headings so they don’t get chapter numbers and aren’t included in the table of contents.\nAt the end of the last section before the table of contents should appear — acknowledgements.qmd in my case — add the following LaTeX code to generate table of contents (and list of tables and list of figures, if desired):\n\\tableofcontents\n\\listoftables\n\\listoffigures\nFinally, at the end of the last front matter section — abbreviations.qmd in my case — add this code to return the \\mainmatter command to life and run it, causing the main section of the document to have ordinary page numbers, starting from 1 again:\n\\let\\mainmatter\\mainmatterreal\n\\mainmatter"
  },
  {
    "objectID": "post/2023/07/quarto-thesis-formatting/index.html#html-output",
    "href": "post/2023/07/quarto-thesis-formatting/index.html#html-output",
    "title": "Some Quarto PDF formatting tips, with particular reference to thesis writing",
    "section": "HTML output",
    "text": "HTML output\nOne nice thing about Quarto is that it can produce multiple output formats from the same input. I find the HTML output particularly convenient for on-screen previewing. I haven’t messed with the appearance of the HTML output much, but here’s the YAML chunk I’m using at the moment:\nformat:\n  html:\n    theme: simplex\n    fontsize: 1.2em\n    linestretch: 1.7\n    mainfont: Helvetica Neue, Helvetica, Arial, sans\n    monofont: Cascadia Mono, Menlo, Consolas, Courier New, Courier\n    backgroundcolor: \"white\"\n    fontcolor: \"black\"\n    knitr:\n      opts_chunk:\n        dev: \"ragg_png\"\nI will probably end up writing a CSS stylesheet at some point as a form of procrastination.\n\nSetting the output directory\nYou can change the directory that the HTML and other outputs are saved to. This may be useful if, for example, you want to use GitHub Pages to publish your document as a web site. GitHub Pages expects the HTML to either be in the repository root or a “docs” subdirectory:\nproject:\n  type: book\n  output-dir: \"docs\""
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html",
    "href": "post/2023/07/logit-rd-rr/index.html",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "",
    "text": "When summarising the effect of a treatment or intervention on an outcome measured on a continuous scale, it’s almost ubiquitous to represent the Average Treatment Effect (ATE) in the form of a difference of means between the treatment and control groups. When the outcome is binary (yes or no, event occurred or event did not occur), there are three commonly-used measures. The risk difference, also known as absolute risk reduction or difference of proportions, is an absolute measure of effect size: the proportion in one group minus the proportion in the other group. The relative risk, also known as the risk ratio, is a relative measure of effect size: the proportion in one group divided by the proportion in the other group. Finally, there is the odds ratio, another relative measure of effect: the odds in one group divided by the odds in the other group. The odds ratio is the odd one out in a few ways, and a bit more controversial.\nIn the first half of this post, I’ll dive into the controversy. In the middle, I’ll summarise the arguments using a meme. In the second half, I’ll work through an example of estimating odds ratios, risk differences, and relative risks in a simulated example, showing R code examples and hand-calculations. I’m taking a particular interest in the application to randomised controlled trials (RCTs) but most of the issues discussed here apply more broadly.\n\n\n\n\n\n\nWhat’s an odds, anyway?\n\n\n\nAn odds is the probability of an event occurring divided by the probability of it not occurring. For example, a 50% chance corresponds to an odds of 0.5/0.5 = 1; a 75% chance corresponds to an odds of 0.75/0.25 = 3; a 25% chance corresponds to an odds of 0.25/0.75 = 1/3. Odds are common in gambling and often expressed as ratios, e.g. 1:1, 3:1 or 1:3. Odds ratios are ratios of oddses1. For example, a change from a 50% proportion to a 75% proportion would be a change from an odds of 1 to an odds of 3, representing an odds ratio of 3/1 = 3."
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#introduction",
    "href": "post/2023/07/logit-rd-rr/index.html#introduction",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "",
    "text": "When summarising the effect of a treatment or intervention on an outcome measured on a continuous scale, it’s almost ubiquitous to represent the Average Treatment Effect (ATE) in the form of a difference of means between the treatment and control groups. When the outcome is binary (yes or no, event occurred or event did not occur), there are three commonly-used measures. The risk difference, also known as absolute risk reduction or difference of proportions, is an absolute measure of effect size: the proportion in one group minus the proportion in the other group. The relative risk, also known as the risk ratio, is a relative measure of effect size: the proportion in one group divided by the proportion in the other group. Finally, there is the odds ratio, another relative measure of effect: the odds in one group divided by the odds in the other group. The odds ratio is the odd one out in a few ways, and a bit more controversial.\nIn the first half of this post, I’ll dive into the controversy. In the middle, I’ll summarise the arguments using a meme. In the second half, I’ll work through an example of estimating odds ratios, risk differences, and relative risks in a simulated example, showing R code examples and hand-calculations. I’m taking a particular interest in the application to randomised controlled trials (RCTs) but most of the issues discussed here apply more broadly.\n\n\n\n\n\n\nWhat’s an odds, anyway?\n\n\n\nAn odds is the probability of an event occurring divided by the probability of it not occurring. For example, a 50% chance corresponds to an odds of 0.5/0.5 = 1; a 75% chance corresponds to an odds of 0.75/0.25 = 3; a 25% chance corresponds to an odds of 0.25/0.75 = 1/3. Odds are common in gambling and often expressed as ratios, e.g. 1:1, 3:1 or 1:3. Odds ratios are ratios of oddses1. For example, a change from a 50% proportion to a 75% proportion would be a change from an odds of 1 to an odds of 3, representing an odds ratio of 3/1 = 3."
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#the-great-odds-ratio-debate",
    "href": "post/2023/07/logit-rd-rr/index.html#the-great-odds-ratio-debate",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "The great odds ratio debate",
    "text": "The great odds ratio debate\nA lot of noise has been made about why an odds ratio may not be a desirable summary for a treatment effect on a binary variable2. One of the biggest practical problems is that normal humans are unable to correctly interpret an odds ratio (Altman et al., 1998). But for statisticians, there are also deeper concerns to worry about.\nIn recent years, people3 have become more aware of the mathematical fact of noncollapsibility (Daniel et al., 2021; Greenland et al., 1999; Morris et al., 2022): the marginal4 odds ratio (population-level odds ratio) is not any kind of average of conditional odds ratios (individual-level odds ratios). Marginal odds ratios are the ones you might calculate directly from summary tables. Where do conditional odds ratios come from? They are artefacts of statistical models — logistic regression models, specifically. In the context of a randomised trial, these models predict the probability of an event occurring for each participant based on their treatment assignment and other variables measured prior to treatment (usually demographic characteristics, but could be any variable that is informative of the outcome)5. The second half of this post shows a worked example where every participant has the same conditional odds ratio of treatment, but the conditional odds ratio is not equal to the marginal odds ratio.\nThe problem of noncollapsibility is relevant to the analysis and reporting of randomised controlled trials. The ICH E9(R1) estimand framework for clinical trials requires a “population-level summary” be defined for each outcome without reference to method used to estimate it. In the language of causal inference, that’s describing an Average Treatment Effect (ATE), and the odds ratio isn’t one6.\nRisk differences (the difference in probability attributable to a particular treatment) and relative risks (the ratio of probabilities due to a particular treatment) avoid both the interpretability and noncollapsibility problems of odds ratios. However, statisticians are usually taught to analyse binary outcomes using logistic regression, the direct output7 of which is odds ratios, so odds ratios are ubiquitous in the research literature. Someone on Stats Twitter rediscovers all of this every few months and starts a heated argument where nobody goes away happy.\n\n\n\nSurprisingly, many statisticians prefer collapsibility.\n\n\nPersonally, I’m a fan of risk differences for communicating potential risks to an individual, ideally presented alongside the baseline level of risk. There’s some evidence that patients and clinicians find these measures easier to interpret than relative measures (Zipkin et al., 2014), especially when presented in the form of a natural frequency, like “6 in 1000 people” rather than “0.6%”.\nSo, with all those disadvantages, will anyone speak up in support of odds ratios? Should we all hang up our logistic regression hats for good?"
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#in-defense-of-logistic-regression",
    "href": "post/2023/07/logit-rd-rr/index.html#in-defense-of-logistic-regression",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "In defense of logistic regression",
    "text": "In defense of logistic regression\nStatistical analyses are often adjusted for covariates, either to reduce confounding (in observational studies) or improve power (in randomised trials). Adjusting for covariates requires specifying some kind of model. If we’re using a generalised linear model8, the mathematical form of covariate adjustment will depend on whether you are modelling log-odds (logistic regression model, where model parameters correspond to an odds ratio), probability (linear probability model, where model parameters correspond to a risk difference) or log-probability (quasi-Poisson or log-link binomial model, where model parameters correspond to a relative risk). When modelling risk differences or relative risks directly, it’s possible to end up with impossible predicted probabilities: “probabilities” which are less than zero or greater than one. Using logistic regression avoids this problem, because any real number on the log-odds scale translates to a probability between zero and one. Any odds ratio can be applied to any level of baseline risk without making mathematicians sad9.\nThere are also compelling — but not universally accepted — arguments that despite the difficulty in interpreting odds ratios, conditional odds ratios are more likely to be transportable between different levels of baseline risk than risk differences or relative risks (Doi et al., 2022; Senn, 2011). This is an empirical matter, not a mathematical one, and the evidence is not clear-cut. If we accept this, though, it is another reason to prefer logistic regression for statistical modelling. Effect size measures being “transportable” is another way of saying that the effects are closer to being additive on the scale that effect measure lives on (probability, log-probability, or log-odds). In the context of regression models, using a scale where the effect size is more transportable reduces the need for interaction terms, which can only be estimated well in large samples.\nIt’s possible to model the data using logistic regression, and then use that model to produce other quantities of interest, such as average risk differences. This approach is described in Permutt (2020), which has some of the best writing I’ve encountered in a statistics paper — being written in the form of a dialogue between a randomiser (the “causal inference” perspective) and a parameteriser (the “statistical modelling” perspective) walking through the Garden of Eden, planning to conduct and analyse a clinical trial. Does this method give us the best of both worlds: the modelling advantages of logistic regression and the interpretability advantages of risk differences and relative risks?\nPermutt (2020) also considered what information different audiences might want from the results of a clinical trial: regulator, patient, and scientist. There’s a fourth audience which I think is worth considering, only very briefly mentioned by Permutt: the meta-analyst, trying to aggregate information from multiple trials.\nPermutt argues in favour of the ATE being the main quantity of regulatory interest:\n\nThe average treatment effect should be of regulatory interest, however. The primary analysis of outcome should be of a variable that is reasonably linear in utility to the patient. Then, if and only if the average effect is positive, the ensemble of patients can be said to be better off under the test condition than under the control. This is perhaps the weakest imaginable statistical condition for approval of a drug product, but it is surely a necessary condition.\n\nPermutt notes that studies designed to be able to detect average treatment effects are unlikely to be adequate for patient-specific decision making or providing a more detailed scientific understanding.\nThe most clearly expressed counter-argument to this comes from a blog post by Frank Harrell arguing against single-number summaries for treatment effects on binary outcomes:\n\nMarginal adjusted estimates may be robust, but may not accurately estimate RD for either any patient in the RCT or for the clinical population to which RCT results are to be applied, because in effect they assume that the RCT sample is a random sample from the clinical population, something not required and never realized for RCTs.\n\nThis refers to the distinction between the population average treatment effect (PATE) and the sample average treatment effect (SATE). RCT participants are not random samples from any population, not even from the eligible pool of participants for a particular study. But the regulator’s decision that Permutt described earlier is motivated by generalising to a broader population, implicitly relying on properties of the PATE. It’s not clear to me whether there are likely to be any real-world scenarios where both (1) the practical conclusions drawn from the SATE and PATE would be different; and (2) the conditional odds ratio derived from the RCT sample is in agreement with the PATE but the risk difference SATE is not.\nI am a coward and not (yet?) willing to take a strong position in this fight, but am always sympathetic to the idea that a single number is rarely sufficient to describe scientific evidence (see also: p-values). At some point I might write another blog about Harrell’s idea of plotting the distribution of estimated patient-level treatment effects, which is intriguing, although I struggle to see the practical purpose of it. Harrell’s other writing on this topic is also worth reading:\n\nIncorrect Covariate Adjustment May Be More Correct than Adjusted Marginal Estimates makes a similar point to White et al. (2021) with a detailed simulated example;\nAssessing Heterogeneity of Treatment Effect, Estimating Patient-Specific Efficacy, and Studying Variation in Odds ratios, Risk Ratios, and Risk Differences argues in favour of the use of odds ratios for modelling, and points out the extreme difficulty of identifying heterogeneous treatment effects even in very large studies; and\nUnadjusted Odds Ratios are Conditional demonstrates noncollapsibility and argues that conditional odds ratios are more useful than marginal odds ratios.\n\nOne issue which I remain unclear about is whether randomised trials powered to detect main treatment effects are likely to provide reasonable estimates of patient-specific baseline risk — a simpler task than patient-specific treatment effects, but still outside of the usual design remit for an efficacy trial. Common analytical approaches for clinical trials have good properties for estimating average treatment effects when the covariates are regarded as nuisance parameters (White et al., 2021), but are not guaranteed to perform so well if the effects of the covariates are themselves of interest.\nFinally, there is the meta-analytic perspective to consider. An effect size which is less heterogeneous between studies is once again desirable. If arguments about the transportability of odds ratios by the Harrell, Senn, Doi, and others are to be believed, we should report conditional odds ratios, as those are likely to be the most useful for meta-analysts. The CONSORT guidelines for reporting randomised trials (Moher et al., 2010) state that both absolute and relative effect sizes should be reported for binary outcomes. I think there is merit in reporting all commonly-used effect size measures: risk difference, relative risk, and odds ratio. This provides the most flexibility for future meta-analysts."
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#summary-in-meme-format",
    "href": "post/2023/07/logit-rd-rr/index.html#summary-in-meme-format",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "Summary, in meme format",
    "text": "Summary, in meme format\nIf all of that was a bit much to take in, maybe this will help:\n\n\n\nTrue facts on both sides.\n\n\nIn the rest of this post I’ll demonstrate how odds ratios are noncollapsible and show how to calculate risk differences and relative risks from logistic regression models using R and the marginaleffects package."
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#an-example",
    "href": "post/2023/07/logit-rd-rr/index.html#an-example",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "An example",
    "text": "An example\nThis is the story of some hypothetical researchers who did a randomised controlled trial (RCT) where 560 patients were randomly assigned to either a treatment or control condition. The primary outcome of the trial was a binary measure, indicating whether a patient’s condition had worsened after 6 weeks. The scenario is based on one from Frank Harrell’s blog, with the details changed and embellished.\nThe hidden R code block below loads some R packages and sets up some simulated data for this trial.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(broom)\nlibrary(cowplot)\nlibrary(marginaleffects)\n\ntheme_set(\n  theme_cowplot(font_size = 11, rel_large = 1, rel_small = 1, rel_tiny = 1)\n)\n\nrct_data &lt;- tribble(\n  ~treatment, ~risk, ~outcome_prob,\n  \"Control\", \"Low risk\", 1/7,\n  \"Control\", \"High risk\", 3/5,\n  \"Treatment\", \"Low risk\", 1/10,\n  \"Treatment\", \"High risk\", 1/2\n) %&gt;%\n  mutate(across(c(treatment, risk), fct_inorder)) %&gt;%\n  rowwise(everything()) %&gt;%\n  reframe(\n    outcome = rep(c(1, 0), times = round(140*c(outcome_prob, 1 - outcome_prob)))\n  )\n\n\nLooking at a 2-way table of our hypothetical trial, we can see that the worse outcome (“1”) is more common in the control group (37%) than the treatment group (30%). This is an absolute risk reduction of 7%, a promising sign that our treatment may be beneficial.\n\ntbl_cross(rct_data, outcome, treatment, percent = \"column\")\n\n\n\n\n\n  \n    \n    \n      \n      \n        treatment\n      \n      Total\n    \n    \n      Control\n      Treatment\n    \n  \n  \n    outcome\n\n\n\n        0\n176 (63%)\n196 (70%)\n372 (66%)\n        1\n104 (37%)\n84 (30%)\n188 (34%)\n    Total\n280 (100%)\n280 (100%)\n560 (100%)\n  \n  \n  \n\n\n\n\nJust looking at a table won’t convince anybody, though. At this point, our hypothetical researchers asked a statistician to help out10. The hypothetical statistician immediately realised the need to estimate the treatment effect in some way that also quantified its uncertainty. A binary logistic regression model seemed like a suitable way to examine the effect of the treatment on the outcome.\n\nlrm_unadj &lt;- glm(\n  outcome ~ treatment,\n  family = binomial,\n  data = rct_data\n)\n\nThe table below shows the results of this logistic regression: an odds ratio of 0.73 (95% CI: 0.51 to 1.03). The odds ratio being less than 1 indicates that the treatment was beneficial (since the outcome occurring was bad, in this case) but the p-value of 0.074 indicates this effect is not statistically significant. The researchers were sad, their dreams of publishing in the British Medical Journal scuppered.\n\ntbl_regression(lrm_unadj, exponentiate = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    treatment\n\n\n\n        Control\n—\n—\n\n        Treatment\n0.73\n0.51, 1.03\n0.074\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#adding-a-covariate",
    "href": "post/2023/07/logit-rd-rr/index.html#adding-a-covariate",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "Adding a covariate",
    "text": "Adding a covariate\nJust as the statistician was finishing writing up the results, the researchers mentioned that the patients they recruited came from two different groups, one of which was known to have much worse outcomes than the other.\n“Does that matter?” they asked.\n“Well, using this information could improve your statistical power” said the statistician.\nThe statistician made another 2-way table, stratified by the risk group. Through some fluke, exactly half of the trial sample was low risk and the other half high risk, perfectly balanced across treatment arms. It turned out that while only 14% of the low risk patients in the control arm experienced the worse outcome, 60% of the high risk patients in the control arm experienced that outcome.\n\ntbl_strata(rct_data, risk, tbl_cross, outcome, treatment, percent = \"column\")\n\n\n\n\n\n  \n    \n    \n      \n      \n        Low risk\n      \n      \n        High risk\n      \n    \n    \n      Control\n      Treatment\n      Total\n      Control\n      Treatment\n      Total\n    \n  \n  \n    outcome\n\n\n\n\n\n\n        0\n120 (86%)\n126 (90%)\n246 (88%)\n56 (40%)\n70 (50%)\n126 (45%)\n        1\n20 (14%)\n14 (10%)\n34 (12%)\n84 (60%)\n70 (50%)\n154 (55%)\n    Total\n140 (100%)\n140 (100%)\n280 (100%)\n140 (100%)\n140 (100%)\n280 (100%)\n  \n  \n  \n\n\n\n\nThe statistician decided to fit another logistic regression model, this time including risk as an additional variable in the model.\n\nlrm_adj &lt;- glm(\n  outcome ~ treatment + risk,\n  family = binomial,\n  data = rct_data\n)\n\nLooking at the coefficients in this model, we can see that the estimated effect of treatment has increased: the odds ratio is further away from 1, now being 0.67 (95% CI: 0.45 to 0.99) instead of 0.73 (95% CI: 0.51 to 1.03). The p-value has decreased from 0.074 to 0.045, meaning that the treatment effect is now statistically significant.\n\ntbl_regression(lrm_adj, exponentiate = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    treatment\n\n\n\n        Control\n—\n—\n\n        Treatment\n0.67\n0.45, 0.99\n0.045\n    risk\n\n\n\n        Low risk\n—\n—\n\n        High risk\n9.00\n5.91, 14.0\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThe researchers went home happy, dreaming once again of publishing in a high impact factor journal, but the statistician was left with a nagging feeling that something was not quite right. The covariate was balanced between treatment groups, with equal numbers of low risk and high risk assigned to each treatment. Undergraduate linear models courses taught that adding a covariate to the model in this scenario shouldn’t change the coefficient for treatment, only the standard error. Why, then, did the estimates change like that? Is logistic regression different from linear regression in this regard?"
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#hand-calculations-back-to-the-2-way-table",
    "href": "post/2023/07/logit-rd-rr/index.html#hand-calculations-back-to-the-2-way-table",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "Hand calculations: back to the 2-way table",
    "text": "Hand calculations: back to the 2-way table\nLet’s take another look at the 2-way table and do some hand calculations.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        treatment\n      \n      Total\n    \n    \n      Control\n      Treatment\n    \n  \n  \n    outcome\n\n\n\n        0\n176 (63%)\n196 (70%)\n372 (66%)\n        1\n104 (37%)\n84 (30%)\n188 (34%)\n    Total\n280 (100%)\n280 (100%)\n560 (100%)\n  \n  \n  \n\n\n\n\nThe marginal risk difference is the difference between treatment and control groups in the proportion of patients experiencing the outcome event. From the table above, ignoring the covariate, we can calculate a risk reduction of 0.071:\n\n104/280 - 84/280\n\n[1] 0.07142857\n\n\nThe marginal odds ratio is the ratio of the oddses for treatment and the control group, where the odds is itself the number who experience the outcome event divided by the number who do not11. We can calculate the marginal odds ratio here as 0.73, the same as the unadjusted logistic regression:\n\n(84/196) / (104/176)\n\n[1] 0.7252747\n\n\nNow let’s look at the table stratified by risk, and see how that affects our calculations.\n\n\n\n\n\n\n  \n    \n    \n      \n      \n        Low risk\n      \n      \n        High risk\n      \n    \n    \n      Control\n      Treatment\n      Total\n      Control\n      Treatment\n      Total\n    \n  \n  \n    outcome\n\n\n\n\n\n\n        0\n120 (86%)\n126 (90%)\n246 (88%)\n56 (40%)\n70 (50%)\n126 (45%)\n        1\n20 (14%)\n14 (10%)\n34 (12%)\n84 (60%)\n70 (50%)\n154 (55%)\n    Total\n140 (100%)\n140 (100%)\n280 (100%)\n140 (100%)\n140 (100%)\n280 (100%)\n  \n  \n  \n\n\n\n\nUsing the same formula as before, we can calculate a risk difference of 0.043 in the low risk group, 0.100 in the high risk group, and the equally-weighted average of those two (because the two risk groups are equally common in this example) is 0.071, same as the marginal risk difference calculated above.\n\n# conditional risk difference in the low risk group\n20/140 - 14/140\n\n[1] 0.04285714\n\n# conditional risk difference in the high risk group\n84/140 - 70/140\n\n[1] 0.1\n\n# average of conditional risk differences is the marginal risk difference\n0.5*((20/140 - 14/140) + (84/140 - 70/140))\n\n[1] 0.07142857\n\n\nUsing the formula for odds ratios, we can calculate an odds ratio of 0.67 in the low risk group and 0.67 in the high risk group. The conditional odds ratios in both groups are equal, and equal to the conditional odds ratio from the adjusted logistic regression, but are different from the marginal odds ratio. This is noncollapsibility in action.\n\n# conditional odds ratio in the low risk group\n(14/126) / (20/120)\n\n[1] 0.6666667\n\n# conditional odds ratio in the high risk group\n(70/70) / (84/56)\n\n[1] 0.6666667\n\n\nIn this example, we observed a treatment-by-risk-group interaction on the risk difference scale (absolute risk reduction of 0.041 in the low-risk group, absolute risk reduction of 0.100 in the high-risk group) but not on the odds ratio scale (conditional odds ratio of 0.67 in both groups). There is no mathematical requirement for the odds ratio scale to have a smaller interaction than other scales, but it is mathematically impossible for there to be no interaction on any scale unless the effect size is zero. This issue is discussed in detail from the perspective of psychological research in Rohrer & Arslan (2021)."
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#calculating-risk-difference-and-relative-risks-using-the-marginaleffects-package",
    "href": "post/2023/07/logit-rd-rr/index.html#calculating-risk-difference-and-relative-risks-using-the-marginaleffects-package",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "Calculating risk difference and relative risks using the marginaleffects package",
    "text": "Calculating risk difference and relative risks using the marginaleffects package\nThe researchers returned to the statistician, requesting adjusted and unadjusted risk differences and relative risks to report in their paper. Fortunately, the statistician was familiar with the marginaleffects package, which makes it fairly easy to calculate risk differences from logistic regression models12.\n\nRisk differences\nThe avg_comparisons() function calculates differences in one variable, averaged across the observed distribution of the other variables. By default it does this calculation on the response scale, i.e. predicted probability, which is what we want for a risk difference. For the unadjusted model, we get an average risk reduction of 7.1% (95% CI: -0.1% to 14.9%).\n\navg_comparisons(lrm_unadj, variables = \"treatment\")\n\n\n      Term Contrast Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %  97.5 %\n treatment    T - C  -0.0714     0.0398 -1.79   0.0727 3.8 -0.149 0.00657\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nFor the adjusted model, we get an average risk reduction of 7.1% (95% CI: 0.0% to 14.1%).\n\navg_comparisons(lrm_adj, variables = \"treatment\")\n\n\n      Term Contrast Estimate Std. Error     z Pr(&gt;|z|)   S  2.5 %   97.5 %\n treatment    T - C  -0.0714     0.0354 -2.02   0.0437 4.5 -0.141 -0.00201\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nUnlike the odds ratio, the point estimate of the risk difference didn’t change when we added the covariate, but its estimate got more precise: the confidence interval shrank.\n\n\nRelative risks\nTo get relative risks using avg_comparisons(), we need to ask for the average of the log of the ratio between treatment conditions (lnratioavg in the code below), and then exponentiate that.\nFor the unadjusted model, the relative risk is 0.81 (95% CI: 0.64 to 1.02).\n\navg_comparisons(lrm_unadj, variables = \"treatment\",\n                comparison = \"lnratioavg\", transform = \"exp\")\n\n\n      Term              Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n treatment ln(mean(T) / mean(C))    0.808   0.0749 3.7 0.639   1.02\n\nColumns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n\nFor the adjusted model, the relative risk is 0.81 (95% CI: 0.65 to 1.00).\n\navg_comparisons(lrm_adj, variables = \"treatment\",\n                comparison = \"lnratioavg\", transform = \"exp\")\n\n\n      Term              Contrast Estimate Pr(&gt;|z|)   S 2.5 % 97.5 %\n treatment ln(mean(T) / mean(C))    0.808   0.0458 4.4 0.655  0.996\n\nColumns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n\nCompared to the unadjusted model, the point estimate hasn’t changed but the confidence interval shrank. Like risk differences, and unlike odds ratios, relative risks are collapsible."
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#in-pictures",
    "href": "post/2023/07/logit-rd-rr/index.html#in-pictures",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "In pictures",
    "text": "In pictures\nOne last way of showing the same thing: below is a plot of unadjusted and adjusted odds ratios, risk differences, and relative risks. You can see that for the odds ratio, the adjustment has moved the point estimate but not made the confidence interval narrower, whereas for the other measures, the point estimate has not changed but the confidence intervals are narrower for the adjusted estimate.\n\n\nCode\n# odds ratio estimates\nor_estimates &lt;- bind_rows(\n  \"Unadjusted\" = \n    tidy(lrm_unadj, conf.int = TRUE, exponentiate = TRUE) %&gt;%\n    filter(term == \"treatmentT\"),\n  \"Adjusted\" = \n    tidy(lrm_adj, conf.int = TRUE, exponentiate = TRUE) %&gt;%\n    filter(term == \"treatmentT\"),\n  .id = \"model\"\n) %&gt;%\n  select(model, estimate, conf.low, conf.high)\n\n# risk difference estimates\nrd_estimates &lt;- bind_rows(\n  \"Unadjusted\" =\n    avg_comparisons(lrm_unadj, variables = \"treatment\"),\n  \"Adjusted\" =\n    avg_comparisons(lrm_adj, variables = \"treatment\"),\n  .id = \"model\"\n) %&gt;%\n  select(model, estimate, conf.low, conf.high)\n\n# relative risk estimates\nrr_estimates &lt;- bind_rows(\n  \"Unadjusted\" =\n    avg_comparisons(lrm_unadj, variables = \"treatment\",\n                    comparison = \"lnratioavg\", transform = \"exp\"),\n  \"Adjusted\" =\n    avg_comparisons(lrm_adj, variables = \"treatment\",\n                    comparison = \"lnratioavg\", transform = \"exp\"),\n  .id = \"model\"\n) %&gt;%\n  select(model, estimate, conf.low, conf.high)\n\n# combine all estimands into a single data frame\nall_estimates &lt;- bind_rows(\n  \"Odds ratio\" = or_estimates,\n  \"Risk difference\" = rd_estimates,\n  \"Relative risk\" = rr_estimates,\n  .id = \"estimand\"\n)\n\n# plot!\nall_estimates %&gt;%\n  mutate(model = fct_inorder(model), estimand = fct_inorder(estimand)) %&gt;%\n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high,\n             y = model)) +\n  geom_pointrange() +\n  scale_y_discrete(limits = rev) +\n  facet_wrap(vars(estimand), nrow = 3, scales = \"free\") +\n  panel_border() +\n  labs(y = NULL, x = \"Estimate (95% CI)\")"
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#acknowledgements",
    "href": "post/2023/07/logit-rd-rr/index.html#acknowledgements",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to those who provided feedback on earlier versions of this post, some which resulted in substantial revisions and improvements:\n\nIsabella Ghement\nLachlan Cribb\nSolomon Kurz\n\nThis does not represent an endorsement of this post by any of the above. Mistakes and opinions are entirely my own.\nFeedback welcome, especially any corrections in cases where I may have misunderstood or misattributed arguments, or made clear factual errors."
  },
  {
    "objectID": "post/2023/07/logit-rd-rr/index.html#footnotes",
    "href": "post/2023/07/logit-rd-rr/index.html#footnotes",
    "title": "You are what you ATE: Choosing an effect size measure for binary outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis plural form is non-standard but I’m trying to make it happen.↩︎\nFor a recent example, see this talk by Jonathan Bartlett.↩︎\nStatisticians.↩︎\nStatisticians use the word “marginal” to refer to a population-level average, or an integral. Economists, on the other hand, use the word “marginal” to refer to a change in a quantity, or a derivative. These opposite meanings can potentially cause a lot of confusion.↩︎\nThis issue is also sometimes raised in longitudinal studies, where it is said that generalised linear mixed models (GLMMs) target conditional odds ratios while generalised estimating equations (GEEs) target marginal odds ratios. This has led to an epidemic of epidemiologists applying GEEs to their data. As far as I understand it, this claim is only half true: odds ratios from GEEs target what you’d get from a GLMM after marginalising over random effects, but are still conditional on other covariates in the model (fixed effects in a GLMM). Confused yet? Probably not nearly as confused as you should be.↩︎\nNeither is the hazard ratio, which means — despite its ubiquity — Cox regression for survival analysis is technically not allowed by the estimand framework.↩︎\nTechnically, logistic regression directly estimates a conditional difference of log-odds. The difference on the log scale is usually exponentiated and reported as an odds ratio.↩︎\nOr even an additive model — exactly the same point applies to Generalised Additive Models (GAMs).↩︎\nAt this point you may find yourself inclined to avoid logistic regression purely to spite the mathematicians. I don’t blame you.↩︎\nStatisticians love being asked to help at the last minute.↩︎\nThe odds ratio is a ratio of ratios, no wonder nobody understands them.↩︎\nOne thing to be aware of is that the standard errors are derived using the delta method, which is a large-sample approximation. There is experimental support for bootstrapping and simulation-based inference in the marginaleffects package to get more accurate estimates in small samples, but this blog post isn’t going there.↩︎"
  },
  {
    "objectID": "post/2023/06/dplyr-fitting-multiple-models-at-once/index.html",
    "href": "post/2023/06/dplyr-fitting-multiple-models-at-once/index.html",
    "title": "Fitting many statistical models at once using dplyr",
    "section": "",
    "text": "One common task in applied statistics is to fit and interpret a number of statistical models at once. For example, fitting a model with the same structure to a number of different outcome or explanatory variables, or fitting several models with different structure to the same data. Here are some examples of how I usually do this, using features that were introduced with dplyr version 1.1.0.\nFor this demonstration, we’ll be using the R packages dplyr, tidyr, ggplot2 (all of which are included in the tidyverse), as well as gt for making tables, emmeans for obtaining estimated means and comparisons, and performance for model-checking.\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(emmeans)\nlibrary(performance)\n\nWe’ll be using the Palmer Penguins data collected at Palmer Station, Antarctica, made available by Dr Kristen Gorman, and conveniently accessible in R using the palmerpenguins package. This dataset contains measurements on a number of penguins of different species on different islands.\n\nlibrary(palmerpenguins)\ndata(penguins)\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSuppose we want to compare bill length, bill depth, flipper length, and body mass between species. We could manually run a separate model for each, but here’s a way to to automate the process. As with many things in R, the trick to doing this easily is to get the data in long form, with the outcomes stacked on top of each other, and a variable indicating which outcome is being measured.\nThe pivot_longer() function from tidyr gets the data into this format. I’ve also taken an extra step of recoding the “outcome” variable to give more descriptive labels. This isn’t required but it will make the tables and plots that we make later look nicer.\n\npenguins_long &lt;- penguins %&gt;%\n  pivot_longer(\n    c(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),\n    names_to = \"outcome\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    outcome = fct_inorder(outcome),\n    outcome = fct_recode(\n      outcome,\n      \"Bill length (mm)\" = \"bill_length_mm\",\n      \"Bill depth (mm)\" = \"bill_depth_mm\",\n      \"Flipper length (mm)\" = \"flipper_length_mm\",\n      \"Body mass (g)\" = \"body_mass_g\"\n    )\n  )\n\nLooking at the first few rows of this data frame, you can see we now have four rows for each penguin, one for each type of measurement:\n\nhead(penguins_long)\n\n# A tibble: 6 × 6\n  species island    sex     year outcome              value\n  &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;                &lt;dbl&gt;\n1 Adelie  Torgersen male    2007 Bill length (mm)      39.1\n2 Adelie  Torgersen male    2007 Bill depth (mm)       18.7\n3 Adelie  Torgersen male    2007 Flipper length (mm)  181  \n4 Adelie  Torgersen male    2007 Body mass (g)       3750  \n5 Adelie  Torgersen female  2007 Bill length (mm)      39.5\n6 Adelie  Torgersen female  2007 Bill depth (mm)       17.4\n\n\nWe can use group_by() and summarise() from the dplyr package to group the rows by outcome, and then “summarise” these groups of rows down to a single row containing a statistical model for each outcome. This makes use of a couple of R tricks: ‘list columns’, a column in a data frame that contains a more complex object than the standard R data types (numeric, character, etc); and the new pick() verb which returns a data frame containing selected columns (in this case, all of them) within the group that’s being operated on.\n\npenguin_models &lt;- penguins_long %&gt;%\n  group_by(outcome) %&gt;%\n  summarise(\n    model = list(\n      lm(value ~ species, data = pick(everything()))\n    )\n  )\n\nYou can see the resulting data from contains four rows, one for each outcome, and a statistical model (“lm”) for each:\n\nprint(penguin_models)\n\n# A tibble: 4 × 2\n  outcome             model \n  &lt;fct&gt;               &lt;list&gt;\n1 Bill length (mm)    &lt;lm&gt;  \n2 Bill depth (mm)     &lt;lm&gt;  \n3 Flipper length (mm) &lt;lm&gt;  \n4 Body mass (g)       &lt;lm&gt;  \n\n\nIdeally we would also do a visual check of model assumptions. One way to do this is something like the code below, which saves a residual plot for each model to an image file, which can be inspected later. It uses the check_model() function in the performance package to generate the plots. The generated residual plots aren’t shown here, but they all look fine.\n\nwalk2(\n  penguin_models$outcome, \n  penguin_models$model,\n  ~ ggsave(\n    paste0(.x, \".png\"),\n    plot(check_model(.y, check = c(\"pp_check\", \"linearity\",\n                                   \"homogeneity\", \"qq\"))),\n    width = 12,\n    height = 9\n  )\n)\n\nOnce we’ve fitted the models, we can obtain quantities of interest from them. In this example we’ll look at estimated means for each species, p-values testing the hypothesis that all species means are equal (against at least one pair of means being different), and comparisons (differences in means) between all pairs of species.\nThe reframe() function from dplyr allows us to run some code that produces a data frame on each model and stack the results on top of each other. We can use the emmeans() function from the emmeans package to obtain estimated marginal means and as_tibble() to convert the result into a data frame. The rowwise(outcome) at the start tells reframe() that we want to call emmeans() separately for each row of the data frame (i.e., each outcome model), and preserve the outcome variable in the result.\n\npenguin_means &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    emmeans(model, \"species\") %&gt;%\n      as_tibble()\n  )\n\nThe first few rows of the resulting data frame are shown below. There is a row for each outcome for each species, containing the mean (emmean), standard error (SE), degrees of freedom (df) and lower and upper confidence limits (lower.CL and upper.CL).\n\nhead(penguin_means)\n\n# A tibble: 6 × 7\n  outcome          species   emmean     SE    df lower.CL upper.CL\n  &lt;fct&gt;            &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Bill length (mm) Adelie      38.8 0.241    339     38.3     39.3\n2 Bill length (mm) Chinstrap   48.8 0.359    339     48.1     49.5\n3 Bill length (mm) Gentoo      47.5 0.267    339     47.0     48.0\n4 Bill depth (mm)  Adelie      18.3 0.0912   339     18.2     18.5\n5 Bill depth (mm)  Chinstrap   18.4 0.136    339     18.2     18.7\n6 Bill depth (mm)  Gentoo      15.0 0.101    339     14.8     15.2\n\n\nWe can use ggplot() to present the results visually. The plot shows that there’s a substantial variation between species in means of all of these measurements, with little or no overlap between many of the confidence intervals. Gentoo penguins appear to be heavier, and have longer flippers but shorter and shallower bills, than the other species.\n\npenguin_means %&gt;%\n  ggplot(aes(x = emmean, y = species, xmin = lower.CL, xmax = upper.CL)) +\n  geom_errorbar(width = 0.5) +\n  geom_point() +\n  scale_y_discrete(limits = rev) +\n  facet_wrap(vars(outcome), nrow = 2, scales = \"free_x\") +\n  labs(\n    x = \"Mean\", \n    y = \"Species\",\n    caption = \"Error bars show 95% confidence interval for mean.\"\n  )\n\n\n\n\nThe gt() function can be used to produce a nice table of results. The code shown below combines the lower.CL and upper.CL columns to produce a single column with the confidence interval, and separately specifies fewer decimal places for body mass than the other measures. The group_by() function before gt() results in a table sub-heading for each outcome. You could easily change this to group_by(species) to arrange the results by species.\n\npenguin_means %&gt;%\n  group_by(outcome) %&gt;%\n  gt() %&gt;%\n  fmt_number(c(emmean, SE, lower.CL, upper.CL),\n             decimals = 1, use_seps = FALSE) %&gt;%\n  fmt_number(c(emmean, SE, lower.CL, upper.CL),\n             rows = outcome == \"Body mass (g)\",\n             decimals = 0, use_seps = FALSE) %&gt;%\n  fmt_number(df, decimals = 0) %&gt;%\n  cols_align(\"left\", species) %&gt;%\n  cols_merge_range(lower.CL, upper.CL, sep = \", \") %&gt;%\n  cols_label(\n    species = \"Species\",\n    emmean = \"Mean\",\n    lower.CL = \"95% Confidence Interval\"\n  )\n\n\n\n\n\n  \n    \n    \n      Species\n      Mean\n      SE\n      df\n      95% Confidence Interval\n    \n  \n  \n    \n      Bill length (mm)\n    \n    Adelie\n38.8\n0.2\n339\n38.3, 39.3\n    Chinstrap\n48.8\n0.4\n339\n48.1, 49.5\n    Gentoo\n47.5\n0.3\n339\n47.0, 48.0\n    \n      Bill depth (mm)\n    \n    Adelie\n18.3\n0.1\n339\n18.2, 18.5\n    Chinstrap\n18.4\n0.1\n339\n18.2, 18.7\n    Gentoo\n15.0\n0.1\n339\n14.8, 15.2\n    \n      Flipper length (mm)\n    \n    Adelie\n190.0\n0.5\n339\n188.9, 191.0\n    Chinstrap\n195.8\n0.8\n339\n194.2, 197.4\n    Gentoo\n217.2\n0.6\n339\n216.0, 218.4\n    \n      Body mass (g)\n    \n    Adelie\n3701\n38\n339\n3627, 3775\n    Chinstrap\n3733\n56\n339\n3623, 3843\n    Gentoo\n5076\n42\n339\n4994, 5158\n  \n  \n  \n\n\n\n\nWe can do similar to produce an overall “F” test for each model, testing the hypothesis that all species have equal means for a particular measure against the hypothesis that at least one pair of means is different. The joint_tests() function in emmeans does this.\n\npenguin_tests &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    joint_tests(model) %&gt;%\n      as_tibble()\n  )\n\nThe resulting data frame is shown below. This time there is one row per model, but if there had been multiple variables in the model, there would have been one row per variable or interaction term (distinguished by the model term column). Each row contains the results of a hypothesis test: numerator and denominator degrees of freedom (df1 and df2), F-statistics (F.ratio) and p-value (p.value).\n\nprint(penguin_tests)\n\n# A tibble: 4 × 6\n  outcome             `model term`   df1   df2 F.ratio   p.value\n  &lt;fct&gt;               &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 Bill length (mm)    species          2   339    411. 2.69e- 91\n2 Bill depth (mm)     species          2   339    360. 1.51e- 84\n3 Flipper length (mm) species          2   339    595. 1.35e-111\n4 Body mass (g)       species          2   339    344. 2.89e- 82\n\n\nAgain, this can be presented in a table using gt():\n\npenguin_tests %&gt;%\n  gt() %&gt;%\n  fmt_number(F.ratio, decimals = 1) %&gt;%\n  fmt_number(p.value, decimals = 3) %&gt;%\n  cols_merge_range(df1, df2, sep = \", \") %&gt;%\n  sub_small_vals(p.value, threshold = 0.001) %&gt;%\n  cols_label(\n    outcome = \"Outcome\",\n    `model term` = \"Predictor\",\n    df1 = \"df\",\n    F.ratio = \"F\",\n    p.value = \"p-value\"\n  )\n\n\n\n\n\n  \n    \n    \n      Outcome\n      Predictor\n      df\n      F\n      p-value\n    \n  \n  \n    Bill length (mm)\nspecies\n2, 339\n410.6\n&lt;0.001\n    Bill depth (mm)\nspecies\n2, 339\n359.8\n&lt;0.001\n    Flipper length (mm)\nspecies\n2, 339\n594.8\n&lt;0.001\n    Body mass (g)\nspecies\n2, 339\n343.6\n&lt;0.001\n  \n  \n  \n\n\n\n\nFinally, we often want to obtain comparisons between particular estimated quantities. In this example we use the emmeans package again for this, this time using the pairs() function to produce comparisons between all pairs of species.\n\npenguin_pairs &lt;- penguin_models %&gt;%\n  rowwise(outcome) %&gt;%\n  reframe(\n    emmeans(model, \"species\") %&gt;%\n      pairs(infer = TRUE, reverse = TRUE, adjust = \"none\") %&gt;%\n      as_tibble()\n  )\n\nThe first few rows of the data frame are shown below. The contents are similar to what we saw earlier for the estimated means, but this time each row contains information on a difference between pairs of means (described in the contrast column), along with the estimated difference in means, standard error, degrees of freedom, confidence interval, t-statistic and p-value.\n\nhead(penguin_pairs)\n\n# A tibble: 6 × 9\n  outcome       contrast estimate    SE    df lower.CL upper.CL t.ratio  p.value\n  &lt;fct&gt;         &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1 Bill length … Chinstr…  10.0    0.432   339    9.19    10.9    23.2   4.23e-72\n2 Bill length … Gentoo …   8.71   0.360   339    8.01     9.42   24.2   5.33e-76\n3 Bill length … Gentoo …  -1.33   0.447   339   -2.21    -0.449  -2.97  3.18e- 3\n4 Bill depth (… Chinstr…   0.0742 0.164   339   -0.248    0.396   0.453 6.50e- 1\n5 Bill depth (… Gentoo …  -3.36   0.136   339   -3.63    -3.10  -24.7   7.93e-78\n6 Bill depth (… Gentoo …  -3.44   0.169   339   -3.77    -3.11  -20.3   1.59e-60\n\n\nThese comparisons can be plotted or presented in a table using code very similar to what we used for the estimated means. The plot below also includes a dotted line indicating zero difference, which can be used as a visual indicator for whether comparisons are statistically significant.\n\npenguin_pairs %&gt;%\n  ggplot(aes(x = estimate, y = contrast, xmin = lower.CL, xmax = upper.CL)) +\n  geom_vline(xintercept = 0, linetype = \"dotted\") +\n  geom_errorbar(width = 0.5) +\n  geom_point() +\n  scale_y_discrete(limits = rev) +\n  facet_wrap(vars(outcome), nrow = 2, scales = \"free_x\") +\n  labs(\n    x = \"Difference in means\", \n    y = \"Contrast\",\n    caption = \"Error bars show 95% confidence interval for difference in mean.\"\n  )"
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "",
    "text": "Missing data refers to data which was intended to have been collected but was not, and is a common scenario in biomedical and social science research. Analysing datasets that have missing data requires extra care and consideration to produce correct results.\nThe best method for dealing with missing data depends on the underlying process causing the missingness. There is no single approach that is always the best, and the terminology commonly used to describe missingness doesn’t directly relate to what analysis methods perform best. In this blog post I’ll begin by defining the commonly used but sometimes unhelpful classifications for missing data mechanisms (you’ll see them everywhere, so you might as well know what they mean) and then explain how to describe missing data processes using causal diagrams (DAGs) and provide some references to help choose an analysis based on that."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#mcar-mar-mnar-or-nah",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#mcar-mar-mnar-or-nah",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "MCAR, MAR, MNAR or nah?",
    "text": "MCAR, MAR, MNAR or nah?\nThe most common classification for missing data mechanisms is that data is either Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR). This classification is based on concepts first introduced in Rubin (1976). These terms are widespread, but can be confusing when first encountered because they do not mean quite what you might expect them to mean. I’ll define them briefly here, but the goal is to not have to focus specifically about which of these mechanisms apply to your data, or use this classification to guide your analytical choices.\nData is considered to be Missing Completely at Random (MCAR) if the probability of being missing does not depend on the values of any of the variables in the data, whether those values are missing or observed (Little & Rubin, 2014, p. 12). In other words, the missingness cannot be related to the research question of interest (Lee et al., 2021).\nData is Missing at Random (MAR) if the probability of being missing depends only on data that was observed (Little & Rubin, 2014, p. 12). Most commonly this refers to a situation where a variable with incomplete data has probability of being missing which relates to completely-observed variables, but there are other possibilities which fit this definition too1. This term is misleading if encountered without context, as a plain-language interpretation suggests it may mean something more like MCAR. However, MCAR is a stricter requirement than MAR. If data is MCAR, it is also considered MAR.\nData is Missing Not at Random (MNAR) if it’s not Missing at Random (Little & Rubin, 2014, p. 12). At least that’s relatively straightforward. In other words, the probability of data being missing is related to what the missing values would have been, had we observed them. It is sometimes implied that MNAR data is a lost cause for statistical analysis, but later in this post we will see specific examples of MNAR mechanisms where common statistical procedures produce valid results for common questions.\nStrictly speaking, these classifications refer to the entire dataset collectively, usually consisting of many variables, some of which may have missing values and some of which may not. When there are multiple variables with missing values — a common situation in real datasets — these classifications are sometimes informally applied to specific variables with missing data, but doing so doesn’t relate to any of the mathematical guarantees about which analysis methods are applicable.\nThese missing data classifications depend on the type of analysis and set of available variables. For example, if additional variables (sometimes called auxiliary variables) which relate to probability of partially-observed variables having missing values are added to the dataset, the missing data mechanism may change from being MNAR to being MAR.\n\n\n\n\n\n\nHistorical aside\n\n\n\nRubin (1976) is often cited as the source of this classification system, but didn’t actually introduce the terms Missing Completely at Random or Missing Not at Random, only Missing at Random. Rubin originally defined an additional condition, Observed at Random. Data which is both Missing at Random and Observed at Random is what we would now commonly refer to as Missing Completely at Random. The term Missing Completely at Random came later, in Marini et al. (1980). This history is given in Little (2021) and was also confirmed in a Tweet by Raphael Nishimura: ‘I was curious about that too and did some digging with the authors. Rod said that “Rubin’s 1976 Biometrika paper defines MAR and OAR (observed at random) but he may not have put the two together.” Don confirmed it and added that “MCAR was first formally defined in a joint paper with Marini and Olsen, I think in 1980, in a more applied paper.”’\n\n\n\n\n\n\n\n\nIgnorable and non-ignorable missingness\n\n\n\nSometimes you may hear missing data described as “ignorable” or “non-ignorable”. These terms are also potentially misleading. “Ignorable” missing data doesn’t mean that you can just ignore the fact that you have missing data when doing an analysis. This term was defined in Rubin (1976) to mean that (1) the data is MAR; (2) the likelihood can be factorised into a part relating to the missingness probability and a part relating to the distribution of the underlying data. These provide a sufficient condition for missing data to be dealt with using likelihood-based methods.\n\n\n\nHow do I know what kind of missing data mechanism I have?\nUnfortunately, there’s no way to determine the missing data mechanism purely by looking at the data — you need to think about the process generating the data and why some of it is missing. Often there will be different reasons for missing data in the same variable. A causal DAG is a good way to reason about the relationship between the variables and their reasons for being missing and can help you decide what analysis to use. Rather than using the DAG to determine whether your data is MCAR, MAR, or MNAR, and then use those classifications to choose an analysis, you can use the DAG directly to guide your choice of analysis method. I’ll have examples of this later in this blog.\n\n\nIf I know my missing data mechanism, what analysis should I do?\nIt depends! (I’m a statistician, you should have known I would say that.) These are the standard results about what missing data methods are appropriate under different processes:\nComplete case analysis is unbiased if the data is MCAR. This is a sufficient condition, not a necessary condition (Little, 2021): there are situations where complete case analysis may be valid, or at least provide a valid estimate of a particular quantity of interest, under MAR or MNAR. Depending on the amount of missing data, complete case analysis may be inefficient (i.e., have lower power than other methods) because only cases where no variables have missing values are used in the analysis.\nMultiple imputation, inverse probability weighting, likelihood-based methods and full Bayesian methods are unbiased if the data is MAR or MCAR (Little, 2021). Multiple imputation is approximately equivalent to a maximum-likelihood method if the underlying model is the same (Collins et al., 2001).\nIn some specific MNAR situations, some of the above methods may still be valid, depending on what you’re trying to estimate (White & Carlin, 2010)."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#some-examples-of-missing-data-mechanisms",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#some-examples-of-missing-data-mechanisms",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Some examples of missing data mechanisms",
    "text": "Some examples of missing data mechanisms\nTo make these definitions more concrete, here are some examples of missing data mechanisms that may occur in real studies.\n\nExample 1: Planned missing data\nSometimes missing data arises from a study design where some data is not collected on some participants. For example, consider a psychometric instrument with a large number of items. The items could be split up into sets which are asked at different times; for example, half at baseline and half a follow-up. Ideally this process would be randomised so each participant receives a different random split. The items which were not asked at a particular time point are missing values, and because the missingness was deliberately introduced by the experimenter using randomisation, we know that the probability of a particular item being missing is unrelated to any other variable in the study.\nThis is one of the few cases where we can be sure the data is MCAR.\n\n\nExample 2: Missingness related to confounders only\nConsider a longitudinal study where participants need to regularly travel to a site, for example a hospital, in order for their data to be recorded. Participants living in rural or remote areas may find the travel more inconvenient if they need to visit a city to participate in the study, and thus be more likely to have missing values in many variables.\nIf we know the location of all of the participants, this missingness mechanism would be MAR.\n\n\nExample 3: Missingness related to confounders and exposure\nThis example is drawn from Moreno-Betancur et al. (2018). Suppose we want to investigate the relationship between childhood maternal mental illness (the exposure, in epidemiologist-speak) and child behaviour in subsequent years (the outcome). In order to provide a causal estimate of the effect of this exposure, we must control for confounding variables, such as maternal alcohol consumption, smoking, and other variables relating to the child’s behaviour.\nIt was considered likely that missingness in all variables was related to both maternal mental illness and the confounding variables. Since child behaviour was measured at a later time than the exposure and confounding variables, it was considered unlikely to have affected missingness in the confounders or exposure.\nIt was uncertain whether missingness in child behaviour could be related to the child behaviour itself — so there are two plausible missing data mechanisms to consider in this example. Both of those plausible mechanisms are MNAR, but we will see later than one possibility is amenable for analysis and the other is not."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#making-the-missing-data-process-explicit-using-causal-diagrams-dags",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#making-the-missing-data-process-explicit-using-causal-diagrams-dags",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Making the missing data process explicit using causal diagrams (DAGs)",
    "text": "Making the missing data process explicit using causal diagrams (DAGs)\nCausal diagrams are used to represent causal relationships between variables in a study (Pearl, 1995). These relationships are shown visually using a directed acyclic graph (DAG), which is a mathematical term for a bunch of circles with arrows between them. The circles (nodes) represent variables and arrows (edges) between them represent direct causal pathways. The direction of the arrows represents the direction of causation, and a valid DAG never has a path leading from a particular point back to that same point2. If there is no path following the arrows between two variables in a DAG, there is no causal connection between them.\nThe structure of the DAG behind your data cannot be inferred just from looking at the data. It needs to come from substantive expertise about the underlying variables in the data, their relationship to each other, and how the data was collected.\nThe idea of using a DAG to represent missing data assumptions was first introduced in Mohan & Pearl (2014). To provide more practical advice for specific scenarios, Moreno-Betancur et al. (2018) introduces “canonical” causal diagrams for missing data in the setting where we want to estimate the relationship between an outcome Y and an exposure X3, adjusting for some confounding variables which may influence both the outcome and the exposure. The confounding variables may be either completely observed (Z1) or have some missing data (Z2). There may also be some unknown, unmeasured factors U which affect the exposure and the confounders.\nThe DAG corresponding to the scenario above is shown below. If you click the “Code” button you can see how it was produced in R, using the packages dagitty (Textor et al., 2017) and ggdag. You don’t need to understand R code to use and apply DAGs but it is provided as an example for those who may want to make causal diagrams for their own studies. The first part of the code describes the underlying relationships between the variables, using the same syntax used to describe regression models in R. The second part of the code sets up the visual coordinates used for plotting the DAG — an aesthetic consideration only.\n\n\nCode\ndag_nomiss &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3),\n    y = c(U = 0, Z1 = 1, Z2 = -1, X = 0, Y = 0)\n  )\n)\nggdag(dag_nomiss) + theme_dag()\n\n\n\n\n\nThis DAG is called a complete-data DAG, or c-DAG. It represents the causal relationships between the variables if all of them had been completely observed.\nTo represent possible causes of missing data, we can introduce three new variables in the DAG: MX, MY and MZ2, representing whether or not the exposure (X), outcome (Y), or partially-observed confounders (Z2) are missing. Think of them as binary indicator variables, where for example MX = 1 means that X wasn’t observed (for a particular case/participant/observation) and MX = 0 means that X was observed. Adding these variables produces a missingness DAG, or m-DAG. Moreno-Betancur et al. (2018) also considers unmeasured causes of missingness (W) which are not related to any of the substantive variables in the study but do affect the probability of missingness.\nThe variables in the DAG which aren’t prefixed with M still refer to values of the variables as if they had been completely observed — which we no longer have full access to in our real data since we only get to see the variables after the missingness mechanism has occurred. This ensures that the causal mechanism in the c-DAG still applies to the m-DAG, so the m-DAG can be produced by adding additional nodes and arrows to the c-DAG without changing any of the arrows between existing nodes.\nIf we turn the c-DAG above into an m-DAG by adding nodes for MX, MY, and MZ2 but no additional arrows, the absence of arrows to MX, MY and MZ2 would mean that there is no causal relationship between any of the variables and their missingness — in other words, the MCAR missingness mechanism. This scenario would be quite unusual in most real studies, unless the missing data was deliberately planned like our first example in the previous section.\nThis m-DAG is shown below, with an additional variable W representing unmeasured variables which affect the missing data process but are not related to any of the variables in the study. The new variables representing the missing data mechanism are shown in blue. In that MCAR scenario, the blue nodes (missing data mechanism) are completely disconnected from black nodes (relating to the main research question).\n\n\nCode\ndag_mcar &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ W,\n  MY ~ W,\n  MZ2 ~ W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_mcar %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nFigure 2 in Moreno-Betancur et al. (2018) provides 10 examples of m-DAGs, with different combinations of the following features:\n\nincomplete confounders and exposure related to missingness of other variables;\nincomplete confounders and exposure related to their own missingness;\noutcome related to missingness of other variables; and\noutcome related to its own missingness.\n\nThe first of these canonical DAGs, m-DAG A, is an example of a MAR process, where the missingness is only related to completely-observed confounders (Z1) and unmeasured variables unrelated to other variables in the study (W). This DAG describes Example 2 in the previous section, with participant location (part of Z1) causing missingness in other variables (MX, MY, MZ2). This DAG is reproduced below, with the nodes and arrows related to the missing data mechanism shown in blue:\n\n\nCode\ndag_a &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ Z1 + W,\n  MY ~ Z1 + W,\n  MZ2 ~ Z1 + W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_a %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nIn this DAG, so long as the model’s adjustment for confounders is correctly specified (a big “if”!), both complete cases analysis and multiple imputation provide valid estimates of not just the relationship between the exposure and the outcome, but also the complete distribution of the outcome. This isn’t something that should be obvious just from looking at the DAG — there’s a derivation in the supplementary materials for Moreno-Betancur et al. (2018) for this and the other DAGs, with the results summarised in Tables 1 and 2 of that paper. Those who are familiar with causal DAGs may want to know that when there is missing data, this derivation is more complex than rules like “d-separation” and cannot be done algorithmically.\nIn other missing data situations, it may be possible to obtain a valid estimate of the effect of the exposure but not, for example, the population mean of the outcome. In some situations, it may not be possible to obtain a valid estimate of either.\nThe figure below shows m-DAG E, which is an example of an MNAR process. The arrows new to this DAG which weren’t in m-DAG A are shown in red. The probability of missingness in all variables is affected by both the exposure X (which may itself have missing values) and partially-observed confounders Z2. This DAG corresponds to Example 3 in the previous section, if we believe that missingness in child behaviour is not related to the missing values of child behaviour. Perhaps surprisingly, even though this missingness process is MNAR, it is possible to obtain a valid estimate of the relationship between the exposure and the outcome using either complete cases analysis or multiple imputation. However, it is not possible to recover the overall population mean of the outcome.\n\n\nCode\ndag_e &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ Z1 + Z2 + X + W,\n  MY ~ Z1 + Z2 + X + W,\n  MZ2 ~ Z1 + Z2 + X + W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_e %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") & name %in% c(\"X\", \"Z2\") ~ \"firebrick3\",\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nThe final example in this blog is m-DAG J, which is another MNAR process. The arrows new to this DAG are shown in purple. In this example, missingness is also affected by the value of the outcome Y. This DAG is similar to the one for Example 3 in the previous section, if we believe that missingness in the outcome (child behaviour) is related to its own unobserved values. For this m-DAG, it is not possible to recover the relationship between the outcome and the exposure.\n\n\nCode\ndag_j &lt;- dagify(\n  Y ~ X + Z1 + Z2,\n  X ~ Z1 + Z2 + U,\n  Z1 ~ U,\n  Z2 ~ U,\n  MX ~ Z1 + Z2 + X + Y + W,\n  MY ~ Z1 + Z2 + X + Y + W,\n  MZ2 ~ Z1 + Z2 + X + Y + W,\n  coords = list(\n    x = c(U = 0.75, Z1 = 1, Z2 = 1, X = 2, Y = 3,\n          MX = 1.5, MY = 2, MZ2 = 2.5, W = 2),\n    y = c(U = 0, Z1 = 0.75, Z2 = -0.75, X = 0, Y = 0,\n          MX = 1.75, MY = 1.75, MZ2 = 1.75, W = 2.5)\n  )\n)\ndag_j %&gt;%\n  tidy_dagitty() %&gt;%\n  mutate(\n    node_colour = case_when(\n      str_starts(name, \"M\") | name == \"W\" ~ \"dodgerblue3\",\n      .default = \"black\"\n    ),\n    edge_colour = case_when(\n      str_starts(to, \"M\") & name == \"Y\" ~ \"darkorchid3\",\n      str_starts(to, \"M\") & name %in% c(\"X\", \"Z2\") ~ \"firebrick3\",\n      str_starts(to, \"M\") ~ \"dodgerblue3\",\n      .default = \"black\"\n    )\n  ) %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges(aes(edge_colour = edge_colour)) +\n  geom_dag_point(size = 16, aes(colour = node_colour)) +\n  geom_dag_text(colour = \"white\", size = 3.88) +\n  theme_dag() +\n  scale_colour_identity()\n\n\n\n\n\nIf you have an m-DAG where a quantity of interest cannot be recovered, you can consider doing a sensitivity analysis using a \\(\\delta\\)-adjustment. This process is described in popular texts on missing data, e.g. van Buuren (2018) sec 9.2 (also available online) or Molenberghs et al. (2015)."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#applying-dags-to-real-missing-data-problems",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#applying-dags-to-real-missing-data-problems",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Applying DAGs to real missing data problems",
    "text": "Applying DAGs to real missing data problems\nThis section draws heavily from Lee et al. (2023) — especially Figure 1 of that paper — which provides practical guidance for data analysis using missingness DAGs.\nStart by determining the estimand of interest: the scientific quantity you would like to estimate, if you had access to the complete data.\nNext, draw a DAG representing the causal relationships between your variables and their possible reasons for being missing. This will require some thinking about which arrows are plausible and which are not, based on your understanding of the real-world relationships between variables and the way the data has been collected.\nBefore deciding on an analysis method, you must first determine whether the estimand is recoverable, i.e. whether it is possible to estimate it from the data4. One possibility is to compare your DAG to the canonical DAGs in Moreno-Betancur et al. (2018) and the recoverability results in Table 1 of that paper.\nIf your estimand is recoverable, you can proceed to deciding on an analysis method to handle the missing data. Again, the canonical DAGs are useful for this. For canonical DAGs A, B, D and E, complete cases analysis was shown to provide a valid estimate of the relationship between the outcome and the exposure. For canonical DAGs A, B, C, D and E, multiple imputation was shown to provide a valid estimate of the relationship between the outcome and the exposure.\nRandomised clinical trials provide some additional guarantees about the nature of the missing data process which observational studies do not: the assigned intervention is completely observed and known to be unrelated to baseline covariates. While it doesn’t use the m-DAG framework, Sullivan et al. (2018) considers several methods for dealing with missing data in randomised trials that may be useful for those working in that setting.\nThe papers above provide examples of situations where multiple imputation is not necessarily better than simpler methods, as well as situations where complete case analysis does not produce valid inference. The situations where particular missing data methods work do not neatly align with the definitions of MCAR/MAR/MNAR but by drawing a DAG and considering the results in the papers above you should be able to determine which popular methods are applicable to your data and research question."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#acknowledgements",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#acknowledgements",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Isabella Ghement, Lachlan Cribb, and Tom Sullivan for providing valuable feedback which greatly improved this blog post. Last updated: 6 July 2023."
  },
  {
    "objectID": "post/2023/06/untangling-mar-mcar-mnar/index.html#footnotes",
    "href": "post/2023/06/untangling-mar-mcar-mnar/index.html#footnotes",
    "title": "Understanding missing data mechanisms using causal DAGs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are a few subtly different ways of defining this concept mathematically; Seaman et al. (2013) goes into the detail for those who are interested.↩︎\nA “cycle” in mathematician-speak. Hence, directed (arrows have a direction) acyclic (can never get back to where you started) graph (set of nodes and edges).↩︎\nThat’s epidemiologist-speak for the main predictor of interest.↩︎\nThis step is analogous to determining identifiability in causal inference.↩︎"
  },
  {
    "objectID": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html",
    "href": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html",
    "title": "Plotting multiple variables at once using ggplot2 and tidyr",
    "section": "",
    "text": "In exploratory data analysis, it’s common to want to make similar plots of a number of variables at once. For example, a randomised trial may look at several outcomes, or a survey may have a large number of questions. Here is a way to achieve to plot them efficiently using R and ggplot2."
  },
  {
    "objectID": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html#pivoting-longer-turning-your-variables-into-rows",
    "href": "post/2019/11/plotting-multiple-variables-ggplot2-tidyr/index.html#pivoting-longer-turning-your-variables-into-rows",
    "title": "Plotting multiple variables at once using ggplot2 and tidyr",
    "section": "Pivoting longer: turning your variables into rows",
    "text": "Pivoting longer: turning your variables into rows\nggplot2 doesn’t provide an easy facility to plot multiple variables at once because this is usually a sign that your data is not “tidy”. For example, in situations where you want to plot two columns on a graph as points with different colours, the two columns often really represent the same variable, and there is a hidden grouping factor which distinguishes the data points you want to colour differently. The usual answer in this scenario is that you should restructure your data before plotting it. As a bonus, it will probably be easier to analyse your data in that form too.\nLikewise, if you want to split a plot into panels (or facets, in ggplot2-speak), you must plot a single response variable, with a grouping variable to indicate which panel the data should be plotted in. The best structure for your data depends on what you’re trying to do with it, and in this situation, even if your data is in the right form for analysis, it may not be right for some of the plots you want to make.\nFortunately, restructuring your data into the right form is straightforward using the tidyr package and the pivot_longer() function. In this example, I’m going to look at some mocked-up survey data, with six questions stored in variables Q1 through Q6. The original data frame looks like this:\n\nprint(survey_data)\n\n# A tibble: 300 × 7\n   group    Q1    Q2    Q3    Q4    Q5    Q6\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 B         4     1     4     1     2     3\n 2 B         5     2     5     4     3     3\n 3 B         5     4     4     2     2     3\n 4 B         5     1     5     2     4     3\n 5 A         5     2     5     1     1     2\n 6 A         5     3     5     3     2     2\n 7 A         4     3     5     1     4     1\n 8 B         4     3     5     1     2     1\n 9 B         4     4     4     1     3     2\n10 B         4     4     5     2     5     4\n# ℹ 290 more rows\n\n\nYou can convert this into a longer data frame where the question number is stored in one column and the response is stored in a separate column:\n\nlonger_data &lt;- survey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\")\nprint(longer_data)\n\n# A tibble: 1,800 × 3\n   group question response\n   &lt;fct&gt; &lt;chr&gt;       &lt;int&gt;\n 1 B     Q1              4\n 2 B     Q2              1\n 3 B     Q3              4\n 4 B     Q4              1\n 5 B     Q5              2\n 6 B     Q6              3\n 7 B     Q1              5\n 8 B     Q2              2\n 9 B     Q3              5\n10 B     Q4              4\n# ℹ 1,790 more rows\n\n\nYou don’t even need to store the ‘long form’ data as a separate variable. If you’re not going to use the data in this form for anything else, it’s simpler to pipe the data straight into ggplot2. Here I use the facet_wrap() function to plot each question in a separate panel, so we can see the distribution of all of the questions at once:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(x = response)) +\n  geom_bar() +\n  facet_wrap(vars(question), ncol = 3) +\n  labs(x = \"Response (on a 1 to 5 scale)\", y = \"Number of respondents\")\n\n\n\n\nYou can use question as a factor anywhere else you would use a categorical variable with ggplot. For example, you can make some box plots:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(y = response, x = question)) +\n  geom_boxplot() +\n  labs(x = \"Question\", y = \"Response (on a 1 to 5 scale)\")\n\n\n\n\nThis is also a nice demonstration of how box plots are rarely the best way to present Likert scale data.\nAny other variables are retained after you call pivot_longer(), so you can e.g. compare the responses to survey questions based on a demographic variable:\n\nsurvey_data %&gt;%\n  pivot_longer(Q1:Q6, names_to = \"question\", values_to = \"response\") %&gt;%\n  ggplot(aes(x = response, colour = group)) +\n  facet_wrap(vars(question), ncol = 3) +\n  geom_point(stat = \"count\") +\n  geom_line(stat = \"count\") +\n  labs(x = \"Response (on a 1 to 5 scale)\", y = \"Number of respondents\")\n\n\n\n\nBy default, R will sort the levels of factors alphabetically. This isn’t always what you want in this situation—often the order of the variables in your data frame has some meaning to it. The fct_inorder() function allows you to reorder levels of a factor in the order of first appearance in the file. If you use that with the column produced by pivot_longer(), the factor will be ordered by the order of the columns in the original data frame."
  },
  {
    "objectID": "post/2019/12/equivalent-to-t-test/index.html",
    "href": "post/2019/12/equivalent-to-t-test/index.html",
    "title": "Things which are equivalent to t-tests",
    "section": "",
    "text": "If you have a continuous measurement and two groups you’d like to compare based on that measurement, what’s the first statistical test that comes to mind? Chances are it’s the two-sample t-test, sometimes known as Student’s t-test. It’s typically the first statistical test taught in an introductory statistics course, it’s well known and understood, and it has good theoretical properties—so if a t-test answers your research question, you should probably use it. (Actually, in practice, you should probably use Welch’s t-test which doesn’t assume equal variance within groups. For the rest of the post, I’m only going to consider the equal variance case.)\nEvery now and again, I find a client in this situation who has done something which is… not a t-test. Here are some other ways to do the same thing which turn out to be identical to the two-sample t-test:\n\nOne-way ANOVA: since ANOVA is often taught immediately after the t-test as “what do I do if I have more than two groups”, it should perhaps be no surprise that ANOVA gives identical p-values and confidence intervals to the t-test when you are only comparing two groups. This applies to both the overall F-test and the “post-hoc” pairwise t-test, which produce identical p-values in the two group scenario.\nLinear regression with an indicator variable: by this I mean an explanatory variable which takes one numerical value for the first group and a different numerical value for the second group. For example, 0 for the first group and 1 for the second group. This also turns out to be equivalent to a t-test. If you’ve seen how ANOVA is implemented by most software as a linear model with indicator variables for categories, this might not surprise you either.\nLinear regression with the indicator variable as the outcome and the measurement as the explanatory variable: as backwards as this may sound, this will also produce exactly the same p-value as a t-test. The hypothesis test for linear regression slope is the same regardless of which variable is the outcome and which is the predictor, although the regression equation is usually different.\nCorrelation with an indicator variable: this is also equivalent to a t-test. This follows from the above two because the hypothesis test for correlation being equal to zero is equivalent to the hypothesis test for linear regression slope being equal to zero.\nLogistic regression with the group variable as the outcome and the continuous variable as a predictor: this is not quite the same as a t-test. However, if the measurements of the two groups are normally distributed, then the t-test and logistic regression are asymptotically equivalent—meaning that for a sufficiently large sample size, they will give the same p-values. The two methods also answer different research questions: the t-test is for a difference in means between two groups while logistic regression estimates the odds ratio of having a positive outcome (being in a particular group) for a given increase in the continuous predictor. The choice between the two methods should be based on which variable is your outcome and which variable is your explanatory variable; or whether you would prefer to discuss an odds ratio or a difference in means.\nPairwise comparisons from an ANOVA model with more than two groups are also not quite equivalent to a t-test. What’s the difference? Both use a test statistic that has a t distribution calculated from the same difference in means, but the ANOVA pairwise comparisons will have better power because they use a pooled standard deviation from all groups (which, if the assumption of equal standard deviation in every group is true, will be more a precise estimate) and a t distribution with more degrees of freedom (which can make a big difference with a small sample size). As the sample size increases, the advantage of ANOVA over individual t-tests diminishes.\nBonus: The Mann-Whitney test (or Wilcoxon rank sum test) is equivalent to proportional odds logistic regression. (This claim is made in Frank Harrell’s Regression Modeling Strategies text; unlike the other examples above, I haven’t proved or read a proof of this equivalence.)"
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html",
    "href": "post/2019/12/understanding-levels-variation/index.html",
    "title": "Understanding levels of variation and mixed models",
    "section": "",
    "text": "Data that has some kind of hierarchical structure to it is very common in many fields, but is rarely discussed in introductory statistics courses. Terms used to describe this kind of data include hierarchical data, multi-level data, longitudinal data, split-plot designs or repeated measures designs. Statistical models used for these types of data include mixed-effects models (often abbreviated to just mixed models), repeated measures ANOVA and generalised estimating equations (GEEs).\nAll of these terms and models arose from different contexts, but they share a common feature. Observations are not independent, as many classical statistical methods assume, and there is structure to the dependence which can be incorporated into a statistical model.\nUsing a statistical model which doesn’t account for the hierarchical nature of this data will give incorrect results."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#common-examples-of-hierarchical-data",
    "href": "post/2019/12/understanding-levels-variation/index.html#common-examples-of-hierarchical-data",
    "title": "Understanding levels of variation and mixed models",
    "section": "Common examples of hierarchical data",
    "text": "Common examples of hierarchical data\nData with two levels of variation often arise when multiple measurements are made on the same units of observation. In the case of designed experiments, treatments may also be assigned to different levels of the hierarchy. Factors are commonly described as within-subject (varying at the lowest level) or between-subject (varying at a higher level). Some examples:\n\nMeasurements made on the same people at several points in time. This is often called longitudinal data. In this example, time would be a within-subject factor and most other variables of interest—e.g., treatment, age or gender—would be between-subject.\nMeasurements made at different depths of a number of rock core samples. In this example, the depths would be a within-subject factor and the location where the sample was obtained would be a between-subject factor.\nAssigning different treatments to different legs, arms or eyes of a number of people. For example, one eye may be given a new drug and the other eye a placebo. In this example, treatment is a within-subject factor.\nA split-plot experiment in agriculture: splitting plots of land into sections, planting different crops in each section, and using different irrigation methods on different plots. In this example, variety is a within-plot factor and irrigation is a between-plot factor.\nA split-mouth design in dentistry: assigning different treatments to different parts of participants’ mouths.\n\nIt is possible to have more than two levels of variation. Some examples from different fields of research:\n\nStudents within classrooms within schools.\nRepeated surveys administered to individuals within organisations.\nGlands within lesions within patients.\nBlocks of land divided into plots divided into subplots.\n\nIn situations with multiple levels, it is common to describe variables based on the level at which they are measured or assigned, e.g. student-level, classroom-level or school-level."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#random-effects",
    "href": "post/2019/12/understanding-levels-variation/index.html#random-effects",
    "title": "Understanding levels of variation and mixed models",
    "section": "Random effects",
    "text": "Random effects\nMulti-level data is commonly modelled using mixed-effects models, which get their name because they have both fixed effects and random effects. Fixed effects are the kind of explanatory variables you may be used to in ANOVA or linear regression: you would like to directly estimate the effect of these variables on your outcome. For example: treatment (drug or placebo) and time; crop variety and irrigation; depth and location of rock core samples. In these examples, the random effects would be the variables which group together correlated observations: participants in a trial; plots of land; rock core samples.\nHere are two different ways to think about random effects:\n\nRandom effects are factors where the individual levels are random samples from a larger population, or can be thought of in this way.\nRandom effects are factors where you don’t care about the actual effect they have on your outcome, just their ability to account for correlation between observations.\n\nFor a random effect, instead of estimating the effect of each specific level of the factor (e.g. each individual in a study), the model estimates the variance explained by that factor. This is sometimes reported as the proportion of variation explained at each level, e.g. 63% of variance was at the individual level.\nRandom effects can be “nested” inside other random effects if there are more than two levels of variation. For example, “classroom within school” could be specified as a random effect."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#simpler-analysis-options",
    "href": "post/2019/12/understanding-levels-variation/index.html#simpler-analysis-options",
    "title": "Understanding levels of variation and mixed models",
    "section": "Simpler analysis options",
    "text": "Simpler analysis options\nIt is sometimes possible to simplify an analysis if there are no variables which distinguish individuals at a particular level. For example, consider an experiment in which treatments were randomly assigned to litters of pigs but measurements were made on individual pigs, with no pig-level variables (e.g. sex) of interest in the analysis. In this situation, analysing litter averages would be a simpler analysis providing the same results as the mixed model.\nAnother common situation is when there is a single within-subjects factor with two levels, for example before and after measurements. This kind of design can be analysed with a paired t-test or Wilcoxon signed rank test.\nAs a practical consideration, random effects work best when they have a reasonable number of different levels. It may be better to treat a factor which is conceptually random as a fixed effect instead in some cases; e.g. if you are studying students from two or three schools, school should probably be a fixed effect rather than a random effect. There are also situations where this approach is not appropriate.\nIf in doubt about how to analyse your multi-level data, consult a statistician."
  },
  {
    "objectID": "post/2019/12/understanding-levels-variation/index.html#study-design-considerations",
    "href": "post/2019/12/understanding-levels-variation/index.html#study-design-considerations",
    "title": "Understanding levels of variation and mixed models",
    "section": "Study design considerations",
    "text": "Study design considerations\nEffects at the lowest level of the hierarchy (e.g. within-subject) are usually estimated more precisely than effects at higher levels (e.g. between-subject). Or equivalently, tests of within-subject effects tend to be more powerful than tests of between-subject effects. One intuition about this is that there are more observations at the lowest level (e.g. number of subplots) than there are at higher levels (e.g. number of plots). Another way to look at this is that for within-subject factors, each individual unit of observation is effectively their own control."
  },
  {
    "objectID": "post/2020/03/beautiful-bar-charts-ggplot/index.html",
    "href": "post/2020/03/beautiful-bar-charts-ggplot/index.html",
    "title": "Making beautiful bar charts with ggplot",
    "section": "",
    "text": "Bar charts (or bar graphs) are commonly used, but they’re also a simple type of graph where the defaults in ggplot leave a lot to be desired. This is a step-by-step description of how I’d go about improving them, describing the thought processess along the way. Every plot is different and the decisions you make need to reflect the message you’re trying to convey, so don’t treat this post as a recipe, treat it as some points to consider—and hopefully, a few tips that will help you achieve the look you want in your own plots.\nFor this blog post, I’m going to use the number of seats won by each political party in the 2018 Victorian state election as an example. This data was obtained from the Victorian Electoral Commission. Victorians may remember this election being described as a “Danslide”, where Labor, led by Premier Daniel Andrews, won a clear majority of seats.\nHere’s the data as an R command you can paste if you want to try making these plots yourself:\n\nelection_data &lt;- tribble(\n                    ~party, ~seats_won,\n       \"Australian Greens\",          3,\n  \"Australian Labor Party\",         55,\n                 \"Liberal\",         21,\n           \"The Nationals\",          6,\n        \"Other Candidates\",          3\n)\n\nThe first decision to make when you’re thinking of making a bar chart is whether you’d be better off using a different type of plot entirely. Bar charts are most suitable for displaying counts, percentages or other quantities where zero has a special meaning. If you make a bar chart, your axis should always start at zero, or the area of the bar gives a misleading visual impression. Other ways of representing data, such as box plots or points with error bars, may be more appropriate for quantities where zero is not an important reference point. If you are representing time series data (repeated observations made over time), a continuous line (perhaps with points at the the times where observations were made) is almost always better than a sequence of bars.\nThe second decision to make is which axis to put the categorical variable and which axis to put the numerical variable. Having the categories on the y axis often works best. It gives you more space when you have either a large number of categories or categories with long labels.\nA lot of software either makes it more difficult to put categories on the y axis, or requires you to change a setting away from the default. This used to be the case for ggplot, but as of version 3.3.0, you just need to tell it which variable goes on which axis, and it will figure out the rest:\n\nggplot(election_data,\n       aes(x = seats_won, y = party)) +\n  geom_col()\n\n\n\n\nThe bar chart above is a good starting point, but quite a few things could be improved. The order of the categories is a bit odd: from top to bottom, it’s in reverse alphabetical order. This is the default in ggplot, but it is almost never what you want. The easiest way to change this is to give the option limits = rev to the y axis scale (this is also new in ggplot version 3.3.0):\n\nggplot(election_data,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_y_discrete(limits = rev)\n\n\n\n\nIn this particular case, alphabetical ordering isn’t the best choice. It’s often best to order categories from most common to least common, or from most to least in the variable you’re displaying (e.g. most to least seats won). There are two functions in the forcats package which can help with this: fct_infreq orders the level of a factor by how frequently they occur in the data, and fct_reorder orders the level of a factor by the values of a different variable.\n\nelection_data_sorted &lt;- election_data %&gt;%\n  mutate(party = fct_reorder(party, seats_won, .desc = TRUE))\n\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_y_discrete(limits = rev)\n\n\n\n\nIf I was doing exploratory data analysis, or making a quick plot to show a colleague, I might stop at this point. But there is still plenty of room for improvement. To start with, the axis labels are the variable names in our data frame, which is better than no labels at all, but are usually too brief or jargon-laden for a wider audience. A good plot can be interpreted clearly with as little supporting information as possible—remember that a reader’s eye will be drawn to a large, colourful figure and ignore the paragraphs of text you’ve written describing the full context.\nI’ve also changed the origin of the x axis so the bars are hard against the axis. Putting a blank space to the left of zero on a bar chart is something I’ve only ever seen in ggplot. It’s caused by ggplot’s standard rule of adding 10% padding on either side of the biggest and smallest values plotted. You can turn it off by setting the expand option on the x axis scale.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\")\n\n\n\n\nThe grey background with white gridlines is a very distinctive ggplot default. Sometimes it works well—it can reduce the visual noise of gridlines in complex plots—but in this case I would normally opt for a simpler. I often use theme_bw or theme_minimal.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw()\n\n\n\n\nThe gridlines on the x axis are useful guides to the eye, but for a categorical variable with only a few categories, the gridlines only introduce clutter. They can be removed using the theme function.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\nThe default dark grey bars look a bit drab. You can choose a colour and give it as an option to geom_col. If you’re like me, you’ll probably try a couple of wrong things first: either passing the colour inside aes() (won’t work, because the colour will be interpreted as data to plot) or using colour instead of fill (which changes the border colour instead). You can either use a hexadecimal colour code (like in HTML), or one of a number of built-in colour names.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party)) +\n  geom_col(fill = \"darkorchid\") +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\nIn this context, it would be conventional to colour the bars based on the party being represented. Here I’ve set fill = party, then given a manual scale for the fill based on party colours found on Wikipedia. I also disabled the legend, which isn’t necessary in this instance—the party names are already next to the bars. I’ve also added a title and a caption indicating the source of the data; these wouldn’t normally be included in an academic publication but are a very good idea for a plot which might be copied out of context.\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party, fill = party)) +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                               \"Australian Greens\", \"Other Candidates\"),\n                    values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                               \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"off\")\n\n\n\n\nDepending on what you’re trying to show with the graph, you may want to add annotations beyond what is contained in the original data. For example, you could add a dashed line at 44 seats indicating the number required for a party to form a majority government (which could slightly misleading, since the Liberal and National parties govern in coalition). In ggplot, you can pass specific data to any geom_ function; in this example I’m using geom_vline to draw the dashed line and geom_text to draw the label:\n\nggplot(election_data_sorted,\n       aes(x = seats_won, y = party, fill = party)) +\n  geom_vline(xintercept = 44, linetype = 2, colour = \"grey20\") +\n  geom_text(x = 45, y = 4, label = \"majority of\\nparliament\", \n            hjust = 0, size = 11 * 0.8 / .pt, colour = \"grey20\") +\n  geom_col() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_fill_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                               \"Australian Greens\", \"Other Candidates\"),\n                    values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                               \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme_bw() +\n  theme(panel.grid.major.y = element_blank(),\n        legend.position = \"off\")\n\n\n\n\nFinally, another good option for representing the same type of data as a bar chart is a line with a point at the end. The point draws the eye to the end of the line, which is the actual value being represented. You can create this in ggplot by using a geom_segment to draw the line segment and geom_point to draw the point. You also need to give xend and yend for geom_segment to work—it’s a general function for drawing line segments, not a specific functon for creating this type of plot.\n\nggplot(election_data_sorted,\n       aes(x = seats_won,\n           xend = 0,\n           y = party,\n           yend = party,\n           colour = party)) +\n  geom_segment() +\n  geom_point() +\n  scale_x_continuous(expand = expansion(mult = c(0, 0.1))) +\n  scale_y_discrete(limits = rev) +\n  scale_colour_manual(breaks = c(\"Australian Labor Party\", \"Liberal\", \"The Nationals\",\n                                 \"Australian Greens\", \"Other Candidates\"),\n                      values = c(\"#DE3533\", \"#0047AB\", \"#006644\",\n                                 \"#10C25B\", \"#808080\")) +\n  labs(x = \"Number of seats won\",\n       y = \"Party\",\n       title = \"Victorian election 2018 lower house results\",\n       caption = \"Data source: Victorian Electoral Commission\") +\n  theme(legend.position = \"off\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cameron Patrick",
    "section": "",
    "text": "I am a PhD student with the Clinical Epidemiology & Biostatistics Unit (CEBU) at the Murdoch Children’s Research Institute in Melbourne, Australia. My research is about estimating complier average causal effects (CACE) in clinical trials in the presence of missing data. My supervisors are Professor Katherine Lee, Associate Professor Margarita Moreno-Betancur and Dr Thomas Sullivan.\nI also work part-time as a statistical consultant at the University of Melbourne Statistical Consulting Centre (SCC), where I’ve worked since 2017. This involves working with researchers at the University and clients outside the University. We can assist with all stages of quantitative research: designing experiments or surveys, planning an appropriate analysis, analysing data, making graphs and communicating results clearly in papers or reports. As part of my role at the SCC, I also teach the short course Introduction to R and Reproducible Research twice per year, alongside Dr Sandy Clarke-Errey.\nI’ve been on the Statistical Society of Australia Victoria and Tasmania Branch Council since 2021.\nYou can find me on Twitter or email me at cameron.patrick@unimelb.edu.au"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Cameron Patrick",
    "section": "Education",
    "text": "Education\nPhD (in progress) University of Melbourne, 2023—\nMaster of Science (Mathematics and Statistics) University of Melbourne, 2016\nBachelor of Science (Pure Mathematics) University of Western Australia, 2009"
  },
  {
    "objectID": "index.html#recent-blog-posts",
    "href": "index.html#recent-blog-posts",
    "title": "Cameron Patrick",
    "section": "Recent blog posts",
    "text": "Recent blog posts"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Cameron Patrick’s Blog",
    "section": "",
    "text": "Some Quarto PDF formatting tips, with particular reference to thesis writing\n\n\n\n\n\n\n\nquarto\n\n\nlatex\n\n\n\n\nI’ve just spent most of a day faffing around trying to get Quarto to produce a nice PDF file that I like the look of and which meets my university’s formatting requirements for a PhD thesis. Maybe my pain and suffering can help reduce yours.\n\n\n\n\n\n\n17 July, 2023\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nYou are what you ATE: Choosing an effect size measure for binary outcomes\n\n\n\n\n\n\n\nr\n\n\nstatistics\n\n\n\n\nWherein I try to make sense of ongoing debates in the statistical community about how to analyse and report clinical trials targeting binary outcomes, and then write some R code to practice what I hesitantly preach.\n\n\n\n\n\n\n14 July, 2023\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding missing data mechanisms using causal DAGs\n\n\n\n\n\n\n\nmissing-data\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\n28 June, 2023\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nFitting many statistical models at once using dplyr\n\n\n\n\n\n\n\nr\n\n\nstatistical-models\n\n\n\n\n\n\n\n\n\n\n\n8 June, 2023\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nMaking beautiful bar charts with ggplot\n\n\n\n\n\n\n\nr\n\n\nggplot2\n\n\n\n\n\n\n\n\n\n\n\n15 March, 2020\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nThings which are equivalent to t-tests\n\n\n\n\n\n\n\nstatistics\n\n\nt-test\n\n\n\n\n\n\n\n\n\n\n\n16 December, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding levels of variation and mixed models\n\n\n\n\n\n\n\nstatistics\n\n\nmixed-models\n\n\n\n\n\n\n\n\n\n\n\n10 December, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\n  \n\n\n\n\nPlotting multiple variables at once using ggplot2 and tidyr\n\n\n\n\n\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\n\n\n\n\n26 November, 2019\n\n\nCameron Patrick\n\n\n\n\n\n\nNo matching items"
  }
]
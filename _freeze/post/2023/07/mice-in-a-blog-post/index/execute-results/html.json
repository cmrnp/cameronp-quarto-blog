{
  "hash": "378bb37b6f0ba06a8558423e25c19447",
  "result": {
    "markdown": "---\ntitle: \"Dirty imputation done dirt cheap: implementing Multiple Imputation by Chained Equations in one blog post\"\nauthor: \"Cameron Patrick\"\ndate: \"2023-07-31\"\ndraft: true\ncategories: [r, statistics]\ncsl: apa.csl\nbibliography: mice.yaml\nformat:\n  html:\n    toc: true\nabstract: |\n  An attempt to understand in detail how Multiple Imputation by Chained\n  Equations (MICE) works, by coding it up from scratch.\nexecute:\n  echo: true\n  message: false\n  warning: false\nfig-width: 5.5\nfig-height: 3.5\nfig-dpi: 130\n---\n\n\n## Introduction\n\nCoding up an algorithm is a great way to make sure you really understand how the details work. In this post I'm going to implement multiple imputation and the MICE algorithm [@vanbuuren2007multipleimputationdiscrete], albeit in much simplified form: only considering missing data in numeric variables, only using Normal-distribution Bayesian linear regression to generate the imputed data, no concerns about robustness of the code for production purposes.\n\nIf the standard version of this method is called MICE, think of this as a smaller, cuter, maybe slightly endangered variation. Perhaps a fat-tailed dunnart.\n\n![Fat-tailed dunnart in Queensland, Australia. Image by [Bernard Dupont, sourced from Wikimedia Commons (CC-SA licence)](https://commons.wikimedia.org/wiki/File:Fat-tailed_Dunnart_(Sminthopsis_crassicaudata)_(9998321085).jpg).](fat-tailed-dunnart.jpg){width=715px}\n\n## Bayesian linear regression\n\nWe will be imputing each individual variable with missing data using Bayesian linear regression with uninformative Jeffreys priors. For this choice of prior, the posterior distribution of the parameters has an easily computable analytic distribution. This approach is explained in detail in @gelman2014bayesiandataanalysis, chapter 14.\n\nThe regression equation is given by $E(Y|X) = X^T \\beta$, where $Y$ is a column vector of observations, $X$ is the design matrix, and $\\beta$ is a column vector of regression coefficients. We assume independent normally distributed errors with equal standard deviation: $Y|X \\sim \\mathrm{MVN}(X^T \\beta, \\sigma^2 I)$. In other words, the usual classical linear regression.\n\n### Obtaining parameter estimates from Jeffreys priors\n\nThe simplest choice of prior (in terms of easily obtaining a posterior distribution) is the Jeffreys prior, which is an improper uninformative prior: uniform over $(\\beta, \\log \\sigma)$. In this case, the posterior distribution of $\\beta$ is multivariate normal: $\\beta \\sim \\mathrm{MVN}(\\hat\\beta, V)$. Here $\\hat\\beta = (X^T X)^{-1} X^T Y$ is the frequentist maximum likelihood estimate (ordinary least squares regression) and $V = \\sigma^2 (X^T X)^{-1}$ is the usual linear regression variance-covariance matrix.\n\nThe posterior distribution of $\\sigma^2$ is $\\mathrm{Inverse-}\\chi^2(n-k, s^2)$ where $s^2$ is the standard frequentist estimate of the residual variance [@gelman2014bayesiandataanalysis, p. 355]. This scaled inverse $\\chi^2$ distribution was a new one to me[^ugbayes]. We say a random variable $U \\sim \\mathrm{Inverse-}\\chi^2(\\nu, \\mu)$ if $V \\sim \\chi^2(\\nu)$ and $U = \\nu\\mu/V$ [@gelman2014bayesiandataanalysis, p. 581].\n\n[^ugbayes]: Well, I probably met it and forgot about it long ago in undergrad Bayes class.\n\nWe could just use the built-in R `lm` function, but it's been a long time since I last implemented linear regression from scratch, so thought I'd give it a go here. If you're not interested in seeing this, [skip to the section about drawing from the posterior distribution](#sec-drawing-posterior).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate_bayes_lm_jeffreys(Y, X): obtain Bayesian linear regression parameter\n# estimates from design matrix X and observations Y, using a Jeffreys prior.\n# Should produce the same output as lm().\nestimate_bayes_lm_jeffreys <- function(Y, X) {\n  stopifnot(is.matrix(X) & is.numeric(X) & !any(is.na(X)))\n  stopifnot(is.vector(Y) & is.numeric(Y) & !any(is.na(Y)))\n  stopifnot(nrow(X) == length(Y))\n\n  # crossprod(X) computes X^T X\n  # solve(X, Y) computes X^-1 Y\n  # Some fiddling is needed since we want our vector outputs to be R vectors,\n  # not R matrices\n  xtx <- crossprod(X)\n  beta <- as.vector(solve(xtx, t(X) %*% Y))\n  df <- nrow(X) - ncol(X)\n  s2 <- as.vector(crossprod(Y - X %*% beta) / df)\n  V <- s2 * solve(xtx)\n\n  # Attach variable names to regression coefficients\n  names(beta) <- colnames(X)\n\n  res <- list(X = X, Y = Y, beta = beta, V = V, df = df, s2 = s2)\n  class(res) <- \"bayeslm\"\n  res\n}\n```\n:::\n\n\nThis function expects a vector of observations and a design matrix containing the predictors, but that's not very convenient in practice. To make this easier to use, let's write a function that implements a formula interface closer to the standard R `lm` function. This takes advantage of two base R functions: `model.frame` takes a model formula and prepares a data frame with the variables mentioned in it, with the outcome variable first, omitting missing values, and providing the option to only include a subset of rows; and `model.matrix` which creates a design matrix from a formula and a data frame, adding an intercept and creating dummy variables for categorical variables if needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayes_lm <- function(\n  formula, data = NULL, subset = NULL, \n  na.action = getOption(\"na.action\"),\n  estimator = estimate_bayes_lm_jeffreys\n) {\n  if (is.character(na.action)) {\n    na.action <- get(na.action)\n  }\n  mf_args <- list(formula = formula, data = data,\n                  subset = subset, na.action = na.action)\n  model_frame <- do.call(stats::model.frame, mf_args)\n  model_matrix <- model.matrix(formula, model_frame)\n  estimator(model_frame[[1]], model_matrix)\n}\n```\n:::\n\n\nTo make the objects that we've created behave a bit more like standard R `lm` objects, we can implement some S3 methods for our new type of object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef.bayeslm <- function(m) m$beta\nvcov.bayeslm <- function(m) m$V\nsigma.bayeslm <- function(m) sqrt(m$s2)\ndf.residual.bayeslm <- function(m) m$df\nresid.bayeslm <- function(m) (m$Y - m$X %*% t(m$beta))\nprint.bayeslm <- function(m, digits = 3) {\n  cat(\"Coefficients:\\n\")\n  print(format(m$beta, digits = digits), quote = FALSE)\n}\nsummary.bayeslm <- function(m)\n  data.frame(\n    term = names(m$beta),\n    estimate = m$beta,\n    std.error = sqrt(diag(m$V)),\n    conf.low = m$beta - qt(0.975, m$df) * sqrt(diag(m$V)),\n    conf.high = m$beta + qt(0.975, m$df) * sqrt(diag(m$V)),\n    row.names = seq_along(m$beta)\n  )\n```\n:::\n\n\nLet's make sure this works as expected, using the `penguins` data from the `palmerpenguins` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\ndata(penguins)\n```\n:::\n\n\nFit two models, one using `lm` and one using `bayes_lm`, to predict bill length from flipper length:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguins_lm <- lm(\n  bill_length_mm ~ flipper_length_mm , \n  data = penguins\n)\npenguins_blm <- bayes_lm(\n  bill_length_mm ~ flipper_length_mm,\n  data = penguins\n)\n```\n:::\n\n\nCheck that the regression coefficients are the same for both models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(penguins_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = bill_length_mm ~ flipper_length_mm, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5792 -2.6715 -0.5721  2.0148 19.1518 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -7.26487    3.20016   -2.27   0.0238 *  \nflipper_length_mm  0.25477    0.01589   16.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.126 on 340 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4306,\tAdjusted R-squared:  0.4289 \nF-statistic: 257.1 on 1 and 340 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(penguins_blm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               term   estimate  std.error    conf.low  conf.high\n1       (Intercept) -7.2648678 3.20015684 -13.5594666 -0.9702689\n2 flipper_length_mm  0.2547682 0.01588914   0.2235148  0.2860216\n```\n:::\n:::\n\n\n### Drawing from the posterior predictive distribution {#sec-drawing-posterior}\n\nTo sample from the posterior predictive distribution, we use a two-stage process. First, draw from the posterior distribution of the parameters (regression coefficients and residual variance). As previously discussed, the regression coefficients have a multivariate normal distribution and the residual variance has an $\\mathrm{Inverse-}\\chi^2$ distribution. Secondly, draw posterior predictions conditional on those parameter estimates, using $Y|X,\\beta,\\sigma^2 \\sim \\mathrm{N}(X^T \\beta, \\sigma^2)$.\n\nThe function below does the first stage, drawing from the posterior distribution of the model parameters. It doesn't require the model to be fit using the code from above, this should work for any `lm` model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw_bayes_lm_params <- function(m, ndraw = 1) {\n  stopifnot(inherits(m, c(\"lm\", \"bayeslm\")))\n  stopifnot(is.numeric(ndraw) & length(ndraw) == 1 & ndraw >= 1)\n\n  # Draw from the posterior distribution of parameters\n  draw_beta <- mvtnorm::rmvnorm(ndraw, coef(m), vcov(m))\n  df <- df.residual(m)\n  draw_sigma <- sigma(m) * sqrt(df / rchisq(ndraw, df))\n  list(beta = draw_beta, sigma = draw_sigma)\n}\n```\n:::\n\n\n\n\nTo get some idea if this is working, let's plot the posterior distribution of our parmaeters. They look like what we might expect - normally distributed about the parameter estimates shown above. In fact the distribution of sigma looks far closer to normal than I expected, given it's actually a scaled inverse Chi squared distribution!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(12345)\npenguins_blm %>%\n  draw_bayes_lm_params(1000) %>%\n  map(as_tibble) %>%\n  bind_cols() %>%\n  rename(sigma = value) %>%\n  pivot_longer(everything(), names_to = \"var\", values_to = \"value\") %>%\n  ggplot(aes(x = value)) +\n  stat_slabinterval(normalize = \"panels\") +\n  scale_y_continuous(breaks = NULL) +\n  facet_wrap(vars(var), scales = \"free\") +\n  labs(x = \"value\", y = \"density\") +\n  panel_border()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/posterior-param-dists-1.png){fig-align='center' width=715}\n:::\n:::\n\n\nThe posterior regression coefficients for flipper length and intercept are negatively correlated, as we might expect:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(54321)\npenguins_blm %>%\n  draw_bayes_lm_params(100) %>%\n  pluck(\"beta\") %>%\n  as_tibble() %>%\n  ggplot(aes(x = flipper_length_mm, y = `(Intercept)`)) +\n  geom_point(pch = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/posterior-param-corr-1.png){fig-align='center' width=715}\n:::\n:::\n\n\nNow we're more confident that seemed to work, we can write another function to estimate the posterior predictive distribution of a bunch of new observations, given their X values. This works by drawing a set of $\\beta$ parameters, calculating Y values from those, and adding some normally-distributed random noise based on $\\sigma^2$. This results in the model-predicted distribution of new observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw_bayes_lm_ppred <- function(m, X = m$X, ndraw = 1) {\n  stopifnot(inherits(m, c(\"lm\", \"bayeslm\")))\n  stopifnot(is.matrix(X) & is.numeric(X) & !any(is.na(X)))\n  stopifnot(ncol(X) == length(coef(m)))\n\n  params <- draw_bayes_lm_params(m, ndraw = ndraw)\n  X %*% t(params$beta) + matrix(\n    rnorm(nrow(X) * ndraw, 0, rep(params$sigma, each = nrow(X))),\n    nrow = nrow(X)\n  )\n}\n```\n:::\n\n\nTo demonstrate this in action, the plot below shows a subset of the penguins data, the regression line (grey), the observed data (solid blue circles), and 10 draws from the posterior predictive distribution (hollow red circles):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(42)\npenguins_subset <- penguins %>%\n  select(bill_length_mm, flipper_length_mm) %>%\n  drop_na() %>%\n  sample_n(30)\npenguins_subset_ppred <- draw_bayes_lm_ppred(\n  penguins_blm, \n  model.matrix(bill_length_mm ~ flipper_length_mm, data = penguins_subset), \n  ndraw = 10\n)\npenguins_subset_ppred_dat <- penguins_subset_ppred %>%\n  as_tibble(.name_repair = \"unique\") %>%\n  bind_cols(flipper_length_mm = penguins_subset$flipper_length_mm) %>%\n  pivot_longer(-flipper_length_mm, \n               names_to = \"rep\",\n               values_to = \"bill_length_mm\")\nggplot(penguins_subset, aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_abline(intercept = coef(penguins_blm)[1],\n              slope = coef(penguins_blm)[2],\n              colour = \"black\",\n              linewidth = 1.5,\n              alpha = 0.5) +\n  geom_point(colour = \"dodgerblue4\", size = 4, alpha = 0.5, stroke = NA) +\n  geom_point(colour = \"firebrick4\", size = 1, pch = 1,\n             data = penguins_subset_ppred_dat, alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/posterior-ppred-plot-1.png){fig-align='center' width=715}\n:::\n:::\n\n\nIf we only had missing values in one variable, we would be at the point of producing imputed datasets now. But real-life missing data problems tend to have missing data in multiple variables, so we will need to use an iterative method to generate imputations for all variables.\n\n## Creating imputations by chained equations\n\n### The first iteration of imputed data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_initial_imputation_for_var <- function(x) {\n  stopifnot(is.vector(x) & is.numeric(x))\n  stopifnot(!all(is.na(x)))\n  stopifnot(any(is.na(x)))\n  replace_na(x, mean(x, na.rm = TRUE))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_initial_imputation_for_df <- function(dat, formula_list) {\n  stopifnot(is.data.frame(dat))\n  stopifnot(is.list(formula_list))\n  vars <- map_chr(formula_list, \\(f) all.vars(f)[1])\n  mutate(dat, across(all_of(vars), dunnart_initial_imputation_for_var))\n}\n```\n:::\n\n\n### Creating subsequent iterations\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_impute_var <- function(dat_orig, dat_cur, formula, verbose = TRUE) {\n  stopifnot(is.data.frame(dat_orig) & is.data.frame(dat_cur) &\n            inherits(formula, \"formula\"))\n  # find the variable we're imputing\n  out_var <- all.vars(formula)[1]\n  if (verbose >= 2) {\n    cat(\"    imputing using\", deparse1(formula), \"\\n\")\n  } else if (verbose >= 1) {\n    cat(\" \", out_var)\n  }\n  # which rows have missing data in the original data frame?\n  miss_rows <- is.na(dat_orig[[out_var]])\n  # fit a regression model to the rows that didn't have missing values in\n  # the original data\n  mod <- lm(formula, data = dat_cur[!miss_rows, ])\n  # draw predictions for the rows with missing values\n  mf <- model.frame(formula, data = dat_cur[miss_rows, ])\n  mat <- model.matrix(formula, data = mf)\n  dat_cur[miss_rows, out_var] <- draw_bayes_lm_ppred(mod, mat)\n  # return updated data frame\n  dat_cur\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_impute_iterate <- function(\n  dat_orig, dat_cur, formula_list, verbose = TRUE, j = NA, m = NA\n) {\n  stopifnot(is.data.frame(dat_orig) & is.data.frame(dat_cur) &\n            is.list(formula_list) & all(!is.na(dat_cur)))\n\n  if (verbose >= 2) {\n    cat(\"  imputation\", j, \"of\", m, \"\\n\")\n  } else if (verbose >= 1) {\n    cat(\"  [\", j, \"/\", m, \"]\")\n  }\n\n  dat_out <- reduce(\n    formula_list,\n    \\(dat, form) dunnart_impute_var(dat_orig, dat, form, verbose),\n    .init = dat_cur\n  )\n\n  if (verbose == 1) {\n    cat(\"\\n\")\n  }\n\n  dat_out\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_multiple_impute_iterate <- function(\n  data, imp_out, formula_list, verbose = TRUE, i = NA, iter = NA\n) {\n  if (verbose >= 1) {\n    cat(\"imputation iteration\", i, \"of\", iter, \"\\n\")\n  }\n\n  # apply one imputation step to each imputed dataset\n  map2(\n    imp_out,\n    seq_along(imp_out),\n    \\(dat_cur, j) dunnart_impute_iterate(\n      data, dat_cur, formula_list, verbose = verbose,\n      j = j, m = length(imp_out)\n    )\n  )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_impute <- function(\n  data, formula_list, m = 5, iter = 10, verbose = TRUE\n) {\n  stopifnot(is.data.frame(data))\n  stopifnot(is.list(formula_list) & \n            all(map_lgl(formula_list, \\(x) inherits(x, \"formula\"))))\n  stopifnot(is.numeric(m) & length(m) == 1 & !is.na(m) & m >= 1)\n  stopifnot(is.numeric(iter) & length(iter) == 1 & !is.na(iter) & iter >= 0)\n  stopifnot((is.logical(verbose) | is.numeric(verbose)) & \n            length(verbose) == 1 & !is.na(verbose))\n\n  # set up initial imputations (mean imputation)\n  if (verbose >= 1) {\n    cat(\"generating\", m, \"initial imputations\\n\")\n  }\n  imp_out <- map(\n    seq_len(m), \n    \\(x) dunnart_initial_imputation_for_df(data, formula_list)\n  )\n\n  # check for any NA values after initial imputation, signifying that some\n  # variables with missing data do not have imputation formulas provided\n  if (any(map_lgl(imp_out, \\(dat) any(is.na(dat))))) {\n    stop(\"NAs found after initial imputation step - fix imputation formulas\")\n  }\n\n  # repeat the imputation iteration step 'iter' times\n  all_imps <- accumulate(\n    seq_len(iter),\n    \\(imp, i) dunnart_multiple_impute_iterate(\n      data, imp, formula_list, verbose = verbose, i = i, iter = iter\n    ),\n    .init = imp_out\n  )\n\n  # return results\n  res <- list(\n    m = m,\n    iter = iter,\n    formula_list = formula_list,\n    orig_data = data,\n    iterations = all_imps,\n    imputations = all_imps[[iter + 1]]\n  )\n  class(res) <- \"dunnart\"\n  res\n}\n```\n:::\n\n\n### Functions for extracting imputed data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint.dunnart <- function(obj) {\n  cat(\"'dunnart' multiple imputation object:\\n\",\n      \" -\", obj$m, \"imputed datasets\\n\",\n      \" -\", obj$iter, \"iterations\\n\",\n      \" -\", nrow(obj$orig_data), \"observations of\", ncol(obj$orig_data),\n      \"variables\\n\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_complete_long <- function(obj, iteration = obj$iter) {\n  stopifnot(inherits(obj, \"dunnart\"))\n  stopifnot(is.numeric(iteration) & length(iteration) == 1)\n  stopifnot(iteration >= 0 & iteration <= obj$iter)\n\n  bind_rows(\n    map(obj$iterations[[iteration + 1]],\n        \\(dat) mutate(dat, .id = row_number(), .before = 1L,\n                      .by = all_of(NULL))),\n    .id = \".imp\"\n  ) %>%\n    mutate(.imp = as.numeric(.imp)) %>%\n    remove_rownames()\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_all_iters_long <- function(obj) {\n  stopifnot(inherits(obj, \"dunnart\"))\n  iters <- seq_len(obj$iter + 1) - 1\n  bind_rows(\n    map(iters, \\(i) dunnart_complete_long(obj, i)) %>%\n      set_names(iters),\n    .id = \".iter\"\n  ) %>%\n    mutate(.iter = as.numeric(.iter))\n}\n```\n:::\n\n\n## Taking it for a test ride\n\nFor this example we'll use the `nhanes` data frame from the `mice` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(nhanes, package = \"mice\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(314159)\nimp <- dunnart_impute(\n  nhanes,\n  list(\n    bmi ~ age + hyp + chl,\n    hyp ~ age + bmi + chl,\n    chl ~ age + bmi + hyp\n  ),\n  m = 5,\n  iter = 10,\n  verbose = FALSE\n)\n```\n:::\n\n\n### Diagnostics\n\n#### Trace plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmiss_rows <- nhanes %>%\n  mutate(.id = row_number()) %>%\n  pivot_longer(c(bmi, hyp, chl), names_to = \"var\", values_to = \"miss\") %>%\n  mutate(miss = is.na(miss))\n\ntrace_data <- dunnart_all_iters_long(imp) %>%\n  pivot_longer(c(bmi, hyp, chl), names_to = \"var\", values_to = \"value\") %>%\n  left_join(miss_rows, by = c(\".id\", \"var\")) %>%\n  filter(miss) %>%\n  summarise(\n    mean = mean(value),\n    sd = sd(value),\n    .by = c(.iter, .imp, var)\n  ) %>%\n  mutate(across(c(.iter, .imp), as.factor))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_grid(\n  trace_data %>%\n    ggplot(aes(x = .iter, y = mean, colour = .imp, group = .imp)) +\n    geom_line() +\n    facet_wrap(~var, scales = \"free_y\") +\n    panel_border() +\n    theme(legend.position = \"off\"),\n  trace_data %>%\n    ggplot(aes(x = .iter, y = sd, colour = .imp, group = .imp)) +\n    geom_line() +\n    facet_wrap(~var, scales = \"free_y\") +\n    panel_border() +\n    theme(legend.position = \"off\"),\n  nrow = 2\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/trace-plot-1.png){fig-align='center' width=715}\n:::\n:::\n\n\n\n#### Distribution of imputed data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndunnart_complete_long(imp) %>%\n  pivot_longer(c(bmi, hyp, chl), names_to = \"var\", values_to = \"value\") %>%\n  left_join(miss_rows, by = c(\".id\", \"var\")) %>%\n  ggplot(aes(x = value, y = .imp, colour = miss)) +\n  geom_point(pch = 1) +\n  scale_colour_manual(values = c(\"dodgerblue3\", \"firebrick3\")) +\n  facet_grid(~var, scales = \"free_x\") +\n  panel_border() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/distribution-plot-1.png){fig-align='center' width=715}\n:::\n:::\n\n\n## Running an analysis on the imputed data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_analyse <- function(obj, fn) {\n  stopifnot(inherits(obj, \"dunnart\"))\n  stopifnot(is.function(fn))\n\n  res <- map(obj$imputations, fn)\n  class(res) <- \"dunnart_analysis\"\n  res\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexample_model <- dunnart_analyse(\n  imp,\n  \\(dat) lm(chl ~ age + bmi + hyp, data = dat)\n)\n```\n:::\n\n\n## Pooling imputations using Rubin's rules\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_pool <- function(obj, tidy_fn = broom::tidy) {\n  tidy_analysis <- bind_rows(\n    map(obj, tidy_fn),\n    .id = \".imp\"\n  )\n  m <- length(obj)\n  tidy_analysis %>%\n    summarise(\n      std.error = sqrt(mean(std.error^2) + (m + 1)/m * sd(estimate)^2),\n      estimate = mean(estimate),\n      .by = term\n    ) %>%\n    relocate(std.error, .after = estimate) %>%\n    mutate(\n      statistic = estimate / std.error,\n      p.value = 2*pnorm(-abs(statistic)),\n      conf.low = estimate - qnorm(0.975)*std.error,\n      conf.high = estimate + qnorm(0.975)*std.error\n    )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndunnart_pool(example_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   -70.0      66.4     -1.05  0.291      -200.        60.1\n2 age            47.7      10.9      4.37  0.0000122    26.3       69.1\n3 bmi             7.12      2.32     3.07  0.00213       2.58      11.7\n4 hyp            -9.92     19.8     -0.502 0.616       -48.7       28.8\n```\n:::\n:::\n\n\nThis is very similar to the results from the complete case analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(lm(chl ~ age + bmi + hyp, data = nhanes), conf.int = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 (Intercept)   -81.0      61.8     -1.31  0.222    -221.        58.8\n2 age            55.2      14.3      3.86  0.00383    22.9       87.5\n3 bmi             7.07      2.05     3.44  0.00736     2.42      11.7\n4 hyp            -6.22     23.2     -0.268 0.794     -58.7       46.2\n```\n:::\n:::\n\n\n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
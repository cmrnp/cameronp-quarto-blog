---
title: "You are what you ATE: Choosing an effect size measure for binary outcomes"
author: "Cameron Patrick"
date: "2023-07-13"
draft: true
categories: [r, statistics]
csl: apa.csl
bibliography: 
  - odds-ratios.yaml
format:
  html:
    toc: true
abstract: |
  | Wherein I try to make sense of ongoing debates in the statistical community about how to analyse and report clinical trials targeting binary outcomes, and then write some R code to practice what I hesitantly preach.
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.width = 6,
  fig.height = 3.5,
  fig.retina = 2,
  dpi = 130
)
```


## The great odds ratio debate (with memes)

A lot of noise has been made about why an odds ratio may not be a desirable summary for a treatment effect on a binary variable[^orrefs]. There is that pesky problem that normal humans are unable to correctly interpret an odds ratio [@altman1998oddsratiosshould]. In recent years, people[^stat] have become more aware of the mathematical fact of noncollapsibility [@greenland1999confoundingcollapsibilitycausal; @daniel2021makingapplesoranges; @morris2022planningmethodcovariate]: the marginal[^econstat] odds ratio (population-level odds ratio) is not any kind of average of conditional odds ratios (individual-level odds ratios)[^longitudinal]. The [ICH E9(R1) estimand framework for clinical trials](https://database.ich.org/sites/default/files/E9-R1_Step4_Guideline_2019_1203.pdf) requires a "population-level summary". In the language of causal inference, that's describing an Average Treatment Effect (ATE), and the odds ratio isn't one[^coxreg].

[^orrefs]: For a recent example, see [this talk by Jonathan Bartlett](https://thestatsgeek.com/2023/06/23/is-the-ich-e9-estimand-addendum-compatible-with-model-based-estimands/).

[^stat]: Statisticians.

[^econstat]: Statisticians use the word "marginal" to refer to a population-level average, or an integral. Economists, on the other hand, use the word "marginal" to refer to a change in a quantity, or a derivative. This could never cause any confusion.

[^longitudinal]: This issue is also sometimes raised in longitudinal studies, where it is said that generalised linear mixed models (GLMMs) target conditional odds ratios while generalised estimating equations (GEEs) target marginal odds ratios. As far as I understand it, this is only half true: odds ratios from GEEs target what you'd get from a GLMM after marginalising over random effects, but are still conditional on other covariates in the model (fixed effects in a GLMM).

[^coxreg]: Neither is the hazard ratio, despite the ubiquity of Cox regression for survival analysis.

Risk differences (the difference in probability attributable to a particular treatment) and relative risks (the ratio of probabilities due to a particular treatment) avoid these problems of odds ratios. However, statisticians are usually taught to analyse binary outcomes using logistic regression, the direct output[^logdirect] of which is odds ratios. Someone on Stats Twitter rediscovers all of this every few months and starts a heated argument where nobody goes away happy.

[^logdirect]: Technically, logistic regression directly estimates a conditional difference of log-odds. The difference on the log scale is usually exponentiated and reported as an odds ratio.

![Surprisingly, many statisticians prefer collapsibility.](building-collapse.jpg){fig-alt="Picture of a collapsing building with the caption \"I'll take the non-collapsible one\""}

Personally, I'm a fan of risk differences for communicating potential risks to an individual, ideally presented alongside the baseline level of risk. There's some evidence that patients and clinicians find these measures easier to interpret than relative measures [@zipkin2014evidencebasedriskcommunication], especially when presented in the form of a natural frequency, like "6 in 1000 people" rather than "0.6%".

Statistical analyses are often adjusted for covariates, either to reduce confounding (in observational studies) or improve power (in randomised trials). There are some technical challenges when doing this with risk differences and relative risks. Adjusting for covariates requires specifying some kind of model. If we're using a generalised linear model[^gam], the natural mathematical form of covariate adjustment will depend on whether you are modelling log-odds (logistic regression model, where model parameters correspond to an odds ratio), probability (linear probability model, where model parameters correspond to a risk difference) or log-probability (quasi-Poisson or log-link binomial model, where model parameters correspond to a relative risk). When modelling risk differences or relative risks directly, it's possible to end up with impossible predicted probabilities: "probabilities" which are less than zero or greater than one. Using logistic regression avoids this problem, because any real number on the log-odds scale translates to a probability between zero and one. Any odds ratio can be applied to any level of baseline risk without making mathematicians sad[^mathsad].

[^gam]: Or even an additive model --- exactly the same point applies to Generalised Additive Models.

[^mathsad]: At this point you may find yourself inclined to avoid logistic regression purely to spite the mathematicians. I don't blame you.

There are also compelling --- but not universally accepted --- arguments that despite the difficulty in interpreting odds ratios, they are more likely to be transportable between different levels of baseline risk than risk differences or relative risks [@doi2022controversydebatequestionable; @senn2011uneasereasonsmistrusting]. This is an empirical matter, not a mathematical one, and the evidence is not clear-cut. If we accept this, though, it is another reason to prefer logistic regression for statistical modelling. Effect size measures being "transportable" is another way of saying that the effects are closer to being additive on the scale that effect measure lives on (probability, log-probability, or log-odds). In the context of regression models, using a scale where the effect size is more transportable reduces the need for interaction terms, which can only be estimated well in large samples.

Thus we have a dilemma: people understand risk differences and (to a lesser extent) relative risks, but they can be problematic to estimate with covariate adjustment; nobody understands odds ratios[^interpor], but logistic regression is probably a better model, operating closer to a scale on which the underlying effects are additive. One possible way out is to model the data using logistic regression, and then use that model to produce other quantities of interest. This approach is described in @permutt2020covariateschangeestimand, which has some of the best writing I've encountered in a statistics paper --- being written in the form of a dialogue between a randomiser (the "causal inference" perspective) and a parameteriser (the "statistical modelling" perspective) walking through the Garden of Eden, planning to conduct and analyse a clinical trial.

[^interpor]: If you can intuitively interpret odds ratios, that's great for you, although I'm not sure I believe you.

@permutt2020covariateschangeestimand also considered what information different audiences might want from the results of a clinical trial: regulator, patient, and scientist. There's a fourth audience which I think is worth considering, only very briefly mentioned by Permutt: the meta-analyst, trying to aggregate information from multiple trials.

Permutt argues in favour of the ATE being the main quantity of regulatory interest:

> The average treatment effect should be of regulatory interest, however. The primary analysis of outcome should be of a variable that is reasonably linear in utility to the patient. Then, if and only if the average effect is positive, the ensemble of patients can be said to be better off under the test condition than under the control. This is perhaps the weakest imaginable statistical condition for approval of a drug product, but it is surely a necessary condition.

Permutt notes that studies designed to be able to detect average treatment effects are unlikely to be adequate for patient-specific decision making or providing a more detailed scientific understanding.

The most clearly expressed counter-argument to this comes from [a blog post by Frank Harrell arguing against single-number summaries for treatment effects on binary outcomes](https://www.fharrell.com/post/rdist/):

> Marginal adjusted estimates may be robust, but may not accurately estimate RD for either any patient in the RCT or for the clinical population to which RCT results are to be applied, because in effect they assume that the RCT sample is a random sample from the clinical population, something not required and never realized for RCTs.

This refers to the distinction between the population average treatment effect (PATE) and the sample average treatment effect (SATE). RCT participants are not random samples from any population, not even from the eligible pool of participants for a particular study. But the regulator's decision that Permutt described earlier is motivated by generalising to a broader population, implicitly relying on properties of the PATE. It's not clear to me whether there are likely to be any real-world scenarios where both (1) the practical conclusions drawn from the SATE and PATE would be different; and (2) the conditional odds ratio derived from the RCT sample is in agreement with the PATE but the risk difference SATE is not.

I am a coward and not (yet?) willing to take a strong position in this fight, but am always sympathetic to the idea that a single number is rarely sufficient to describe scientific evidence (see also: p-values). At some point I might write another blog about Harrell's idea of plotting the distribution of estimated patient-level treatment effects, which is intriguing, although I struggle to see the practical purpose of it. Harrell's other writing on this topic is also worth reading:

- [Incorrect Covariate Adjustment May Be More Correct than Adjusted Marginal Estimates](https://www.fharrell.com/post/robcov/) makes a similar point to @white2021covariateadjustmentrandomised with a detailed simulated example;
- [Assessing Heterogeneity of Treatment Effect, Estimating Patient-Specific Efficacy, and Studying Variation in Odds ratios, Risk Ratios, and Risk Differences](https://www.fharrell.com/post/varyor/) argues in favour of the use of odds ratios for modelling, and points out the extreme difficulty of identifying heterogeneous treatment effects even in very large studies; and
- [Unadjusted Odds Ratios are Conditional](https://www.fharrell.com/post/marg/) demonstrates noncollapsibility and argues that conditional odds ratios are more useful than marginal odds ratios.

One issue which I remain unclear about is whether randomised trials powered to detect main treatment effects are likely to reasonably estimate patient-specific baseline risk --- a simpler task than patient-specific treatment effects, but still outside of the usual design remit for an efficacy trial. Common analytical approaches for clinical trials have good properties for estimating average treatment effects when the covariates are regarded as nuisance parameters [@white2021covariateadjustmentrandomised], but are not guaranteed to perform so well if the effects of the covariates are themselves of interest.

Finally, there is the meta-analytic perspective to consider. An effect size which is less heterogeneous between studies is once again desirable. If arguments about the transportability of odds ratios by the Harrell, Senn, Doi, and others are to be believed, we should report odds ratios, as those are likely to be the most useful for meta-analysts. There is some merit in reporting all commonly-used effect size measures: risk difference, relative risk, and odds ratio. This provides the most flexibility for future meta-analysts.

If all of that was a bit much to take in, maybe this will help:

![True facts on both sides.](american-chopper-argument.jpeg){fig-alt="American Chopper argument meme: risk differences aren't transportable / nobody understands odds ratios / clinical trials aren't random samples from the population / marginal odds ratios aren't the average of conditional odds ratios / conditional odds ratios still tell us if an intervention works"}

In the rest of this post I'll demonstrate how odds ratios are noncollapsible and show how to calculate risk differences and relative risks from logistic regression models using R and the `marginaleffects` package.

## An example

This is the story of some hypothetical researchers who did a randomised controlled trial (RCT) where 560 patients were randomly assigned to either a treatment or control condition. The primary outcome of the trial was a binary measure, indicating whether a patient's condition had worsened after 6 weeks. The scenario is based on [one from Frank Harrell's blog](https://www.fharrell.com/post/marg/), with the details changed and embellished.

The hidden R code block below loads some R packages and sets up some simulated data for this trial.

```{r, message = FALSE, warning = FALSE}
#| code-fold: true

library(tidyverse)
library(gt)
library(gtsummary)
library(broom)
library(cowplot)
library(marginaleffects)

theme_set(
  theme_cowplot(font_size = 11, rel_large = 1, rel_small = 1, rel_tiny = 1)
)

rct_data <- tribble(
  ~treatment, ~risk, ~outcome_prob,
  "Control", "Low risk", 1/7,
  "Control", "High risk", 3/5,
  "Treatment", "Low risk", 1/10,
  "Treatment", "High risk", 1/2
) %>%
  mutate(across(c(treatment, risk), fct_inorder)) %>%
  rowwise(everything()) %>%
  reframe(
    outcome = rep(c(1, 0), times = round(140*c(outcome_prob, 1 - outcome_prob)))
  )
```

Looking at a 2-way table of our hypothetical trial, we can see that the worse outcome ("1") is more common in the control group (37%) than the treatment group (30%). This is an absolute risk reduction of 7%, a promising sign that our treatment may be beneficial.

```{r}
tbl_cross(rct_data, outcome, treatment, percent = "column")
```

Just looking at a table won't convince anybody, though. At this point, our hypothetical trialists asked a statistician to help out[^lastminute]. Taking the role of the hypothetical statistician, we'll need to estimate the treatment effect in some way that also quantifies our uncertainty. Let's fit a binary logistic regression model to this data to examine the effect our treatment had on the outcome.

[^lastminute]: Statisticians love being asked to help at the last minute.

```{r}
lrm_unadj <- glm(
  outcome ~ treatment,
  family = binomial,
  data = rct_data
)
```

The table below shows the results of this logistic regression: an odds ratio of 0.73 (95% CI: 0.51 to 1.03). The odds ratio being less than 1 indicates that the treatment was beneficial (since the outcome occurring was bad, in this case) but the p-value of 0.074 indicates this effect is not statistically significant. The trialists were sad, their dreams of publishing in the British Medical Journal scuppered.

```{r}
tbl_regression(lrm_unadj, exponentiate = TRUE)
```

## Adding a covariate

Just as the statistician was finishing writing up the results, the trialists mentioned that the patients they recruited came from two different groups, one of which was known to have much worse outcomes than the other. 

"Does that matter?" they asked. 

"Well, using this information could improve your statistical power" said the statistician.

The statistician made another 2-way table, stratified by the risk group. Through some fluke, exactly half of the trial sample was low risk and the other half high risk, perfectly balanced across treatment arms. It turned out that while only 14% of the low risk patients in the control arm experienced the adverse outcome, 60% of the high risk patients in the control arm experienced the outcome.

```{r}
tbl_strata(rct_data, risk, tbl_cross, outcome, treatment, percent = "column")
```

The statistician decided to fit another logistic regression model, this time including risk as an additional variable in the model.

```{r}
lrm_adj <- glm(
  outcome ~ treatment + risk,
  family = binomial,
  data = rct_data
)
```

Looking at the coefficients in this model, we can see that the estimated effect of treatment has increased: the odds ratio is further away from 1, now being 0.67 (95% CI: 0.45 to 0.99) instead of 0.73 (95% CI: 0.51 to 1.03). The p-value has decreased from 0.074 to 0.045, meaning that the treatment effect is now statistically significant.

```{r}
tbl_regression(lrm_adj, exponentiate = TRUE)
```

The trialists went home happy, dreaming once again of publishing in a high impact factor journal, but the statistician was left with a nagging feeling that something was not quite right. The covariate was balanced between treatment groups, with equal numbers of low risk and high risk assigned to each treatment. Undergraduate linear models courses taught that adding a covariate to the model in this scenario shouldn't change the coefficient for treatment, only the standard error. Why, then, did the estimates change like that? Is logistic regression different from linear regression in this regard?

## Hand calculations: back to the 2-way table

Let's take another look at the 2-way table and do some hand calculations.

```{r, echo = FALSE}
tbl_cross(rct_data, outcome, treatment, percent = "column")
```

The marginal risk difference is the difference between treatment and control groups in the proportion of patients experiencing the outcome event. From the table above, ignoring the covariate, we can calculate a risk reduction of 0.071:

```{r}
#| code-fold: false
104/280 - 84/280
```

The marginal odds ratio is the ratio of the oddses for treatment and the control group, where the odds is itself the number who experience the outcome event divided by the number who do not[^orargh]. We can calculate the marginal odds ratio here as 0.73, the same as the unadjusted logistic regression:

[^orargh]: The odds ratio is a ratio of ratios, no wonder nobody understands them.

```{r}
#| code-fold: false
(84/196) / (104/176)
```

Now let's look at the table stratified by risk, and see how that affects our calculations.

```{r, echo = FALSE}
tbl_strata(rct_data, risk, tbl_cross, outcome, treatment, percent = "column")
```

Using the same formula as before, we can calculate a risk difference of 0.043 in the low risk group, 0.100 in the high risk group, and the equally-weighted average of those two (because the two risk groups are equally common in this example) is 0.071, same as the marginal risk difference calculated above.

```{r}
#| code-fold: false
# conditional risk difference in the low risk group
20/140 - 14/140
# conditional risk difference in the high risk group
84/140 - 70/140
# average of conditional risk differences is the marginal risk difference
0.5*((20/140 - 14/140) + (84/140 - 70/140))
```

Using the formula for odds ratios, we can calculate an odds ratio of 0.67 in the low risk group and 0.67 in the high risk group. The conditional odds ratios in both groups are equal, and equal to the conditional odds ratio from the adjusted logistic regression, but are different from the marginal odds ratio. This is noncollapsibility in action.

```{r}
#| code-fold: false
# conditional odds ratio in the low risk group
(14/126) / (20/120)
# conditional odds ratio in the high risk group
(70/70) / (84/56)
```

In this example, we observed a treatment-by-risk-group interaction on the risk difference scale (absolute risk reduction of 0.041 in the low-risk group, absolute risk reduction of 0.100 in the high-risk group) but not on the odds ratio scale (conditional odds ratio of 0.67 in both groups). There is no mathematical requirement for the odds ratio scale to have a smaller interaction than other scales, but it is mathematically impossible for there to be no interaction on *any* scale unless the effect size is zero. This issue is discussed in detail from the perspective of psychological research in @rohrer2021preciseanswersvague.

## Calculating risk difference and relative risks using the marginaleffects package

```{r, include = FALSE}
rct_data <- rct_data %>%
  mutate(treatment = fct_recode(treatment, "T" = "Treatment", "C" = "Control"))
lrm_unadj <- glm(
  outcome ~ treatment,
  family = binomial,
  data = rct_data
)
lrm_adj <- glm(
  outcome ~ treatment + risk,
  family = binomial,
  data = rct_data
)
```

The trialists returned to the statistician, requesting adjusted and unadjusted risk differences and relative risks to report in their paper. Fortunately, the statistician was familiar with the `marginaleffects` package, which makes it fairly easy to calculate risk differences from logistic regression models[^stderr].

[^stderr]: One thing to be aware of is that the standard errors are derived using the delta method, which is a large-sample approximation. There is experimental support for bootstrapping and simulation-based inference in the `marginaleffects` package to get more accurate estimates in small samples, but this blog post isn't going there.

### Risk differences

The `avg_comparisons()` function calculates differences in one variable, averaged across the observed distribution of the other variables. By default it does this calculation on the response scale, i.e. predicted probability, which is what we want for a risk difference. For the unadjusted model, we get an average risk reduction of 7.1% (95% CI: -0.1% to 14.9%).

```{r}
avg_comparisons(lrm_unadj, variables = "treatment")
```

For the adjusted model, we get an average risk reduction of 7.1% (95% CI: 0.0% to 14.1%).

```{r}
avg_comparisons(lrm_adj, variables = "treatment")
```

Unlike the odds ratio, the point estimate of the risk difference didn't change when we added the covariate, but its estimate got more precise: the confidence interval shrank.

### Relative risks

To get relative risks using `avg_comparisons()`, we need to ask for the average of the log of the ratio between treatment conditions (`lnratioavg` in the code below), and then exponentiate that.

For the unadjusted model, the relative risk is 0.81 (95% CI: 0.64 to 1.02).

```{r}
avg_comparisons(lrm_unadj, variables = "treatment",
                comparison = "lnratioavg", transform = "exp")
```

For the adjusted model, the relative risk is 0.81 (95% CI: 0.65 to 1.00).

```{r}
avg_comparisons(lrm_adj, variables = "treatment",
                comparison = "lnratioavg", transform = "exp")
```

Compared to the unadjusted model, the point estimate hasn't changed but the confidence interval shrank. Like risk differences, and unlike odds ratios, relative risks are collapsible.

## In pictures

One last way of showing the same thing: below is a plot of unadjusted and adjusted odds ratios, risk differences, and relative risks. You can see that for the odds ratio, the adjustment has moved the point estimate but not made the confidence interval narrower, whereas for the other measures, the point estimate has not changed but the confidence intervals are narrower for the adjusted estimate.

```{r coef-plot, fig.width = 6, fig.height = 3}
#| code-fold: true

# odds ratio estimates
or_estimates <- bind_rows(
  "Unadjusted" = 
    tidy(lrm_unadj, conf.int = TRUE, exponentiate = TRUE) %>%
    filter(term == "treatmentT"),
  "Adjusted" = 
    tidy(lrm_adj, conf.int = TRUE, exponentiate = TRUE) %>%
    filter(term == "treatmentT"),
  .id = "model"
) %>%
  select(model, estimate, conf.low, conf.high)

# risk difference estimates
rd_estimates <- bind_rows(
  "Unadjusted" =
    avg_comparisons(lrm_unadj, variables = "treatment"),
  "Adjusted" =
    avg_comparisons(lrm_adj, variables = "treatment"),
  .id = "model"
) %>%
  select(model, estimate, conf.low, conf.high)

# relative risk estimates
rr_estimates <- bind_rows(
  "Unadjusted" =
    avg_comparisons(lrm_unadj, variables = "treatment",
                    comparison = "lnratioavg", transform = "exp"),
  "Adjusted" =
    avg_comparisons(lrm_adj, variables = "treatment",
                    comparison = "lnratioavg", transform = "exp"),
  .id = "model"
) %>%
  select(model, estimate, conf.low, conf.high)

# combine all estimands into a single data frame
all_estimates <- bind_rows(
  "Odds ratio" = or_estimates,
  "Risk difference" = rd_estimates,
  "Relative risk" = rr_estimates,
  .id = "estimand"
)

# plot!
all_estimates %>%
  mutate(model = fct_inorder(model), estimand = fct_inorder(estimand)) %>%
  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high,
             y = model)) +
  geom_pointrange() +
  scale_y_discrete(limits = rev) +
  facet_wrap(vars(estimand), nrow = 3, scales = "free") +
  panel_border() +
  labs(y = NULL, x = "Estimate (95% CI)")
```

## Acknowledgements

Thanks to those who provided feedback on earlier versions of this post, some which resulted in substantial revisions and improvements:

- Isabella Ghement
- Lachlan Cribb

Feedback welcome, especially any corrections in cases where I may have misunderstood or misattributed arguments, or made clear factual errors.

## References
